{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import scipy \n",
    "# import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import sklearn\n",
    "import pyspark as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ALLOW_THREADS',\n",
       " 'AxisError',\n",
       " 'BUFSIZE',\n",
       " 'CLIP',\n",
       " 'ComplexWarning',\n",
       " 'DataSource',\n",
       " 'ERR_CALL',\n",
       " 'ERR_DEFAULT',\n",
       " 'ERR_IGNORE',\n",
       " 'ERR_LOG',\n",
       " 'ERR_PRINT',\n",
       " 'ERR_RAISE',\n",
       " 'ERR_WARN',\n",
       " 'FLOATING_POINT_SUPPORT',\n",
       " 'FPE_DIVIDEBYZERO',\n",
       " 'FPE_INVALID',\n",
       " 'FPE_OVERFLOW',\n",
       " 'FPE_UNDERFLOW',\n",
       " 'False_',\n",
       " 'Inf',\n",
       " 'Infinity',\n",
       " 'MAXDIMS',\n",
       " 'MAY_SHARE_BOUNDS',\n",
       " 'MAY_SHARE_EXACT',\n",
       " 'ModuleDeprecationWarning',\n",
       " 'NAN',\n",
       " 'NINF',\n",
       " 'NZERO',\n",
       " 'NaN',\n",
       " 'PINF',\n",
       " 'PZERO',\n",
       " 'RAISE',\n",
       " 'RankWarning',\n",
       " 'SHIFT_DIVIDEBYZERO',\n",
       " 'SHIFT_INVALID',\n",
       " 'SHIFT_OVERFLOW',\n",
       " 'SHIFT_UNDERFLOW',\n",
       " 'ScalarType',\n",
       " 'Tester',\n",
       " 'TooHardError',\n",
       " 'True_',\n",
       " 'UFUNC_BUFSIZE_DEFAULT',\n",
       " 'UFUNC_PYVALS_NAME',\n",
       " 'VisibleDeprecationWarning',\n",
       " 'WRAP',\n",
       " '_CopyMode',\n",
       " '_NoValue',\n",
       " '_UFUNC_API',\n",
       " '__NUMPY_SETUP__',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__config__',\n",
       " '__deprecated_attrs__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__expired_functions__',\n",
       " '__file__',\n",
       " '__former_attrs__',\n",
       " '__future_scalars__',\n",
       " '__getattr__',\n",
       " '__git_version__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " '_add_newdoc_ufunc',\n",
       " '_builtins',\n",
       " '_distributor_init',\n",
       " '_financial_names',\n",
       " '_get_promotion_state',\n",
       " '_globals',\n",
       " '_int_extended_msg',\n",
       " '_mat',\n",
       " '_no_nep50_warning',\n",
       " '_pyinstaller_hooks_dir',\n",
       " '_pytesttester',\n",
       " '_set_promotion_state',\n",
       " '_specific_msg',\n",
       " '_version',\n",
       " 'abs',\n",
       " 'absolute',\n",
       " 'add',\n",
       " 'add_docstring',\n",
       " 'add_newdoc',\n",
       " 'add_newdoc_ufunc',\n",
       " 'all',\n",
       " 'allclose',\n",
       " 'alltrue',\n",
       " 'amax',\n",
       " 'amin',\n",
       " 'angle',\n",
       " 'any',\n",
       " 'append',\n",
       " 'apply_along_axis',\n",
       " 'apply_over_axes',\n",
       " 'arange',\n",
       " 'arccos',\n",
       " 'arccosh',\n",
       " 'arcsin',\n",
       " 'arcsinh',\n",
       " 'arctan',\n",
       " 'arctan2',\n",
       " 'arctanh',\n",
       " 'argmax',\n",
       " 'argmin',\n",
       " 'argpartition',\n",
       " 'argsort',\n",
       " 'argwhere',\n",
       " 'around',\n",
       " 'array',\n",
       " 'array2string',\n",
       " 'array_equal',\n",
       " 'array_equiv',\n",
       " 'array_repr',\n",
       " 'array_split',\n",
       " 'array_str',\n",
       " 'asanyarray',\n",
       " 'asarray',\n",
       " 'asarray_chkfinite',\n",
       " 'ascontiguousarray',\n",
       " 'asfarray',\n",
       " 'asfortranarray',\n",
       " 'asmatrix',\n",
       " 'atleast_1d',\n",
       " 'atleast_2d',\n",
       " 'atleast_3d',\n",
       " 'average',\n",
       " 'bartlett',\n",
       " 'base_repr',\n",
       " 'binary_repr',\n",
       " 'bincount',\n",
       " 'bitwise_and',\n",
       " 'bitwise_not',\n",
       " 'bitwise_or',\n",
       " 'bitwise_xor',\n",
       " 'blackman',\n",
       " 'block',\n",
       " 'bmat',\n",
       " 'bool_',\n",
       " 'broadcast',\n",
       " 'broadcast_arrays',\n",
       " 'broadcast_shapes',\n",
       " 'broadcast_to',\n",
       " 'busday_count',\n",
       " 'busday_offset',\n",
       " 'busdaycalendar',\n",
       " 'byte',\n",
       " 'byte_bounds',\n",
       " 'bytes_',\n",
       " 'c_',\n",
       " 'can_cast',\n",
       " 'cast',\n",
       " 'cbrt',\n",
       " 'cdouble',\n",
       " 'ceil',\n",
       " 'cfloat',\n",
       " 'char',\n",
       " 'character',\n",
       " 'chararray',\n",
       " 'choose',\n",
       " 'clip',\n",
       " 'clongdouble',\n",
       " 'clongfloat',\n",
       " 'column_stack',\n",
       " 'common_type',\n",
       " 'compare_chararrays',\n",
       " 'compat',\n",
       " 'complex128',\n",
       " 'complex64',\n",
       " 'complex_',\n",
       " 'complexfloating',\n",
       " 'compress',\n",
       " 'concatenate',\n",
       " 'conj',\n",
       " 'conjugate',\n",
       " 'convolve',\n",
       " 'copy',\n",
       " 'copysign',\n",
       " 'copyto',\n",
       " 'corrcoef',\n",
       " 'correlate',\n",
       " 'cos',\n",
       " 'cosh',\n",
       " 'count_nonzero',\n",
       " 'cov',\n",
       " 'cross',\n",
       " 'csingle',\n",
       " 'ctypeslib',\n",
       " 'cumprod',\n",
       " 'cumproduct',\n",
       " 'cumsum',\n",
       " 'datetime64',\n",
       " 'datetime_as_string',\n",
       " 'datetime_data',\n",
       " 'deg2rad',\n",
       " 'degrees',\n",
       " 'delete',\n",
       " 'deprecate',\n",
       " 'deprecate_with_doc',\n",
       " 'diag',\n",
       " 'diag_indices',\n",
       " 'diag_indices_from',\n",
       " 'diagflat',\n",
       " 'diagonal',\n",
       " 'diff',\n",
       " 'digitize',\n",
       " 'disp',\n",
       " 'divide',\n",
       " 'divmod',\n",
       " 'dot',\n",
       " 'double',\n",
       " 'dsplit',\n",
       " 'dstack',\n",
       " 'dtype',\n",
       " 'e',\n",
       " 'ediff1d',\n",
       " 'einsum',\n",
       " 'einsum_path',\n",
       " 'emath',\n",
       " 'empty',\n",
       " 'empty_like',\n",
       " 'equal',\n",
       " 'errstate',\n",
       " 'euler_gamma',\n",
       " 'exp',\n",
       " 'exp2',\n",
       " 'expand_dims',\n",
       " 'expm1',\n",
       " 'extract',\n",
       " 'eye',\n",
       " 'fabs',\n",
       " 'fastCopyAndTranspose',\n",
       " 'fft',\n",
       " 'fill_diagonal',\n",
       " 'find_common_type',\n",
       " 'finfo',\n",
       " 'fix',\n",
       " 'flatiter',\n",
       " 'flatnonzero',\n",
       " 'flexible',\n",
       " 'flip',\n",
       " 'fliplr',\n",
       " 'flipud',\n",
       " 'float16',\n",
       " 'float32',\n",
       " 'float64',\n",
       " 'float_',\n",
       " 'float_power',\n",
       " 'floating',\n",
       " 'floor',\n",
       " 'floor_divide',\n",
       " 'fmax',\n",
       " 'fmin',\n",
       " 'fmod',\n",
       " 'format_float_positional',\n",
       " 'format_float_scientific',\n",
       " 'format_parser',\n",
       " 'frexp',\n",
       " 'from_dlpack',\n",
       " 'frombuffer',\n",
       " 'fromfile',\n",
       " 'fromfunction',\n",
       " 'fromiter',\n",
       " 'frompyfunc',\n",
       " 'fromregex',\n",
       " 'fromstring',\n",
       " 'full',\n",
       " 'full_like',\n",
       " 'gcd',\n",
       " 'generic',\n",
       " 'genfromtxt',\n",
       " 'geomspace',\n",
       " 'get_array_wrap',\n",
       " 'get_include',\n",
       " 'get_printoptions',\n",
       " 'getbufsize',\n",
       " 'geterr',\n",
       " 'geterrcall',\n",
       " 'geterrobj',\n",
       " 'gradient',\n",
       " 'greater',\n",
       " 'greater_equal',\n",
       " 'half',\n",
       " 'hamming',\n",
       " 'hanning',\n",
       " 'heaviside',\n",
       " 'histogram',\n",
       " 'histogram2d',\n",
       " 'histogram_bin_edges',\n",
       " 'histogramdd',\n",
       " 'hsplit',\n",
       " 'hstack',\n",
       " 'hypot',\n",
       " 'i0',\n",
       " 'identity',\n",
       " 'iinfo',\n",
       " 'imag',\n",
       " 'in1d',\n",
       " 'index_exp',\n",
       " 'indices',\n",
       " 'inexact',\n",
       " 'inf',\n",
       " 'info',\n",
       " 'infty',\n",
       " 'inner',\n",
       " 'insert',\n",
       " 'int16',\n",
       " 'int32',\n",
       " 'int64',\n",
       " 'int8',\n",
       " 'int_',\n",
       " 'intc',\n",
       " 'integer',\n",
       " 'interp',\n",
       " 'intersect1d',\n",
       " 'intp',\n",
       " 'invert',\n",
       " 'is_busday',\n",
       " 'isclose',\n",
       " 'iscomplex',\n",
       " 'iscomplexobj',\n",
       " 'isfinite',\n",
       " 'isfortran',\n",
       " 'isin',\n",
       " 'isinf',\n",
       " 'isnan',\n",
       " 'isnat',\n",
       " 'isneginf',\n",
       " 'isposinf',\n",
       " 'isreal',\n",
       " 'isrealobj',\n",
       " 'isscalar',\n",
       " 'issctype',\n",
       " 'issubclass_',\n",
       " 'issubdtype',\n",
       " 'issubsctype',\n",
       " 'iterable',\n",
       " 'ix_',\n",
       " 'kaiser',\n",
       " 'kron',\n",
       " 'lcm',\n",
       " 'ldexp',\n",
       " 'left_shift',\n",
       " 'less',\n",
       " 'less_equal',\n",
       " 'lexsort',\n",
       " 'lib',\n",
       " 'linalg',\n",
       " 'linspace',\n",
       " 'little_endian',\n",
       " 'load',\n",
       " 'loadtxt',\n",
       " 'log',\n",
       " 'log10',\n",
       " 'log1p',\n",
       " 'log2',\n",
       " 'logaddexp',\n",
       " 'logaddexp2',\n",
       " 'logical_and',\n",
       " 'logical_not',\n",
       " 'logical_or',\n",
       " 'logical_xor',\n",
       " 'logspace',\n",
       " 'longcomplex',\n",
       " 'longdouble',\n",
       " 'longfloat',\n",
       " 'longlong',\n",
       " 'lookfor',\n",
       " 'ma',\n",
       " 'mask_indices',\n",
       " 'mat',\n",
       " 'math',\n",
       " 'matmul',\n",
       " 'matrix',\n",
       " 'max',\n",
       " 'maximum',\n",
       " 'maximum_sctype',\n",
       " 'may_share_memory',\n",
       " 'mean',\n",
       " 'median',\n",
       " 'memmap',\n",
       " 'meshgrid',\n",
       " 'mgrid',\n",
       " 'min',\n",
       " 'min_scalar_type',\n",
       " 'minimum',\n",
       " 'mintypecode',\n",
       " 'mod',\n",
       " 'modf',\n",
       " 'moveaxis',\n",
       " 'msort',\n",
       " 'multiply',\n",
       " 'nan',\n",
       " 'nan_to_num',\n",
       " 'nanargmax',\n",
       " 'nanargmin',\n",
       " 'nancumprod',\n",
       " 'nancumsum',\n",
       " 'nanmax',\n",
       " 'nanmean',\n",
       " 'nanmedian',\n",
       " 'nanmin',\n",
       " 'nanpercentile',\n",
       " 'nanprod',\n",
       " 'nanquantile',\n",
       " 'nanstd',\n",
       " 'nansum',\n",
       " 'nanvar',\n",
       " 'nbytes',\n",
       " 'ndarray',\n",
       " 'ndenumerate',\n",
       " 'ndim',\n",
       " 'ndindex',\n",
       " 'nditer',\n",
       " 'negative',\n",
       " 'nested_iters',\n",
       " 'newaxis',\n",
       " 'nextafter',\n",
       " 'nonzero',\n",
       " 'not_equal',\n",
       " 'numarray',\n",
       " 'number',\n",
       " 'obj2sctype',\n",
       " 'object_',\n",
       " 'ogrid',\n",
       " 'oldnumeric',\n",
       " 'ones',\n",
       " 'ones_like',\n",
       " 'outer',\n",
       " 'packbits',\n",
       " 'pad',\n",
       " 'partition',\n",
       " 'percentile',\n",
       " 'pi',\n",
       " 'piecewise',\n",
       " 'place',\n",
       " 'poly',\n",
       " 'poly1d',\n",
       " 'polyadd',\n",
       " 'polyder',\n",
       " 'polydiv',\n",
       " 'polyfit',\n",
       " 'polyint',\n",
       " 'polymul',\n",
       " 'polynomial',\n",
       " 'polysub',\n",
       " 'polyval',\n",
       " 'positive',\n",
       " 'power',\n",
       " 'printoptions',\n",
       " 'prod',\n",
       " 'product',\n",
       " 'promote_types',\n",
       " 'ptp',\n",
       " 'put',\n",
       " 'put_along_axis',\n",
       " 'putmask',\n",
       " 'quantile',\n",
       " 'r_',\n",
       " 'rad2deg',\n",
       " 'radians',\n",
       " 'random',\n",
       " 'ravel',\n",
       " 'ravel_multi_index',\n",
       " 'real',\n",
       " 'real_if_close',\n",
       " 'rec',\n",
       " 'recarray',\n",
       " 'recfromcsv',\n",
       " 'recfromtxt',\n",
       " 'reciprocal',\n",
       " 'record',\n",
       " 'remainder',\n",
       " 'repeat',\n",
       " 'require',\n",
       " 'reshape',\n",
       " 'resize',\n",
       " 'result_type',\n",
       " 'right_shift',\n",
       " 'rint',\n",
       " 'roll',\n",
       " 'rollaxis',\n",
       " 'roots',\n",
       " 'rot90',\n",
       " 'round',\n",
       " 'round_',\n",
       " 'row_stack',\n",
       " 's_',\n",
       " 'safe_eval',\n",
       " 'save',\n",
       " 'savetxt',\n",
       " 'savez',\n",
       " 'savez_compressed',\n",
       " 'sctype2char',\n",
       " 'sctypeDict',\n",
       " 'sctypes',\n",
       " 'searchsorted',\n",
       " 'select',\n",
       " 'set_numeric_ops',\n",
       " 'set_printoptions',\n",
       " 'set_string_function',\n",
       " 'setbufsize',\n",
       " 'setdiff1d',\n",
       " 'seterr',\n",
       " 'seterrcall',\n",
       " 'seterrobj',\n",
       " 'setxor1d',\n",
       " 'shape',\n",
       " 'shares_memory',\n",
       " 'short',\n",
       " 'show_config',\n",
       " 'show_runtime',\n",
       " 'sign',\n",
       " 'signbit',\n",
       " 'signedinteger',\n",
       " 'sin',\n",
       " 'sinc',\n",
       " 'single',\n",
       " 'singlecomplex',\n",
       " 'sinh',\n",
       " 'size',\n",
       " 'sometrue',\n",
       " 'sort',\n",
       " 'sort_complex',\n",
       " 'source',\n",
       " 'spacing',\n",
       " 'split',\n",
       " 'sqrt',\n",
       " 'square',\n",
       " 'squeeze',\n",
       " 'stack',\n",
       " 'std',\n",
       " 'str_',\n",
       " 'string_',\n",
       " 'subtract',\n",
       " 'sum',\n",
       " 'swapaxes',\n",
       " 'take',\n",
       " 'take_along_axis',\n",
       " 'tan',\n",
       " 'tanh',\n",
       " 'tensordot',\n",
       " 'test',\n",
       " 'testing',\n",
       " 'tile',\n",
       " 'timedelta64',\n",
       " 'trace',\n",
       " 'tracemalloc_domain',\n",
       " 'transpose',\n",
       " 'trapz',\n",
       " 'tri',\n",
       " 'tril',\n",
       " 'tril_indices',\n",
       " 'tril_indices_from',\n",
       " 'trim_zeros',\n",
       " 'triu',\n",
       " 'triu_indices',\n",
       " 'triu_indices_from',\n",
       " 'true_divide',\n",
       " 'trunc',\n",
       " 'typecodes',\n",
       " 'typename',\n",
       " 'ubyte',\n",
       " 'ufunc',\n",
       " 'uint',\n",
       " 'uint16',\n",
       " 'uint32',\n",
       " 'uint64',\n",
       " 'uint8',\n",
       " 'uintc',\n",
       " 'uintp',\n",
       " 'ulonglong',\n",
       " 'unicode_',\n",
       " 'union1d',\n",
       " 'unique',\n",
       " 'unpackbits',\n",
       " 'unravel_index',\n",
       " 'unsignedinteger',\n",
       " 'unwrap',\n",
       " 'use_hugepage',\n",
       " 'ushort',\n",
       " 'vander',\n",
       " 'var',\n",
       " 'vdot',\n",
       " 'vectorize',\n",
       " 'version',\n",
       " 'void',\n",
       " 'vsplit',\n",
       " 'vstack',\n",
       " 'where',\n",
       " 'who',\n",
       " 'zeros',\n",
       " 'zeros_like']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of methods(in-class functions) in a module (library) \n",
    "dir(np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list       [a,b] is mutable   (modifiable)\n",
    "# Tuple      (a,b) is immutable (unmodifiable/cannot be changed)\n",
    "# Dictionary {a:b} is used for assigning a uniqe key to a value and catching the value by calling the key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 100 1000\n",
      "10 100 1000\n",
      "a b c\n",
      "10\n",
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "#Unpacking/Assigning elements\n",
    "my_list       = [10,100,1000]\n",
    "my_tuple      = (10,100,1000)\n",
    "my_dictionary = {'a':10 , 'b':100 , 'c':1000}\n",
    "\n",
    "num1,num2,num3 = my_list\n",
    "print(num1,num2,num3)\n",
    "num1,num2,num3 = my_tuple\n",
    "print(num1,num2,num3)\n",
    "key1,key2,key3 = my_dictionary\n",
    "print(key1,key2,key3)               #Note that dictionary unpacking gives keys (not values)\n",
    "\n",
    "#Accessing elements\n",
    "print(my_list[0])\n",
    "print(my_tuple[0])                  #Note that for accessing all types we use []\n",
    "print(my_dictionary['a'])           #Note that for dictionary we don't have index (instead we have key)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "str1str2str3\n",
      "str1str1str1\n",
      "<class 'str'>\n",
      "100\n",
      "<class 'NoneType'>\n",
      "100\n",
      "<class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "object1 = \"str1\" + \"str2\" + \"str3\"  #Concatenating strings\n",
    "print(object1)\n",
    "object2 = \"str1\" * 3                #Concatenating multiply times\n",
    "print(object2)\n",
    "\n",
    "x1 = str(100)                       #Convering to string (object)\n",
    "print(type(x1))                     #Printing type of variable\n",
    "x2 = print(100)\n",
    "print(type(x2))                     #Assigning a variable to 'print' function results in type of 'NoneType'\n",
    "x3 = print(str(100))                #Assigning a variable to 'print' function results in type of 'NoneType'\n",
    "print(type(x3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year  = [1950,1970,1990,2010]\n",
    "pop   = [2.519,3.692,5.263,6.972]\n",
    "year1 = [1800,1850,1900]                #Add data at the end of list\n",
    "year1.extend(year)\n",
    "pop1  = [1.0,1.262,1.650]               #Add data at the end of list\n",
    "pop1.extend(pop)\n",
    "#year = [1800,1850,1900]  + year        #Add data + Operation\n",
    "#pop  = [1.0,1.262,1.650] + pop         #Add data + Operation\n",
    "#year.insert(0,1800)                    #Insert adds an element to the specified index of the list\n",
    "#year.appendleft(1800)\n",
    "plt.plot(year1,pop1)\n",
    "plt.title('World population')           #title\n",
    "plt.xlabel('Year')                      #xlabel (notice \"label\" not \"lable like table\")\n",
    "plt.ylabel('Population')                #ylabel (notice \"label\" not \"lable like table\")\n",
    "plt.yticks([0,2,4,6,8]                  #yticks limits \n",
    "          ,['0','2B','4B','6B','8B'])   #replace axis with string (Billion)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotsize = [100,500,1000,1500]                       #Using a list to change dot size in scatter plot\n",
    "colour  = ['red','green','blue','black']  \n",
    "plt.scatter(year,pop,s=dotsize,c=colour,alpha=0.3)  #Alpha specify the transparency from 0 to 1\n",
    "plt.yscale('log')\n",
    "plt.grid(True)\n",
    "plt.text(1948,2.6,\"Small\")                            #Add text to the plot\n",
    "plt.text(2008,6.2,\"Large\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function hist in module matplotlib.pyplot:\n",
      "\n",
      "hist(x, bins=None, range=None, density=False, weights=None, cumulative=False, bottom=None, histtype='bar', align='mid', orientation='vertical', rwidth=None, log=False, color=None, label=None, stacked=False, *, data=None, **kwargs)\n",
      "    Compute and plot a histogram.\n",
      "    \n",
      "    This method uses `numpy.histogram` to bin the data in *x* and count the\n",
      "    number of values in each bin, then draws the distribution either as a\n",
      "    `.BarContainer` or `.Polygon`. The *bins*, *range*, *density*, and\n",
      "    *weights* parameters are forwarded to `numpy.histogram`.\n",
      "    \n",
      "    If the data has already been binned and counted, use `~.bar` or\n",
      "    `~.stairs` to plot the distribution::\n",
      "    \n",
      "        counts, bins = np.histogram(x)\n",
      "        plt.stairs(counts, bins)\n",
      "    \n",
      "    Alternatively, plot pre-computed bins and counts using ``hist()`` by\n",
      "    treating each bin as a single point with a weight equal to its count::\n",
      "    \n",
      "        plt.hist(bins[:-1], bins, weights=counts)\n",
      "    \n",
      "    The data input *x* can be a singular array, a list of datasets of\n",
      "    potentially different lengths ([*x0*, *x1*, ...]), or a 2D ndarray in\n",
      "    which each column is a dataset. Note that the ndarray form is\n",
      "    transposed relative to the list form. If the input is an array, then\n",
      "    the return value is a tuple (*n*, *bins*, *patches*); if the input is a\n",
      "    sequence of arrays, then the return value is a tuple\n",
      "    ([*n0*, *n1*, ...], *bins*, [*patches0*, *patches1*, ...]).\n",
      "    \n",
      "    Masked arrays are not supported.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    x : (n,) array or sequence of (n,) arrays\n",
      "        Input values, this takes either a single array or a sequence of\n",
      "        arrays which are not required to be of the same length.\n",
      "    \n",
      "    bins : int or sequence or str, default: :rc:`hist.bins`\n",
      "        If *bins* is an integer, it defines the number of equal-width bins\n",
      "        in the range.\n",
      "    \n",
      "        If *bins* is a sequence, it defines the bin edges, including the\n",
      "        left edge of the first bin and the right edge of the last bin;\n",
      "        in this case, bins may be unequally spaced.  All but the last\n",
      "        (righthand-most) bin is half-open.  In other words, if *bins* is::\n",
      "    \n",
      "            [1, 2, 3, 4]\n",
      "    \n",
      "        then the first bin is ``[1, 2)`` (including 1, but excluding 2) and\n",
      "        the second ``[2, 3)``.  The last bin, however, is ``[3, 4]``, which\n",
      "        *includes* 4.\n",
      "    \n",
      "        If *bins* is a string, it is one of the binning strategies\n",
      "        supported by `numpy.histogram_bin_edges`: 'auto', 'fd', 'doane',\n",
      "        'scott', 'stone', 'rice', 'sturges', or 'sqrt'.\n",
      "    \n",
      "    range : tuple or None, default: None\n",
      "        The lower and upper range of the bins. Lower and upper outliers\n",
      "        are ignored. If not provided, *range* is ``(x.min(), x.max())``.\n",
      "        Range has no effect if *bins* is a sequence.\n",
      "    \n",
      "        If *bins* is a sequence or *range* is specified, autoscaling\n",
      "        is based on the specified bin range instead of the\n",
      "        range of x.\n",
      "    \n",
      "    density : bool, default: False\n",
      "        If ``True``, draw and return a probability density: each bin\n",
      "        will display the bin's raw count divided by the total number of\n",
      "        counts *and the bin width*\n",
      "        (``density = counts / (sum(counts) * np.diff(bins))``),\n",
      "        so that the area under the histogram integrates to 1\n",
      "        (``np.sum(density * np.diff(bins)) == 1``).\n",
      "    \n",
      "        If *stacked* is also ``True``, the sum of the histograms is\n",
      "        normalized to 1.\n",
      "    \n",
      "    weights : (n,) array-like or None, default: None\n",
      "        An array of weights, of the same shape as *x*.  Each value in\n",
      "        *x* only contributes its associated weight towards the bin count\n",
      "        (instead of 1).  If *density* is ``True``, the weights are\n",
      "        normalized, so that the integral of the density over the range\n",
      "        remains 1.\n",
      "    \n",
      "    cumulative : bool or -1, default: False\n",
      "        If ``True``, then a histogram is computed where each bin gives the\n",
      "        counts in that bin plus all bins for smaller values. The last bin\n",
      "        gives the total number of datapoints.\n",
      "    \n",
      "        If *density* is also ``True`` then the histogram is normalized such\n",
      "        that the last bin equals 1.\n",
      "    \n",
      "        If *cumulative* is a number less than 0 (e.g., -1), the direction\n",
      "        of accumulation is reversed.  In this case, if *density* is also\n",
      "        ``True``, then the histogram is normalized such that the first bin\n",
      "        equals 1.\n",
      "    \n",
      "    bottom : array-like, scalar, or None, default: None\n",
      "        Location of the bottom of each bin, i.e. bins are drawn from\n",
      "        ``bottom`` to ``bottom + hist(x, bins)`` If a scalar, the bottom\n",
      "        of each bin is shifted by the same amount. If an array, each bin\n",
      "        is shifted independently and the length of bottom must match the\n",
      "        number of bins. If None, defaults to 0.\n",
      "    \n",
      "    histtype : {'bar', 'barstacked', 'step', 'stepfilled'}, default: 'bar'\n",
      "        The type of histogram to draw.\n",
      "    \n",
      "        - 'bar' is a traditional bar-type histogram.  If multiple data\n",
      "          are given the bars are arranged side by side.\n",
      "        - 'barstacked' is a bar-type histogram where multiple\n",
      "          data are stacked on top of each other.\n",
      "        - 'step' generates a lineplot that is by default unfilled.\n",
      "        - 'stepfilled' generates a lineplot that is by default filled.\n",
      "    \n",
      "    align : {'left', 'mid', 'right'}, default: 'mid'\n",
      "        The horizontal alignment of the histogram bars.\n",
      "    \n",
      "        - 'left': bars are centered on the left bin edges.\n",
      "        - 'mid': bars are centered between the bin edges.\n",
      "        - 'right': bars are centered on the right bin edges.\n",
      "    \n",
      "    orientation : {'vertical', 'horizontal'}, default: 'vertical'\n",
      "        If 'horizontal', `~.Axes.barh` will be used for bar-type histograms\n",
      "        and the *bottom* kwarg will be the left edges.\n",
      "    \n",
      "    rwidth : float or None, default: None\n",
      "        The relative width of the bars as a fraction of the bin width.  If\n",
      "        ``None``, automatically compute the width.\n",
      "    \n",
      "        Ignored if *histtype* is 'step' or 'stepfilled'.\n",
      "    \n",
      "    log : bool, default: False\n",
      "        If ``True``, the histogram axis will be set to a log scale.\n",
      "    \n",
      "    color : color or array-like of colors or None, default: None\n",
      "        Color or sequence of colors, one per dataset.  Default (``None``)\n",
      "        uses the standard line color sequence.\n",
      "    \n",
      "    label : str or None, default: None\n",
      "        String, or sequence of strings to match multiple datasets.  Bar\n",
      "        charts yield multiple patches per dataset, but only the first gets\n",
      "        the label, so that `~.Axes.legend` will work as expected.\n",
      "    \n",
      "    stacked : bool, default: False\n",
      "        If ``True``, multiple data are stacked on top of each other If\n",
      "        ``False`` multiple data are arranged side by side if histtype is\n",
      "        'bar' or on top of each other if histtype is 'step'\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    n : array or list of arrays\n",
      "        The values of the histogram bins. See *density* and *weights* for a\n",
      "        description of the possible semantics.  If input *x* is an array,\n",
      "        then this is an array of length *nbins*. If input is a sequence of\n",
      "        arrays ``[data1, data2, ...]``, then this is a list of arrays with\n",
      "        the values of the histograms for each of the arrays in the same\n",
      "        order.  The dtype of the array *n* (or of its element arrays) will\n",
      "        always be float even if no weighting or normalization is used.\n",
      "    \n",
      "    bins : array\n",
      "        The edges of the bins. Length nbins + 1 (nbins left edges and right\n",
      "        edge of last bin).  Always a single array even when multiple data\n",
      "        sets are passed in.\n",
      "    \n",
      "    patches : `.BarContainer` or list of a single `.Polygon` or list of such objects\n",
      "        Container of individual artists used to create the histogram\n",
      "        or list of such containers if there are multiple input datasets.\n",
      "    \n",
      "    Other Parameters\n",
      "    ----------------\n",
      "    data : indexable object, optional\n",
      "        If given, the following parameters also accept a string ``s``, which is\n",
      "        interpreted as ``data[s]`` (unless this raises an exception):\n",
      "    \n",
      "        *x*, *weights*\n",
      "    \n",
      "    **kwargs\n",
      "        `~matplotlib.patches.Patch` properties\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    hist2d : 2D histogram with rectangular bins\n",
      "    hexbin : 2D histogram with hexagonal bins\n",
      "    stairs : Plot a pre-computed histogram\n",
      "    bar : Plot a pre-computed histogram\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    For large numbers of bins (>1000), plotting can be significantly\n",
      "    accelerated by using `~.Axes.stairs` to plot a pre-computed histogram\n",
      "    (``plt.stairs(*np.histogram(data))``), or by setting *histtype* to\n",
      "    'step' or 'stepfilled' rather than 'bar' or 'barstacked'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(plt.hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(pop,bins=3)\n",
    "plt.show() #show plot\n",
    "plt.clf()  #cleanup plot\n",
    "plt.hist(pop,bins=5)\n",
    "plt.show() #show plot\n",
    "plt.clf()  #cleanup plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function scatter in module matplotlib.pyplot:\n",
      "\n",
      "scatter(x, y, s=None, c=None, marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=None, linewidths=None, *, edgecolors=None, plotnonfinite=False, data=None, **kwargs)\n",
      "    A scatter plot of *y* vs. *x* with varying marker size and/or color.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    x, y : float or array-like, shape (n, )\n",
      "        The data positions.\n",
      "    \n",
      "    s : float or array-like, shape (n, ), optional\n",
      "        The marker size in points**2 (typographic points are 1/72 in.).\n",
      "        Default is ``rcParams['lines.markersize'] ** 2``.\n",
      "    \n",
      "    c : array-like or list of colors or color, optional\n",
      "        The marker colors. Possible values:\n",
      "    \n",
      "        - A scalar or sequence of n numbers to be mapped to colors using\n",
      "          *cmap* and *norm*.\n",
      "        - A 2D array in which the rows are RGB or RGBA.\n",
      "        - A sequence of colors of length n.\n",
      "        - A single color format string.\n",
      "    \n",
      "        Note that *c* should not be a single numeric RGB or RGBA sequence\n",
      "        because that is indistinguishable from an array of values to be\n",
      "        colormapped. If you want to specify the same RGB or RGBA value for\n",
      "        all points, use a 2D array with a single row.  Otherwise,\n",
      "        value-matching will have precedence in case of a size matching with\n",
      "        *x* and *y*.\n",
      "    \n",
      "        If you wish to specify a single color for all points\n",
      "        prefer the *color* keyword argument.\n",
      "    \n",
      "        Defaults to `None`. In that case the marker color is determined\n",
      "        by the value of *color*, *facecolor* or *facecolors*. In case\n",
      "        those are not specified or `None`, the marker color is determined\n",
      "        by the next color of the ``Axes``' current \"shape and fill\" color\n",
      "        cycle. This cycle defaults to :rc:`axes.prop_cycle`.\n",
      "    \n",
      "    marker : `~.markers.MarkerStyle`, default: :rc:`scatter.marker`\n",
      "        The marker style. *marker* can be either an instance of the class\n",
      "        or the text shorthand for a particular marker.\n",
      "        See :mod:`matplotlib.markers` for more information about marker\n",
      "        styles.\n",
      "    \n",
      "    cmap : str or `~matplotlib.colors.Colormap`, default: :rc:`image.cmap`\n",
      "        The Colormap instance or registered colormap name used to map scalar data\n",
      "        to colors.\n",
      "    \n",
      "        This parameter is ignored if *c* is RGB(A).\n",
      "    \n",
      "    norm : str or `~matplotlib.colors.Normalize`, optional\n",
      "        The normalization method used to scale scalar data to the [0, 1] range\n",
      "        before mapping to colors using *cmap*. By default, a linear scaling is\n",
      "        used, mapping the lowest value to 0 and the highest to 1.\n",
      "    \n",
      "        If given, this can be one of the following:\n",
      "    \n",
      "        - An instance of `.Normalize` or one of its subclasses\n",
      "          (see :doc:`/tutorials/colors/colormapnorms`).\n",
      "        - A scale name, i.e. one of \"linear\", \"log\", \"symlog\", \"logit\", etc.  For a\n",
      "          list of available scales, call `matplotlib.scale.get_scale_names()`.\n",
      "          In that case, a suitable `.Normalize` subclass is dynamically generated\n",
      "          and instantiated.\n",
      "    \n",
      "        This parameter is ignored if *c* is RGB(A).\n",
      "    \n",
      "    vmin, vmax : float, optional\n",
      "        When using scalar data and no explicit *norm*, *vmin* and *vmax* define\n",
      "        the data range that the colormap covers. By default, the colormap covers\n",
      "        the complete value range of the supplied data. It is an error to use\n",
      "        *vmin*/*vmax* when a *norm* instance is given (but using a `str` *norm*\n",
      "        name together with *vmin*/*vmax* is acceptable).\n",
      "    \n",
      "        This parameter is ignored if *c* is RGB(A).\n",
      "    \n",
      "    alpha : float, default: None\n",
      "        The alpha blending value, between 0 (transparent) and 1 (opaque).\n",
      "    \n",
      "    linewidths : float or array-like, default: :rc:`lines.linewidth`\n",
      "        The linewidth of the marker edges. Note: The default *edgecolors*\n",
      "        is 'face'. You may want to change this as well.\n",
      "    \n",
      "    edgecolors : {'face', 'none', *None*} or color or sequence of color, default: :rc:`scatter.edgecolors`\n",
      "        The edge color of the marker. Possible values:\n",
      "    \n",
      "        - 'face': The edge color will always be the same as the face color.\n",
      "        - 'none': No patch boundary will be drawn.\n",
      "        - A color or sequence of colors.\n",
      "    \n",
      "        For non-filled markers, *edgecolors* is ignored. Instead, the color\n",
      "        is determined like with 'face', i.e. from *c*, *colors*, or\n",
      "        *facecolors*.\n",
      "    \n",
      "    plotnonfinite : bool, default: False\n",
      "        Whether to plot points with nonfinite *c* (i.e. ``inf``, ``-inf``\n",
      "        or ``nan``). If ``True`` the points are drawn with the *bad*\n",
      "        colormap color (see `.Colormap.set_bad`).\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    `~matplotlib.collections.PathCollection`\n",
      "    \n",
      "    Other Parameters\n",
      "    ----------------\n",
      "    data : indexable object, optional\n",
      "        If given, the following parameters also accept a string ``s``, which is\n",
      "        interpreted as ``data[s]`` (unless this raises an exception):\n",
      "    \n",
      "        *x*, *y*, *s*, *linewidths*, *edgecolors*, *c*, *facecolor*, *facecolors*, *color*\n",
      "    **kwargs : `~matplotlib.collections.Collection` properties\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    plot : To plot scatter plots when markers are identical in size and\n",
      "        color.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    * The `.plot` function will be faster for scatterplots where markers\n",
      "      don't vary in size or color.\n",
      "    \n",
      "    * Any or all of *x*, *y*, *s*, and *c* may be masked arrays, in which\n",
      "      case all masks will be combined and only unmasked points will be\n",
      "      plotted.\n",
      "    \n",
      "    * Fundamentally, scatter works with 1D arrays; *x*, *y*, *s*, and *c*\n",
      "      may be input as N-D arrays, but within scatter they will be\n",
      "      flattened. The exception is *c*, which will be flattened only if its\n",
      "      size matches the size of *x* and *y*.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(plt.scatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A1': 'B1', 'A2': 'B2', 'A3': 'B3', 'A4': 'B4'}\n",
      "{'A2': 'B2', 'A3': 'B3', 'A4': 'B4'}\n",
      "dict_keys(['A2', 'A3', 'A4'])\n",
      "dict_values(['B2', 'B3', 'B4'])\n",
      "dict_items([('A2', 'B2'), ('A3', 'B3'), ('A4', 'B4')])\n"
     ]
    }
   ],
   "source": [
    "AA = {'A1':'B1','A2':'B2'}          #Definnig dictionary\n",
    "AA['A3'] = 'B3'                     #Calling and adding a value by the key\n",
    "AA['A4'] = 'B4'\n",
    "print(AA)\n",
    "del(AA['A1'])                       #Deleting an element in the dictionary\n",
    "print(AA)\n",
    "print(AA.keys())                    #Printing all keys\n",
    "print(AA.values())                  #Printing all values\n",
    "print(AA.items())                   #Printing all keys and values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BB = {'B1':{'C1':'D1','C2':'D2'},'B3':{'C3':'D3','C4':'D4'}}    #Dic in dic\n",
    "BB['B1']['C2']                                                  #Calling a value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      CarName\n",
      "AR  alfaRomeo\n",
      "NI     Nissan\n",
      "TO     Toyota\n",
      "BM        BMW\n",
      "BE       Benz\n",
      "JE       Jeep\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MPG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>225.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median</th>\n",
       "      <td>225.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          MPG\n",
       "mean    225.0\n",
       "median  225.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D1       = ['alfaRomeo','Nissan','Toyota','BMW','Benz','Jeep']\n",
    "D2       = [100,150,200,250,300,350]\n",
    "D3       = [False,False,True,True,True,False]\n",
    "Dic1     = {'CarName':D1 ,'MPG':D2, \"Sport?\":D3}               #Definnig a dic\n",
    "Cars     = pd.DataFrame(Dic1)                                  #Definnig Datafram using dic\n",
    "RowLabels= ['AR','NI','TO','BM','BE','JE']\n",
    "Cars.index = RowLabels                                         #Replacing string instead of integer index\n",
    "print(Cars[[\"CarName\"]])                                       #Getting a specific column\n",
    "Cars.agg({'MPG':['mean','median']})                            #Aggregating using \"agg\" method to calculate \"mean\" and \"median\" of a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">MPG</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sport?</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>200.0</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>250.0</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          MPG       \n",
       "         mean median\n",
       "Sport?              \n",
       "False   200.0  150.0\n",
       "True    250.0  250.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sport_Car_Group = Cars.groupby(['Sport?'])                        #Grouping using pandas \"groupby\" method\n",
    "Sport_Summary = Sport_Car_Group.agg({'MPG':['mean','median']})    #Aggregating using \"agg\" method to calculate \"mean\" and \"median\" of a column\n",
    "Sport_Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <DataFrameName>['ColumnName']   ===> Outputs Pandas DataSerie\n",
    "# <DataFrameName>[['ColumnName']] ===> Outputs Pandas DataFrame\n",
    "# For one row or one column you can use DataSerie or DataFrame\n",
    "# For more than one row or one column you can only use DataFrame\n",
    "# At the end of a Serie always there is a row indicating \"Name\" and \"dtype\" of the DataSerie "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "    MPG  Sport?\n",
      "AR  100   False\n",
      "    MPG  Sport?\n",
      "AR  100   False\n",
      "NI  150   False\n",
      "    MPG  Sport?\n",
      "AR  100   False\n",
      "NI  150   False\n",
      "TO  200    True\n",
      "BM  250    True\n",
      "BE  300    True\n",
      "JE  350   False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CarName</th>\n",
       "      <th>MPG</th>\n",
       "      <th>Sport?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>alfaRomeo</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NI</th>\n",
       "      <td>Nissan</td>\n",
       "      <td>150</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TO</th>\n",
       "      <td>Toyota</td>\n",
       "      <td>200</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BM</th>\n",
       "      <td>BMW</td>\n",
       "      <td>250</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BE</th>\n",
       "      <td>Benz</td>\n",
       "      <td>300</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JE</th>\n",
       "      <td>Jeep</td>\n",
       "      <td>350</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CarName  MPG  Sport?\n",
       "AR  alfaRomeo  100   False\n",
       "NI     Nissan  150   False\n",
       "TO     Toyota  200    True\n",
       "BM        BMW  250    True\n",
       "BE       Benz  300    True\n",
       "JE       Jeep  350   False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting specific rows and columns based on index labels outputing DataSerie\n",
    "print(Cars.loc['AR','MPG'])\n",
    "# Selecting specific rows and columns based on index labels outputing DataFrame\n",
    "print(Cars.loc[['AR'],['MPG','Sport?']])\n",
    "print(Cars.loc[['AR','NI'],['MPG','Sport?']])\n",
    "# Selecting All rows of specific columns based on index labels outputing DataFrame\n",
    "print(Cars.loc[:,['MPG','Sport?']])\n",
    "Cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "    MPG  Sport?\n",
      "AR  100   False\n",
      "    MPG  Sport?\n",
      "AR  100   False\n",
      "NI  150   False\n",
      "    MPG  Sport?\n",
      "AR  100   False\n",
      "NI  150   False\n",
      "TO  200    True\n",
      "BM  250    True\n",
      "BE  300    True\n",
      "JE  350   False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CarName</th>\n",
       "      <th>MPG</th>\n",
       "      <th>Sport?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>alfaRomeo</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NI</th>\n",
       "      <td>Nissan</td>\n",
       "      <td>150</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TO</th>\n",
       "      <td>Toyota</td>\n",
       "      <td>200</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BM</th>\n",
       "      <td>BMW</td>\n",
       "      <td>250</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BE</th>\n",
       "      <td>Benz</td>\n",
       "      <td>300</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JE</th>\n",
       "      <td>Jeep</td>\n",
       "      <td>350</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CarName  MPG  Sport?\n",
       "AR  alfaRomeo  100   False\n",
       "NI     Nissan  150   False\n",
       "TO     Toyota  200    True\n",
       "BM        BMW  250    True\n",
       "BE       Benz  300    True\n",
       "JE       Jeep  350   False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting specific rows and columns based on position outputing DataSerie\n",
    "print(Cars.iloc[0,1])\n",
    "# Selecting specific rows and columns based on position outputing DataFrame\n",
    "print(Cars.iloc[[0],[1,2]])\n",
    "print(Cars.iloc[[0,1],[1,2]])\n",
    "# Selecting All rows of specific columns based on position outputing DataFrame\n",
    "print(Cars.iloc[:,[1,2]])\n",
    "Cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CarName  MPG  Sport?\n",
      "TO  Toyota  200    True\n",
      "BM     BMW  250    True\n",
      "BE    Benz  300    True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CarName</th>\n",
       "      <th>MPG</th>\n",
       "      <th>Sport?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>alfaRomeo</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NI</th>\n",
       "      <td>Nissan</td>\n",
       "      <td>150</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TO</th>\n",
       "      <td>Toyota</td>\n",
       "      <td>200</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BM</th>\n",
       "      <td>BMW</td>\n",
       "      <td>250</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BE</th>\n",
       "      <td>Benz</td>\n",
       "      <td>300</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JE</th>\n",
       "      <td>Jeep</td>\n",
       "      <td>350</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CarName  MPG  Sport?\n",
       "AR  alfaRomeo  100   False\n",
       "NI     Nissan  150   False\n",
       "TO     Toyota  200    True\n",
       "BM        BMW  250    True\n",
       "BE       Benz  300    True\n",
       "JE       Jeep  350   False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Filter1   = Cars[\"Sport?\"] == True              # Filtering a Dataframe (Type 1)\n",
    "SportCars = Cars[Filter1]\n",
    "print(SportCars)\n",
    "Cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CarName  MPG  Sport?\n",
      "BM     BMW  250    True\n",
      "BE    Benz  300    True\n",
      "JE    Jeep  350   False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CarName</th>\n",
       "      <th>MPG</th>\n",
       "      <th>Sport?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>alfaRomeo</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NI</th>\n",
       "      <td>Nissan</td>\n",
       "      <td>150</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TO</th>\n",
       "      <td>Toyota</td>\n",
       "      <td>200</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BM</th>\n",
       "      <td>BMW</td>\n",
       "      <td>250</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BE</th>\n",
       "      <td>Benz</td>\n",
       "      <td>300</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JE</th>\n",
       "      <td>Jeep</td>\n",
       "      <td>350</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CarName  MPG  Sport?\n",
       "AR  alfaRomeo  100   False\n",
       "NI     Nissan  150   False\n",
       "TO     Toyota  200    True\n",
       "BM        BMW  250    True\n",
       "BE       Benz  300    True\n",
       "JE       Jeep  350   False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Filter2      = Cars['MPG'] > 200                   # Filtering a Dataframe (Type 2)\n",
    "High_MPG_Car = Cars[Filter2 == True]\n",
    "print(High_MPG_Car)\n",
    "Cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CarName  MPG  Sport?\n",
      "TO  Toyota  200    True\n",
      "BM     BMW  250    True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CarName</th>\n",
       "      <th>MPG</th>\n",
       "      <th>Sport?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>alfaRomeo</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NI</th>\n",
       "      <td>Nissan</td>\n",
       "      <td>150</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TO</th>\n",
       "      <td>Toyota</td>\n",
       "      <td>200</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BM</th>\n",
       "      <td>BMW</td>\n",
       "      <td>250</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BE</th>\n",
       "      <td>Benz</td>\n",
       "      <td>300</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JE</th>\n",
       "      <td>Jeep</td>\n",
       "      <td>350</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CarName  MPG  Sport?\n",
       "AR  alfaRomeo  100   False\n",
       "NI     Nissan  150   False\n",
       "TO     Toyota  200    True\n",
       "BM        BMW  250    True\n",
       "BE       Benz  300    True\n",
       "JE       Jeep  350   False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MPG_Value    = Cars['MPG']\n",
    "# Multiple Filtering using \"np.logical_(and/or/not)\"\n",
    "Filter3      = np.logical_and(MPG_Value > 150 , MPG_Value < 300)\n",
    "MidLevelCars = Cars[Filter3]\n",
    "print(MidLevelCars)\n",
    "Cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False]\n",
      "[ True  True  True  True]\n",
      "[False False False False]\n"
     ]
    }
   ],
   "source": [
    "C1 = np.array([1,10,100,1000])              #Definning a list (which is called \"Array\" in numpy)\n",
    "C2 = np.array([2,20,200,2000])\n",
    "print(np.logical_and(C1 < C2, 4*C1 <= C2))  #np.logical_and is \"and\" in numpy\n",
    "print(np.logical_or(C1 < C2, 4*C1 <= C2))   #np.logical_or  is  \"or\" in numpy\n",
    "print(np.logical_not(C1 < C2, 4*C1 <= C2))  #np.logical_not is \"not\" in numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number is the multiply of 3\n"
     ]
    }
   ],
   "source": [
    "D = 405\n",
    "if   D % 2 == 0 :                                    # if structure conatining elif and else\n",
    "     print(\"The number is the multiply of 2\")\n",
    "elif D % 3 == 0 :\n",
    "     print(\"The number is the multiply of 3\")\n",
    "elif D % 5 == 0 :\n",
    "     print(\"The number is the multiply of 5\")\n",
    "else :\n",
    "     print(\"The number is unknown\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number is 20\n",
      "The number is 19\n",
      "The number is 18\n",
      "The number is 17\n",
      "The number is 16\n",
      "The number is 15\n",
      "The number is 14\n",
      "The number is 13\n",
      "The number is 12\n",
      "The number is 11\n"
     ]
    }
   ],
   "source": [
    "initial_value = 20\n",
    "while initial_value > 10 :                          # While structure\n",
    "    print(\"The number is\" ,initial_value)\n",
    "    initial_value = initial_value - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Michael\n",
      "MICHAEL\n",
      "michael\n"
     ]
    }
   ],
   "source": [
    "# ____ (four space = indent)\n",
    "B = 2\n",
    "C = str(B)                         # Change type to string\n",
    "E = \"michael\"\n",
    "print(E.capitalize())              # Capitalize first letter\n",
    "print(E.upper())                   # Upper all letter\n",
    "print(E.lower())                   # Lower all letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Jack\n",
      "1 joe\n",
      "2 john\n",
      "3 jesica\n"
     ]
    }
   ],
   "source": [
    "D = [\"Jack\",\"joe\",\"john\",\"jesica\"]      \n",
    "# for structure\n",
    "for index , item in enumerate(D) :      # enumerate makes a pair of (indexs,items) of the list\n",
    "    print(index,item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The height of Jack is 160\n",
      "The height of joe is 165\n",
      "The height of john is 170\n",
      "The height of jesica is 175\n"
     ]
    }
   ],
   "source": [
    "F = [[\"Jack\", 160],[\"joe\",165],[\"john\",170],[\"jesica\",175]]\n",
    "  # Selecting elements of a list in a list \n",
    "for x , y in F :\n",
    "    print(\"The height of\", str(x),\"is\", str(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A2 B2\n",
      "A3 B3\n",
      "A4 B4\n",
      "{'A2': 'B2', 'A3': 'B3', 'A4': 'B4'}\n"
     ]
    }
   ],
   "source": [
    "# Selecting keys and values of dictionary using \"items()\" method\n",
    "for key , val in AA.items() :           \n",
    "    print(key,val)\n",
    "print(AA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "height      = np.array([100,150,200,250])\n",
    "weight      = np.array([60,70,80,90])\n",
    "combination = np.array([height,weight])\n",
    "# Selecting 2D numpy array elements by using \"np.nditer\" numpy function\n",
    "for val in np.nditer(combination) :      \n",
    "    print(val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AR :  alfaRomeo\n",
      "NI :  Nissan\n",
      "TO :  Toyota\n",
      "BM :  BMW\n",
      "BE :  Benz\n",
      "JE :  Jeep\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CarName</th>\n",
       "      <th>MPG</th>\n",
       "      <th>Sport?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>alfaRomeo</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NI</th>\n",
       "      <td>Nissan</td>\n",
       "      <td>150</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TO</th>\n",
       "      <td>Toyota</td>\n",
       "      <td>200</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BM</th>\n",
       "      <td>BMW</td>\n",
       "      <td>250</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BE</th>\n",
       "      <td>Benz</td>\n",
       "      <td>300</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JE</th>\n",
       "      <td>Jeep</td>\n",
       "      <td>350</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CarName  MPG  Sport?\n",
       "AR  alfaRomeo  100   False\n",
       "NI     Nissan  150   False\n",
       "TO     Toyota  200    True\n",
       "BM        BMW  250    True\n",
       "BE       Benz  300    True\n",
       "JE       Jeep  350   False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting elements of panda DataFrame using \"iterrows()\" method (mention the s at the end)\n",
    "for label , row in Cars.iterrows():\n",
    "    print(label,\": \",row[\"CarName\"])\n",
    "Cars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CarName</th>\n",
       "      <th>MPG</th>\n",
       "      <th>Sport?</th>\n",
       "      <th>CarNameLenght</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>alfaRomeo</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NI</th>\n",
       "      <td>Nissan</td>\n",
       "      <td>150</td>\n",
       "      <td>False</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TO</th>\n",
       "      <td>Toyota</td>\n",
       "      <td>200</td>\n",
       "      <td>True</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BM</th>\n",
       "      <td>BMW</td>\n",
       "      <td>250</td>\n",
       "      <td>True</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BE</th>\n",
       "      <td>Benz</td>\n",
       "      <td>300</td>\n",
       "      <td>True</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JE</th>\n",
       "      <td>Jeep</td>\n",
       "      <td>350</td>\n",
       "      <td>False</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CarName  MPG  Sport?  CarNameLenght\n",
       "AR  alfaRomeo  100   False            9.0\n",
       "NI     Nissan  150   False            6.0\n",
       "TO     Toyota  200    True            6.0\n",
       "BM        BMW  250    True            3.0\n",
       "BE       Benz  300    True            4.0\n",
       "JE       Jeep  350   False            4.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for label , row in Cars.iterrows() :\n",
    "#Creating a new column by using \"loc\" function to measure car name lenght using by \"len()\" function (Method 1)\n",
    "    Cars.loc[label,\"CarNameLenght\"] = len(row[\"CarName\"])\n",
    "#Cars = Cars.drop(columns=\"CarNameLenght\")\n",
    "Cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CarName</th>\n",
       "      <th>MPG</th>\n",
       "      <th>Sport?</th>\n",
       "      <th>CarNameLenght</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>alfaRomeo</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NI</th>\n",
       "      <td>Nissan</td>\n",
       "      <td>150</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TO</th>\n",
       "      <td>Toyota</td>\n",
       "      <td>200</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BM</th>\n",
       "      <td>BMW</td>\n",
       "      <td>250</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BE</th>\n",
       "      <td>Benz</td>\n",
       "      <td>300</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JE</th>\n",
       "      <td>Jeep</td>\n",
       "      <td>350</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CarName  MPG  Sport?  CarNameLenght\n",
       "AR  alfaRomeo  100   False              9\n",
       "NI     Nissan  150   False              6\n",
       "TO     Toyota  200    True              6\n",
       "BM        BMW  250    True              3\n",
       "BE       Benz  300    True              4\n",
       "JE       Jeep  350   False              4"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating a new column by using by \"apply()\" function without using for loop (Method 2 - better than Method 1)\n",
    "Cars[\"CarNameLenght\"] = Cars[\"CarName\"].apply(len)\n",
    "#Cars = Cars.drop(columns=\"CarNameLenght\")\n",
    "Cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2717511889804102\n"
     ]
    }
   ],
   "source": [
    "# Generating Random number (like a tie or coin)\n",
    "print(np.random.rand())         #Genarating random number between [0,1] which changes all the time you run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6964691855978616\n"
     ]
    }
   ],
   "source": [
    "# Generating Random number (like a tie or coin)\n",
    "np.random.seed(123)             #Defining a fix seed for reproducing a same random number between [0,1] which won't change by running again\n",
    "print(np.random.rand())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Dice number is 5\n",
      "Coin is Head\n"
     ]
    }
   ],
   "source": [
    "# Generating Random number (like a dice or coin)\n",
    "print(np.random.randint(1,10))  #Definnig a random integer number start from 1 to 9\n",
    "\n",
    "dice = np.random.randint(1,7)   #Definnig a random dice\n",
    "print(\"Dice number is\",dice)\n",
    "\n",
    "coin = np.random.randint(0,2)   #Definnig a random coin\n",
    "if coin == 0 :\n",
    "    print(\"Coin is Head\")\n",
    "else :\n",
    "    print(\"Coin is Tail\") \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice number is 6\n",
      "Current Step is 53\n"
     ]
    }
   ],
   "source": [
    "# Simulating a dice game:\n",
    "\n",
    "# you are in 50th floor of a building. throw a dice. \n",
    "# if [1,2] go one floor down.\n",
    "# if [3,5] go one floor up.\n",
    "# if 6 throw the dice again and go that number up.\n",
    "\n",
    "np.random.seed(123)\n",
    "Dice = np.random.randint(1,7)       #Difinnig a fix Dice\n",
    "Step = 50                           #Initial Value of Step\n",
    "if Dice <= 2 :\n",
    "    Step = Step - 1\n",
    "if Dice <= 5 :\n",
    "    Step = Step + 1\n",
    "else :\n",
    "    Step = Step + np.random.randint(1,7)\n",
    "print(\"Dice number is\",Dice)\n",
    "print(\"Current Step is\",Step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating a dice game:\n",
    "\n",
    "# throw a coin for 20 times and save the results\n",
    "Result = []\n",
    "np.random.seed(123)\n",
    "for i in range(20) :\n",
    "    Coin = np.random.randint(0,2)\n",
    "    if Coin == 0 :\n",
    "        Result.append(\"Head\")           #Adding into the list by using \"append()\" method\n",
    "    else :\n",
    "        Result.append(\"Tail\")\n",
    "print(\"Result =\", Result)\n",
    "\n",
    "#Adding a plot\n",
    "plt.xticks(range(20))\n",
    "plt.title(\"Result plot\")\n",
    "plt.xlabel(\"Turn\")\n",
    "plt.ylabel(\"Coin\")\n",
    "plt.plot(Result, c='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating a dice game (Random Walk):\n",
    "\n",
    "# throw a dice (for 20 times) and go upside of a building by defined algorithm: \n",
    "Random_Walk = [0]\n",
    "np.random.seed(123)\n",
    "for i in range(20) :\n",
    "    Step = Random_Walk[-1]                    #Last item of Random_Walk should be the new Step (Steps are related to the last Random_Walk position)\n",
    "    Dice = np.random.randint(1,7)             #Definnig a Dice\n",
    "    if   Dice <= 2 :\n",
    "         Step = max(0 , Step - 1)             #Step can't go beyond 0 (bulding doesn't have minus floors)\n",
    "    elif Dice <= 5 :\n",
    "         Step = Step + 1\n",
    "    else           :\n",
    "         Step = Step + np.random.randint(1,7)     \n",
    "    Random_Walk.append(Step)                  #Updating Random_Walk position\n",
    "print(\"Random Walk =\", Random_Walk)\n",
    "\n",
    "#Adding a plot\n",
    "plt.title(\"Random Walking plot\")\n",
    "plt.xlabel(\"Turn\")\n",
    "plt.ylabel(\"Random Walking Result\")\n",
    "plt.hist(Random_Walk)                   \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Walk Distribution:\n",
    "\n",
    "# Simulate random walk 20 times \n",
    "all_Walks = []\n",
    "np.random.seed(123)\n",
    "for i in range(20) :\n",
    "    random_walk = [0]\n",
    "    for x in range(100) :\n",
    "        Step = Random_Walk[-1]                    #Last item of Random_Walk should be the new Step (Steps are related to the last Random_Walk position)\n",
    "        Dice = np.random.randint(1,7)             #Definnig a Dice\n",
    "        if   Dice <= 2 :\n",
    "            Step = max(0 , Step - 1)             #Step can't go beyond 0 (bulding doesn't have minus floors)\n",
    "        elif Dice <= 5 :\n",
    "            Step = Step + 1\n",
    "        else           :\n",
    "         Step = Step + np.random.randint(1,7)\n",
    "        if np.random.rand() <= 0.001 :\n",
    "            step=0\n",
    "        Random_Walk.append(Step)                  #Updating Random_Walk position\n",
    "        print(Random_Walk)\n",
    "    all_Walks.append(random_walk)\n",
    "print(all_Walks)\n",
    "np_aw_t = np.transpose(np.array(all_Walks))\n",
    "end = np_aw_t[-1]\n",
    "#Adding a plot\n",
    "plt.hist(end)                   \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-03\n",
      "2023\n",
      "9\n",
      "3\n",
      "Sunday\n",
      "Sun\n",
      "2000-02-02\n",
      "2023-09-03 14:32:42.255455\n",
      "14:32:42.255455\n",
      "14:32:42\n",
      "14:32\n",
      "02 PM\n",
      "14\n",
      "32\n",
      "42\n",
      "255455\n",
      "12:30:00\n",
      "2023-09-02\n",
      "2023-09-04\n",
      "2024-09-03\n",
      "366 days, 0:00:00\n"
     ]
    }
   ],
   "source": [
    "# Python datetime module:\n",
    "\n",
    "#Importing classes from module \"datetime\":\n",
    "from datetime import date, datetime, time, timedelta \n",
    "\n",
    "current_date        = date.today()                           #Current date\n",
    "current_year        = date.today().year                      #Current year\n",
    "current_month       = date.today().month                     #Current month\n",
    "current_day         = date.today().day                       #Current day\n",
    "current_day_name    = datetime.now().strftime(\"%A\")          #Current day name (long)\n",
    "current_day_name2   = datetime.now().strftime(\"%a\")          #Current day name (short)\n",
    "my_date             = date(2000,2,2)                         #Convert to date\n",
    "\n",
    "now                 = datetime.now()                         #Current date and time\n",
    "current_time        = datetime.now().time()                  #Current time with microsecond\n",
    "current_time2       = datetime.now().strftime(\"%H:%M:%S\")    #Current time without microsecond\n",
    "current_time3       = datetime.now().strftime(\"%H:%M\")       #Current time without second\n",
    "current_time4       = datetime.now().strftime(\"%I %p\")       #Current time (using AM/PM)\n",
    "current_hour        = datetime.now().hour                    #Current hour              \n",
    "current_minute      = datetime.now().minute                  #Current minute\n",
    "current_second      = datetime.now().second                  #Current second\n",
    "current_microsecond = datetime.now().microsecond             #Current microsecond\n",
    "my_time             = time(12,30,00)                         #Convert to time\n",
    "\n",
    "yesterday_date      = date.today() - timedelta(days=1)       #Yesterday date\n",
    "tommorow_date       = date.today() + timedelta(days=1)       #Tommorow date\n",
    "one_year_later_date = date.today() + timedelta(days=366)     #One year later\n",
    "\n",
    "max_lapse_date = current_date - timedelta(days=7)       #Remove users who lapsed till 7 days ago\n",
    "\n",
    "print(current_date)\n",
    "print(current_year)\n",
    "print(current_month)\n",
    "print(current_day)\n",
    "print(current_day_name)\n",
    "print(current_day_name2)\n",
    "print(my_date)\n",
    "print(now)\n",
    "print(current_time)\n",
    "print(current_time2)\n",
    "print(current_time3)\n",
    "print(current_time4)\n",
    "print(current_hour)\n",
    "print(current_minute)\n",
    "print(current_second)\n",
    "print(current_microsecond)\n",
    "print(my_time)\n",
    "print(yesterday_date)\n",
    "print(tommorow_date)\n",
    "print(one_year_later_date)\n",
    "print(one_year_later_date - current_date)                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((16, 25), (64, 125))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Functions:\n",
    " \n",
    "# Docstring describes what your function does\n",
    "# Docstring is placed after function header with triple double quotes\n",
    "# \"\"\"Docstring\"\"\" :\n",
    "\n",
    "# Defining a function without parameter\n",
    "def A1() :                                      #<Function header>\n",
    "    \"\"\"Calculating the square of 4\"\"\"           #Docstring\n",
    "    new_value = 4 **2                           #Function body\n",
    "    print(new_value)\n",
    "A1()\n",
    "\n",
    "# Defining a function with one parameter \n",
    "def A2(value) :\n",
    "    \"\"\"Calculating the square of a value\"\"\"   \n",
    "    new_value = value ** 2\n",
    "    print(new_value)\n",
    "A2(5)\n",
    "\n",
    "# Defining a function with one parameter to return one value\n",
    "def A3(value) :\n",
    "    \"\"\"Returns the square of a value\"\"\"\n",
    "    new_value = value ** 2\n",
    "    return new_value\n",
    "A3(6)\n",
    "\n",
    "# Defining a function with multiple parameters to return multiple values\n",
    "def A4(value1,value2) :\n",
    "    \"\"\"Returns the power of 2 and 3 of the two values\"\"\"\n",
    "    power2_value1  = value1 ** 2\n",
    "    power2_value2  = value2 ** 2\n",
    "    power3_value1  = value1 ** 3\n",
    "    power3_value2  = value2 ** 3\n",
    "    power2_tuple  = (power2_value1, power2_value2)\n",
    "    power3_tuple  = (power3_value1, power3_value2)\n",
    "    return power2_tuple , power3_tuple\n",
    "A4(4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alfaRomeo': 1, 'Nissan': 1, 'Toyota': 1, 'BMW': 1, 'Benz': 1, 'Jeep': 1, 100: 1, 150: 1, 200: 1, 250: 1, 300: 1, 350: 1, False: 3, True: 3}\n"
     ]
    }
   ],
   "source": [
    "# Defining a function \n",
    "# to get DataFrame and unlimited number of columns as arguments \n",
    "# and counts the number of entries as output\n",
    "# Google it => More data science practical function example\n",
    "\n",
    "def count_entries(df,*args):\n",
    "    \"\"\"Return a dictionary with counts of \n",
    "    occurrences as value for each key.\"\"\"\n",
    "    dic_cols_count = {}                                     # Initialize an empty dictionary: dic_cols_count\n",
    "    #if args not in df.columns:\n",
    "    #    raise ValueError(F'The DataFrame does not have a {args} column.')\n",
    "    try: \n",
    "        for i in args:                                          # Iterate over requested columns in DataFrame\n",
    "            col = df[i]                                         # Select one of requested columns\n",
    "            for i in col:                                       # Iterate over the rows of selected column\n",
    "                if i in dic_cols_count.keys():                  # Note that \"in\" is used in if clause\n",
    "                    dic_cols_count[i] = dic_cols_count[i] + 1   # If the value exists, add 1 to it\n",
    "                else:                               \n",
    "                    dic_cols_count[i] = 1                       # Else add it to keys and set the value to 1\n",
    "        #Return the dictionary\n",
    "        return dic_cols_count\n",
    "    except:\n",
    "        print(\"There was an error\")\n",
    "        \n",
    "#Test (using pre-defined Car DataFrame)\n",
    "Test1 = count_entries(Cars,'CarName','MPG','Sport?')\n",
    "print(Test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ArithmeticError',\n",
       " 'AssertionError',\n",
       " 'AttributeError',\n",
       " 'BaseException',\n",
       " 'BaseExceptionGroup',\n",
       " 'BlockingIOError',\n",
       " 'BrokenPipeError',\n",
       " 'BufferError',\n",
       " 'BytesWarning',\n",
       " 'ChildProcessError',\n",
       " 'ConnectionAbortedError',\n",
       " 'ConnectionError',\n",
       " 'ConnectionRefusedError',\n",
       " 'ConnectionResetError',\n",
       " 'DeprecationWarning',\n",
       " 'EOFError',\n",
       " 'Ellipsis',\n",
       " 'EncodingWarning',\n",
       " 'EnvironmentError',\n",
       " 'Exception',\n",
       " 'ExceptionGroup',\n",
       " 'False',\n",
       " 'FileExistsError',\n",
       " 'FileNotFoundError',\n",
       " 'FloatingPointError',\n",
       " 'FutureWarning',\n",
       " 'GeneratorExit',\n",
       " 'IOError',\n",
       " 'ImportError',\n",
       " 'ImportWarning',\n",
       " 'IndentationError',\n",
       " 'IndexError',\n",
       " 'InterruptedError',\n",
       " 'IsADirectoryError',\n",
       " 'KeyError',\n",
       " 'KeyboardInterrupt',\n",
       " 'LookupError',\n",
       " 'MemoryError',\n",
       " 'ModuleNotFoundError',\n",
       " 'NameError',\n",
       " 'None',\n",
       " 'NotADirectoryError',\n",
       " 'NotImplemented',\n",
       " 'NotImplementedError',\n",
       " 'OSError',\n",
       " 'OverflowError',\n",
       " 'PendingDeprecationWarning',\n",
       " 'PermissionError',\n",
       " 'ProcessLookupError',\n",
       " 'RecursionError',\n",
       " 'ReferenceError',\n",
       " 'ResourceWarning',\n",
       " 'RuntimeError',\n",
       " 'RuntimeWarning',\n",
       " 'StopAsyncIteration',\n",
       " 'StopIteration',\n",
       " 'SyntaxError',\n",
       " 'SyntaxWarning',\n",
       " 'SystemError',\n",
       " 'SystemExit',\n",
       " 'TabError',\n",
       " 'TimeoutError',\n",
       " 'True',\n",
       " 'TypeError',\n",
       " 'UnboundLocalError',\n",
       " 'UnicodeDecodeError',\n",
       " 'UnicodeEncodeError',\n",
       " 'UnicodeError',\n",
       " 'UnicodeTranslateError',\n",
       " 'UnicodeWarning',\n",
       " 'UserWarning',\n",
       " 'ValueError',\n",
       " 'Warning',\n",
       " 'WindowsError',\n",
       " 'ZeroDivisionError',\n",
       " '__IPYTHON__',\n",
       " '__build_class__',\n",
       " '__debug__',\n",
       " '__doc__',\n",
       " '__import__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__pybind11_internals_v4_mingw_libstdcpp_cxxabi1014__',\n",
       " '__spec__',\n",
       " 'abs',\n",
       " 'aiter',\n",
       " 'all',\n",
       " 'anext',\n",
       " 'any',\n",
       " 'ascii',\n",
       " 'bin',\n",
       " 'bool',\n",
       " 'breakpoint',\n",
       " 'bytearray',\n",
       " 'bytes',\n",
       " 'callable',\n",
       " 'chr',\n",
       " 'classmethod',\n",
       " 'compile',\n",
       " 'complex',\n",
       " 'copyright',\n",
       " 'credits',\n",
       " 'delattr',\n",
       " 'dict',\n",
       " 'dir',\n",
       " 'display',\n",
       " 'divmod',\n",
       " 'enumerate',\n",
       " 'eval',\n",
       " 'exec',\n",
       " 'execfile',\n",
       " 'filter',\n",
       " 'float',\n",
       " 'format',\n",
       " 'frozenset',\n",
       " 'get_ipython',\n",
       " 'getattr',\n",
       " 'globals',\n",
       " 'hasattr',\n",
       " 'hash',\n",
       " 'help',\n",
       " 'hex',\n",
       " 'id',\n",
       " 'input',\n",
       " 'int',\n",
       " 'isinstance',\n",
       " 'issubclass',\n",
       " 'iter',\n",
       " 'len',\n",
       " 'license',\n",
       " 'list',\n",
       " 'locals',\n",
       " 'map',\n",
       " 'max',\n",
       " 'memoryview',\n",
       " 'min',\n",
       " 'next',\n",
       " 'object',\n",
       " 'oct',\n",
       " 'open',\n",
       " 'ord',\n",
       " 'pow',\n",
       " 'print',\n",
       " 'property',\n",
       " 'range',\n",
       " 'repr',\n",
       " 'reversed',\n",
       " 'round',\n",
       " 'runfile',\n",
       " 'set',\n",
       " 'setattr',\n",
       " 'slice',\n",
       " 'sorted',\n",
       " 'staticmethod',\n",
       " 'str',\n",
       " 'sum',\n",
       " 'super',\n",
       " 'tuple',\n",
       " 'type',\n",
       " 'vars',\n",
       " 'zip']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#List of built ins keywords (reserved)\n",
    "import builtins\n",
    "dir(builtins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 64\n"
     ]
    }
   ],
   "source": [
    "#Nested function (inner function in outer funtion)\n",
    "\n",
    "def OuterFunc(n):\n",
    "    \"\"\"Create InnerFunc function and returns it\"\"\"\n",
    "    def InnerFunc(x):\n",
    "        \"\"\"Create InnerFunc which raises x to the power of pre-defined n\"\"\"\n",
    "        result = x ** n             #note that exponentiation and power sign is '**' not '^'\n",
    "        return result\n",
    "    return InnerFunc\n",
    "#Test\n",
    "Square = OuterFunc(2)\n",
    "Cube   = OuterFunc(3)\n",
    "print(Square(4),Cube(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function dict.keys>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Python LEGB Rule in resolving names,variables and parameters:\n",
    "\n",
    "#The priority is as follows:\n",
    "#1)Local Scope              (variable which is defined inside a function)\n",
    "#2)Enclosing/nonlocal Scope (variable which is defined inside outer function of two nested functions)\n",
    "#3)Global/module Scope      (variable which is defined outside of all functions)\n",
    "#4)Built-in Scope           (variable which is defined by means of python itself which is reserved)\n",
    "\n",
    "#Notes:\n",
    "#To use an enclosing variable of outer function in the inner function use \"nonlocal <variable_name>\" in the inner one.\n",
    "#To use a variable in all the code use \"global <variable_name>\"\n",
    "\n",
    "\n",
    "\n",
    "#google it => python namespace, __dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "HEY!!!\n"
     ]
    }
   ],
   "source": [
    "#Setting default arguments in defining a function\n",
    "\n",
    "def power(num1,num2=1):\n",
    "    \"\"\"Raise num1 to the power of num2\"\"\"\n",
    "    new_value = num1 ** num2\n",
    "    return new_value\n",
    "#Test\n",
    "Test1 = power(5)\n",
    "\n",
    "def shout_echo(word1, echo=1, intense=False):\n",
    "    \"\"\"Concatenate echo copies of word1\"\"\"\n",
    "    echo_word = word1 * echo\n",
    "    if intense==True:\n",
    "        echo_word_new = echo_word.upper() + '!!!'\n",
    "    else:\n",
    "        echo_word_new = echo_word + '!!!'\n",
    "    return echo_word_new\n",
    "#Test\n",
    "Test2 = shout_echo(\"Hey\",intense=True)\n",
    "#Note: if you skip a default argument, for the next argument you specify the name \n",
    "print(Test1)\n",
    "print(Test2)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: Ashkan Moradi\n",
      "job: Data Analyst\n",
      "275\n"
     ]
    }
   ],
   "source": [
    "#Setting flexible (Unlimited) arguments in defining a function\n",
    "\n",
    "def add_all(*args):     #note the single star before Arguments\n",
    "    \"\"\"Sum all values in *args tuple\"\"\"\n",
    "    sum_all = 0\n",
    "    for i in args:\n",
    "        sum_all = sum_all + i\n",
    "    return sum_all \n",
    "#Test\n",
    "Test1 = add_all(5,10,15,20,25,30,35,40,45,50)\n",
    "\n",
    "def print_all(**kwarg): #note the double star before KeyWordArguments\n",
    "    \"\"\"Print out key-value pairs in **kwargs dictionary\"\"\"\n",
    "    for key, value in kwarg.items():\n",
    "        result = key + \": \" + value\n",
    "        print(result)\n",
    "    #return is extra here because of print syntax\n",
    "#Test\n",
    "Test2 = print_all(name='Ashkan Moradi', job='Data Analyst')\n",
    "print(Test1)\n",
    "Test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "[1, 4, 27, 256, 3125]\n",
      "['jack', 'john', 'sarah']\n",
      "jackjohnjoesarahkim\n"
     ]
    }
   ],
   "source": [
    "# Defining a lambda function (Dirty-Short-Anonymous function)\n",
    "\n",
    "# function_name = (lambda input_1,input_2,...,input_n : output)\n",
    "power = (lambda num1,num2 : num1 ** num2)\n",
    "\"\"\"Raise num1 to the power of num2\"\"\"\n",
    "#Test\n",
    "Test1 = power(5,1)\n",
    "print(Test1)\n",
    "\n",
    "#Using lambda function in the context of the map() function\n",
    "\n",
    "#map() applies a function over an object, such as a list and outputs a map object => map(function,object or DF)\n",
    "result1_map = map((lambda num1 : num1 ** num1),[1,2,3,4,5])       \n",
    "#convert the map object type into a list\n",
    "result1 = list(result1_map)                                       \n",
    "print(result1)\n",
    "\n",
    "#Using lambda function in the context of the filter() function\n",
    "\n",
    "#filter() applies a function over an object, such as a list and outputs a filter object => filter(function,object or DF)\n",
    "result2 = filter((lambda name1 : len(name1) > 3),['jack','john','joe','sarah','kim'])\n",
    "#convert the map object type into a list\n",
    "result2 = list(result2)\n",
    "print(result2)\n",
    "\n",
    "#Using lambda function in the context of the reduce() function\n",
    "from functools import reduce \n",
    "#reduce() applies a function over an object, such as a list and outputs a single value => reduce(function,object or DF)\n",
    "result3 = reduce((lambda item1,item2: item1 + item2),['jack','john','joe','sarah','kim'])\n",
    "print(result3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Error and exception handling\n",
    "\n",
    "#Some common error group name are: ValueError, TypeError, UnboundLocalError, UnicodeError, NameError, IndexError  \n",
    "#for more detail check Python online documentation\n",
    "#try-except clause: \n",
    "def sqrt1(x):\n",
    "    \"\"\"Return the square root of a number\"\"\"\n",
    "    try:\n",
    "        return x ** 0.5\n",
    "    except: #you can use 'except <error_group_name>'\n",
    "        print('x must be an int or float')\n",
    "\n",
    "#raise-try-except clause:\n",
    "def sqrt2(x):\n",
    "    \"\"\"Return the square root of a number\"\"\"\n",
    "    if x < 0:\n",
    "        raise ValueError('x must be non-negative') #note: raise <error_group_name>('error_description')\n",
    "    try:\n",
    "        return x ** 0.5\n",
    "    except: #you can use 'except <error_group_name>'\n",
    "        print('x must be an int or float')\n",
    "\n",
    "#Test\n",
    "Test1 = sqrt2(-50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jack\n",
      "john\n",
      "joe\n",
      "sarah\n",
      "kim\n",
      "jack\n",
      "john\n",
      "joe\n",
      "sarah\n",
      "kim\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "range"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Iterating Itrerable variable over Iterator\n",
    "\n",
    "#lists, strings, dictionaries, file connections are all \"Iterable\"\n",
    "#applying \"iter()\" to an \"Iterable\" creates \"Iterator\"\n",
    "#iterator produces next value with \"next()\"\n",
    "\n",
    "A = ['jack','john','joe','sarah','kim']\n",
    "\n",
    "#iterating over for loop\n",
    "for i in A:\n",
    "    print(i)\n",
    "\n",
    "#creating an iterator and iterating over it\n",
    "it = iter(A)\n",
    "#printing iterator elements\n",
    "print(next(it)) \n",
    "print(next(it))\n",
    "print(next(it))\n",
    "print(next(it))\n",
    "print(next(it))\n",
    "#range outputs an range object not a list\n",
    "type(range(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'enumerate'>\n",
      "[(0, 'hawkeye'), (1, 'iron man'), (2, 'thor'), (3, 'quicksilver')]\n",
      "0 hawkeye\n",
      "1 iron man\n",
      "2 thor\n",
      "3 quicksilver\n",
      "10 quicksilver\n",
      "11 quicksilver\n",
      "12 quicksilver\n",
      "13 quicksilver\n",
      "<class 'enumerate'> <class 'enumerate'>\n"
     ]
    }
   ],
   "source": [
    "#enumerate() function:\n",
    "\n",
    "#takes any iterable and returns \"enumerate\" object which \n",
    "#consists of pairs containing original iterable along with their index (as default start with 0)\n",
    "avengers = ['hawkeye','iron man','thor','quicksilver']\n",
    "e = enumerate(avengers)\n",
    "#convert enumerate object into a list \n",
    "e_list = list(e)\n",
    "print(type(e))\n",
    "print(e_list)\n",
    "#unpacking enumerate\n",
    "for index, value in enumerate(avengers):        \n",
    "    #note: enumerate should be written (cannot be refered to an variable like 'e')\n",
    "    print(index, value)\n",
    "\n",
    "#enumerate() starts with a specific number: (by defining 'start' parameter):\n",
    "for index, valu in enumerate(avengers, start=10):\n",
    "    print(index, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'zip'>\n",
      "[('hawkeye', 'baron'), ('iron man', 'stark'), ('thor', 'odinson'), ('quicksilver', 'maximoff')]\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#zip() function:\n",
    "\n",
    "#accept numbers of iterables and return an iterator of tuples as a \"zip\" object\n",
    "#first element of the output is a tuple containing the first elements of each list and so on... \n",
    "avengers = ['hawkeye','iron man','thor','quicksilver']\n",
    "names = ['baron','stark','odinson','maximoff']\n",
    "z = zip(avengers, names)\n",
    "#convert zip object into a list \n",
    "z_list = list(z)\n",
    "print(type(z))\n",
    "print(z_list)\n",
    "#unpacking zip method1:\n",
    "for z1,z2 in z:\n",
    "    print(z1,z2)\n",
    "#unpacking zip method1:\n",
    "for z3 in z:\n",
    "    print(*z3)\n",
    "    #print all elements\n",
    "\n",
    "#check whether the unpacked tuples are equivalent to original lists\n",
    "z4 = zip(avengers, names)\n",
    "result1, result2 = zip(*z4)\n",
    "#Unpacking ouputs tuples and needs to convert to list\n",
    "result1_list= list(result1)\n",
    "result2_list= list(result2)\n",
    "print(result1_list==avengers)\n",
    "print(result2_list==names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1996566 1996566\n"
     ]
    }
   ],
   "source": [
    "#loading data in chunks(pieces) to load large files into memory\n",
    "\n",
    "#Calcultaing sum of a column by loading in chunks of 10 entries at a time (Method one)\n",
    "result1 = []\n",
    "#loop over an iterable object created by read_csv\n",
    "for chunk in pd.read_csv('Census_Data_-_Selected_socioeconomic_indicators_in_Chicago__2008___2012.csv', chunksize=5):\n",
    "    result1.append(sum(chunk['PER CAPITA INCOME ']))\n",
    "#sum of all is the sum of all chunks\n",
    "total1 = sum(result1)\n",
    "\n",
    "#Calcultaing sum of a column by loading in chunks of 10 entries at a time (Method two)\n",
    "total2 = 0\n",
    "for chunk in pd.read_csv('Census_Data_-_Selected_socioeconomic_indicators_in_Chicago__2008___2012.csv', chunksize=5):\n",
    "    total2 = total2 + sum(chunk['PER CAPITA INCOME '])\n",
    "\n",
    "print(total1,total2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Rogers Park': 1, 'West Ridge': 1, 'Uptown': 1, 'Lincoln Square': 1, 'North Center': 1, 'Lake View': 1, 'Lincoln Park': 1, 'Near North Side': 1, 'Edison Park': 1, 'Norwood Park': 1, 'Jefferson Park': 1, 'Forest Glen': 1, 'North Park': 1, 'Albany Park': 1, 'Portage Park': 1, 'Irving Park': 1, 'Dunning': 1, 'Montclaire': 1, 'Belmont Cragin': 1, 'Hermosa': 1, 'Avondale': 1, 'Logan Square': 1, 'Humboldt park': 1, 'West Town': 1, 'Austin': 1, 'West Garfield Park': 1, 'East Garfield Park': 1, 'Near West Side': 1, 'North Lawndale': 1, 'South Lawndale': 1, 'Lower West Side': 1, 'Loop': 1, 'Near South Side': 1, 'Armour Square': 1, 'Douglas': 1, 'Oakland': 1, 'Fuller Park': 1, 'Grand Boulevard': 1, 'Kenwood': 1, 'Washington Park': 1, 'Hyde Park': 1, 'Woodlawn': 1, 'South Shore': 1, 'Chatham': 1, 'Avalon Park': 1, 'South Chicago': 1, 'Burnside': 1, 'Calumet Heights': 1, 'Roseland': 1, 'Pullman': 1, 'South Deering': 1, 'East Side': 1, 'West Pullman': 1, 'Riverdale': 1, 'Hegewisch': 1, 'Garfield Ridge': 1, 'Archer Heights': 1, 'Brighton Park': 1, 'McKinley Park': 1, 'Bridgeport': 1, 'New City': 1, 'West Elsdon': 1, 'Gage Park': 1, 'Clearing': 1, 'West Lawn': 1, 'Chicago Lawn': 1, 'West Englewood': 1, 'Englewood': 1, 'Greater Grand Crossing': 1, 'Ashburn': 1, 'Auburn Gresham': 1, 'Beverly': 1, 'Washington Height': 1, 'Mount Greenwood': 1, 'Morgan Park': 1, \"O'Hare\": 1, 'Edgewater': 1, 'CHICAGO': 1}\n"
     ]
    }
   ],
   "source": [
    "# Defining a function\n",
    "# to get csv file, chunk size and column name as arguments  \n",
    "# loading data in chunks(pieces) and iterate over column name\n",
    "# and counts the number of entries as output\n",
    "\n",
    "def count_entries(csv_file,c_size,colname):\n",
    "    \"\"\"Return a dictionary with counts of \n",
    "    occurrences as value for each key.\"\"\"\n",
    "    try:\n",
    "        #empty dictionary\n",
    "        dic_cols_count = {}                                     \n",
    "        # Iterate over the rows of selected column\n",
    "        for chunk in pd.read_csv(csv_file, chunksize=c_size):\n",
    "            for i in chunk[colname]:\n",
    "                if i in dic_cols_count.keys():                  # Note that \"in\" is used in if clause\n",
    "                    dic_cols_count[i] = dic_cols_count[i] + 1   # If the value exists, add 1 to it\n",
    "                else:                               \n",
    "                    dic_cols_count[i] = 1                       # Else add it to keys and set the value to 1\n",
    "        return dic_cols_count\n",
    "    except:\n",
    "        print(\"There was an error\") \n",
    "\n",
    "#Test (using pre-defined Census Data csv file)\n",
    "Test1 = count_entries('Census_Data_-_Selected_socioeconomic_indicators_in_Chicago__2008___2012.csv',5,'COMMUNITY AREA NAME')\n",
    "print(Test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connecting to SQLServer using pyodbc library and retrieving data into tuples and Pandas DataFrame (not supported)\n",
    "\n",
    "#Importing library\n",
    "import pyodbc as odbc\n",
    "import pandas as pd\n",
    "#Defining connection parameters\n",
    "DRIVER_NAME   = 'SQL Server'\n",
    "SERVER_NAME   = 'GOLD-THHQ\\LOCALSQLSERVER'\n",
    "DATABASE_NAME = 'PowerBIReportServerTest'\n",
    "#PORT          = '1433' \n",
    "USER_NAME     = 'sa'\n",
    "PASSWORD      = '12345@qaz'\n",
    "#Creating connection\n",
    "connection_string1 = f\"\"\"\n",
    "    DRIVER={{{DRIVER_NAME}}};\n",
    "    SERVER={SERVER_NAME};\n",
    "    DATABASE={DATABASE_NAME};\n",
    "    UID={USER_NAME};\n",
    "    PWD={PASSWORD};\n",
    "\"\"\"\n",
    "conn1 = odbc.connect(connection_string1)\n",
    "#Creating cursor\n",
    "cursor1 = conn1.cursor()\n",
    "query1 = \"SELECT * FROM USERS\"\n",
    "cursor1.execute(query1)\n",
    "#Printing output\n",
    "output1 = cursor1.fetchall()\n",
    "for row1 in output1:\n",
    "    print(row1)\n",
    "\n",
    "#Retrive into DataFrame using query    \n",
    "#Warning = Pandas do not support using pyodbc library (instead supports sqlalchamy and sqlite)\n",
    "df01 = pd.read_sql_query(query1,conn1)\n",
    "df01\n",
    "\n",
    "conn1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connecting to SQLServer using sqlalchemy + pyodbc libraries, retrieving data into Pandas DataFrame (Best Practice) and retrieving data using SQL Magic from ipython-sql library (Which uses sqlalchemy)\n",
    "\n",
    "#Importing library\n",
    "import pyodbc\n",
    "import sqlalchemy as sal\n",
    "import urllib\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import Select, Table, Column, Integer, String, MetaData, ForeignKey  \n",
    "#Defining connection parameters\n",
    "DRIVER_NAME   = 'SQL Server'\n",
    "SERVER_NAME   = 'GOLD-THHQ\\LOCALSQLSERVER'\n",
    "DATABASE_NAME = 'PowerBIReportServerTest'\n",
    "PORT          = '1433' \n",
    "USER_NAME     = 'sa'\n",
    "PASSWORD      = '12345@qaz'\n",
    "\n",
    "#Creating connection parameters using URL\n",
    "params = urllib.parse.quote_plus(F\"\"\"DRIVER={DRIVER_NAME};SERVER={SERVER_NAME};DATABASE={DATABASE_NAME};UID={USER_NAME};PWD={PASSWORD}\"\"\")\n",
    "\n",
    "#Creating connection\n",
    "#connection_string1 = f\"\"\"mssql+pyodbc://{USER_NAME}:{PASSWORD}@{SERVER_NAME}:{PORT}/{DATABASE_NAME}?driver={DRIVER_NAME}\"\"\"\n",
    "connection_string2 = \"mssql+pyodbc:///?odbc_connect={}\".format(params)\n",
    "engine = sal.create_engine(connection_string2)\n",
    "conn2 = engine.connect()\n",
    "\n",
    "#Retrive into DataFrame using query\n",
    "query1 = \"SELECT * FROM USERS\"\n",
    "df02 = pd.read_sql_query(query1,conn2)\n",
    "\n",
    "#retrieving data using SQL Magic from ipython-sql library (Which uses sqlalchemy)\n",
    "#load ipython-SQL extension (SQL Magic)\n",
    "%load_ext sql\n",
    "#connecting SQL Magic to sqlaclchemy \n",
    "%sql {connection_string2}\n",
    "#Query\n",
    "output2 = %sql SELECT * FROM USERS\n",
    "output2\n",
    "\n",
    "#conn2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to SQL Databases\n",
    "import sqlalchemy as sal\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "def extract_from_SQL(connection_URL,Query):\n",
    "    \"\"\"Defining a function to connect and run SQL queries\"\"\"\n",
    "    db_engine = sal.create_engine(connection_URL)\n",
    "    return pd.read_sql(Query,db_engine)\n",
    "#Connection URL    =  Schema_identifier://username:password@host:port/db\n",
    "connection_string3 = \"mssql+pyodbc://as.moradi@localhost:1433/AdventureWorks2022\"\n",
    "query3 = \"SELECT * FROM [AdventureWorks2022].[Person].[Person]\"\n",
    "extract_from_SQL(connection_string3,query3)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Database using sqlite library and retrieving data into tuples and Pandas DataFrame\n",
    "\n",
    "#Import libraries\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "#Closing previews opened database in order to prevent database locking\n",
    "conn3.close()\n",
    "#Create Database and connection\n",
    "DATABASE_NAME = 'A3'\n",
    "conn3 = sqlite3.connect(F\"\"\"{DATABASE_NAME}.db\"\"\")\n",
    "#Creating cursor\n",
    "cursor_obj = conn3.cursor()\n",
    "cursor_obj.execute(F\"DROP TABLE IF EXISTS {DATABASE_NAME}\")\n",
    "table = F\"\"\" CREATE TABLE IF NOT EXISTS {DATABASE_NAME} (ID INTEGER PRIMARY KEY NOT NULL, FNAME VARCHAR(20), LNAME VARCHAR(20), CITY VARCHAR(20), CCODE CHAR(2) )\"\"\"\n",
    "cursor_obj.execute(table)\n",
    "#Insert data into Database\n",
    "cursor_obj.execute(F\"\"\"insert into {DATABASE_NAME} values (1,'Rav','Ahuja','TORONTO','CA')\"\"\")\n",
    "cursor_obj.execute(F\"\"\"insert into {DATABASE_NAME} values (2,'Raul','Chong','Markham','CA') , (3,'Hima','Vasudevan','Chicago','US')\"\"\")\n",
    "\n",
    "#Query1\n",
    "query1 = F\"\"\"SELECT * FROM {DATABASE_NAME}\"\"\"\n",
    "cursor_obj.execute(query1)\n",
    "#Printing output\n",
    "print('output1:')\n",
    "output31 = cursor_obj.fetchall()\n",
    "for row31 in output31:\n",
    "    print(row31)\n",
    "\n",
    "#Query2\n",
    "query2 = F\"\"\"UPDATE {DATABASE_NAME} SET CITY='MOOSETOWN' WHERE FNAME ='Rav' \"\"\"\n",
    "cursor_obj.execute(query2)\n",
    "cursor_obj.execute(query1)\n",
    "print('output2:')\n",
    "output32 = cursor_obj.fetchall()\n",
    "for row32 in output32:\n",
    "    print(row32)\n",
    "\n",
    "#Retrive into DataFrame using query\n",
    "df03 = pd.read_sql_query(query1,conn3)\n",
    "\n",
    "print(df03['ID'].idxmax())              #prints the position of maximum number of selected column\n",
    "print(df03.at[2,'FNAME'])               #prints the value in the selected position  column\n",
    "df03\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Database using sqlite library and retrieving data using SQL Magic from ipython-sql library (Which uses sqlalchemy) with ploting (Writing sql query in python)  \n",
    "\n",
    "#Import Libraries\n",
    "import sqlite3\n",
    "#import csv\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "#load ipython-SQL extension (SQL Magic)\n",
    "%load_ext sql\n",
    "#Closing previews opened database in order to prevent database locking\n",
    "conn4.close()\n",
    "#Create Database and connection\n",
    "DATABASE_NAME = 'A4'\n",
    "conn4 = sqlite3.connect(F\"\"\"{DATABASE_NAME}.db\"\"\")\n",
    "#Creating cursor\n",
    "cursor_obj = conn4.cursor()\n",
    "\n",
    "#Query\n",
    "# %sql  shows sql query in a line\n",
    "# %%sql shows sql query in a cell\n",
    "#to use python variable into a sql query use \":variable_name\"\n",
    "TABLE_NAME = \"INTERNATIONAL_STUDENT_TEST_SCORES\"\n",
    "%sql DROP TABLE IF EXISTS {TABLE_NAME} \n",
    "%sql CREATE TABLE IF NOT EXISTS {TABLE_NAME} (country VARCHAR(50), first_name VARCHAR(50), last_name VARCHAR(50), test_score INT);\n",
    "%sql INSERT INTO {TABLE_NAME} (country, first_name, last_name, test_score) VALUES ('United States', 'Marshall', 'Bernadot', 54),('Ghana', 'Celinda', 'Malkin', 51),('Ukraine', 'Guillermo', 'Furze', 53),('Greece', 'Aharon', 'Tunnow', 48),('Russia', 'Bail', 'Goodwin', 46),('Poland', 'Cole', 'Winteringham', 49),('Sweden', 'Emlyn', 'Erricker', 55),('Russia', 'Cathee', 'Sivewright', 49)\n",
    "%sql INSERT INTO {TABLE_NAME} (country, first_name, last_name, test_score) VALUES ('Russia', 'Cathee', 'Sivewright', 49),('China', 'Barny', 'Ingerson', 57),('Uganda', 'Sharla', 'Papaccio', 55),('China', 'Stella', 'Youens', 51),('Poland', 'Julio', 'Buesden', 48),('United States', 'Tiffie', 'Cosely', 58),('Poland', 'Auroora', 'Stiffell', 45),('China', 'Clarita', 'Huet', 52)\n",
    "%sql INSERT INTO {TABLE_NAME} (country, first_name, last_name, test_score) VALUES ('Poland', 'Shannon', 'Goulden', 45),('Philippines', 'Emylee', 'Privost', 50),('France', 'Madelina', 'Burk', 49),('China', 'Saunderson', 'Root', 58),('Indonesia', 'Bo', 'Waring', 55),('China', 'Hollis', 'Domotor', 45),('Russia', 'Robbie', 'Collip', 46),('Philippines', 'Davon', 'Donisi', 46),('China', 'Cristabel', 'Radeliffe', 48)\n",
    "\n",
    "#Assign a query into python variable\n",
    "output41 = %sql SELECT * FROM {TABLE_NAME}\n",
    "output42 = %sql SELECT test_score AS test_score, count(*) AS frequency FROM {TABLE_NAME} GROUP BY test_score\n",
    "print('output42 type is:',type(output42))\n",
    "print(output42)\n",
    "#Ploting a query\n",
    "#output needs conversion to DataFrame \n",
    "\n",
    "#df04 = output42.DataFrame()\n",
    "#print('df04 type is:',type(df04))\n",
    "#plot = sns.barplot(x='test_score',y='frequency', data=df04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sql extension is already loaded. To reload it, use:\n",
      "  %reload_ext sql\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Program Files\\Python311\\Lib\\site-packages\\sql\\magic.py\", line 196, in execute\n",
      "    conn = sql.connection.Connection.set(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Program Files\\Python311\\Lib\\site-packages\\sql\\connection.py\", line 82, in set\n",
      "    raise ConnectionError(\n",
      "sql.connection.ConnectionError: Environment variable $DATABASE_URL not set, and no connect string given.\n",
      "\n",
      "Connection info needed in SQLAlchemy format, example:\n",
      "               postgresql://username:password@hostname/dbname\n",
      "               or an existing connection: dict_keys([])\n"
     ]
    }
   ],
   "source": [
    "#  \n",
    "\n",
    "#Import Libraries\n",
    "import sqlite3\n",
    "import csv\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "#load ipython-SQL extension (SQL Magic)\n",
    "%load_ext sql\n",
    "#Closing previews opened database in order to prevent database locking\n",
    "conn5.close()\n",
    "#Create Database and connection\n",
    "DATABASE_NAME = 'A5'\n",
    "TABLE_NAME = 'CHICAGO_DATA'\n",
    "conn5 = sqlite3.connect(F\"\"\"{DATABASE_NAME}.db\"\"\")\n",
    "#Creating cursor\n",
    "cursor_obj = conn5.cursor()\n",
    "df05 = pd.read_csv('Census_Data_-_Selected_socioeconomic_indicators_in_Chicago__2008___2012.csv')\n",
    "#Retrive into SQL Server using DataFrame\n",
    "df05.to_sql(F\"{TABLE_NAME}\" ,conn5 ,if_exists='replace' ,index=False ,method='multi')\n",
    "%sql SELECT * FROM F\"{TABLE_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MachineLearning in python course (365DataScience)\n",
    "#Simple Linear regression\n",
    "\n",
    "#Calculating GPA of students based on the SAT mark \n",
    "data = pd.read_csv('Machine Learning in Python (365DataScience)/machine-learning-in-python/Section 1/7_First regression in Python/Course notes/First regression in Python Dataset/1.01. Simple linear regression.csv')\n",
    "y  = data['GPA']\n",
    "x1 = data['SAT']\n",
    "#using predefined statsmodels.api library as 'sm' and add a constant\n",
    "x = sm.add_constant(x1)\n",
    "# Ordinary Least Squares (OLS) Regression\n",
    "result = sm.OLS(y,x).fit()\n",
    "result.summary()\n",
    "# Plot using coefs of summary\n",
    "\n",
    "#plt.scatter(x1,y)\n",
    "#yhat = 0.0017*x1 + 0.275\n",
    "#fig = plt.plot(x1, yhat, lw=2, c='red', label='Regression Line')\n",
    "#plt.xlabel('SAT',fontsize=10)\n",
    "#plt.ylabel('GPA',fontsize=10)\n",
    "#plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting Lat and Long using a csv file\n",
    "\n",
    "df06 = pd.read_csv('worldcities.csv')\n",
    "df06.columns\n",
    "LAT  = df06['lat']\n",
    "LONG = df06['lng']\n",
    "plt.plot(LAT,LONG)\n",
    "#plt.title('World population')           #title\n",
    "#plt.xlabel('Year')                      #xlabel (notice \"label\" not \"lable like table\")\n",
    "#plt.ylabel('Population')                #ylabel (notice \"label\" not \"lable like table\")\n",
    "#plt.yticks([0,2,4,6,8]                  #yticks limits \n",
    "#          ,['0','2B','4B','6B','8B'])   #replace axis with string (Billion)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13, 9, 22, 4, 17]\n",
      "[13, 9, 22, 4, 17]\n"
     ]
    }
   ],
   "source": [
    "#List Comprehension (DataCamp Python Toolbox2)\n",
    "#Old Method\n",
    "nums = [12,8,21,3,16]\n",
    "new_nums = []\n",
    "for num in nums:\n",
    "    new_nums.append(num + 1)\n",
    "print(new_nums)\n",
    "#Using List Comprehension\n",
    "# [[output expression] for iterator variable in iterable]\n",
    "new_nums = [num + 1 for num in nums]\n",
    "print(new_nums)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n",
      "[0, 1, 2, 3, 4]\n",
      "[0, 1, 2, 3, 4]\n",
      "[0, 1, 2, 3, 4]\n",
      "[0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "#Nested List Comprehension\n",
    "# Create a 5 x 5 matrix using a list of lists: matrix\n",
    "matrix = [[col for col in range(5)] for row in range(5)]\n",
    "\n",
    "# Print the matrix\n",
    "for row in matrix:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['samwise', 'aragorn', 'legolas', 'boromir']\n"
     ]
    }
   ],
   "source": [
    "# Using conditionals in comprehensions Type1 (if in iterable)\n",
    "# [output expression for iterator in iterable if expression ]\n",
    "# Create a list of strings: fellowship\n",
    "fellowship = ['frodo', 'samwise', 'merry', 'aragorn', 'legolas', 'boromir', 'gimli']\n",
    "# Create list comprehension: new_fellowship\n",
    "new_fellowship = [member for member in fellowship if len(member) >= 7]\n",
    "print(new_fellowship)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'samwise', '', 'aragorn', 'legolas', 'boromir', '']\n"
     ]
    }
   ],
   "source": [
    "# Using conditionals in comprehensions Type2 (if in output expression)\n",
    "# [output expression if expression for iterator in iterable ]\n",
    "# Create list comprehension: new_fellowship\n",
    "fellowship = ['frodo', 'samwise', 'merry', 'aragorn', 'legolas', 'boromir', 'gimli']\n",
    "new_fellowship = [member if len(member)>=7 else '' for member in fellowship]\n",
    "print(new_fellowship)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'frodo': 5, 'samwise': 7, 'merry': 5, 'aragorn': 7, 'legolas': 7, 'boromir': 7, 'gimli': 5}\n"
     ]
    }
   ],
   "source": [
    "#Dict comprehensions\n",
    "# {key:value for key(iterator) in iterable}\n",
    "# Create a list of strings: fellowship\n",
    "fellowship = ['frodo', 'samwise', 'merry', 'aragorn', 'legolas', 'boromir', 'gimli']\n",
    "# Create dict comprehension: new_fellowship\n",
    "new_fellowship = {member: len(member) for member in fellowship }\n",
    "print (type(new_fellowship))\n",
    "print (new_fellowship)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['samwise', 'aragorn', 'legolas', 'boromir']\n",
      "<class 'list'>\n",
      "<generator object <genexpr> at 0x000001CAD9DF6F80>\n",
      "<class 'generator'>\n"
     ]
    }
   ],
   "source": [
    "# List comprehensions vs. generators\n",
    "# generator expressions look very similar in their syntax, \n",
    "# we use parentheses () in generator expressions and brackets [] in list comprehensions\n",
    "# list comprehensions are stored in Memory but generators are on fly\n",
    "# heavy list comprehensions are not suitable for servers\n",
    "# A list comprehension produces a list as output, a generator produces a generator object\n",
    "# Both list comprehension and generator are iterables and you can iterate over them\n",
    "# When you use functions, Python creates generators for you behind the scenes\n",
    "# you can change a generator into a list using func List()\n",
    "\n",
    "# List of strings\n",
    "fellowship = ['frodo', 'samwise', 'merry', 'aragorn', 'legolas', 'boromir', 'gimli']\n",
    "# List comprehension\n",
    "fellow1 = [member for member in fellowship if len(member) >= 7]\n",
    "# Generator expression\n",
    "fellow2 = (member for member in fellowship if len(member) >= 7)\n",
    "print(fellow1)\n",
    "print (type(fellow1))\n",
    "print(fellow2)\n",
    "print (type(fellow2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# Create generator object: result\n",
    "# Lazy evaluation\n",
    "result = (num for num in range(10))\n",
    "# Print the first 5 values\n",
    "print(next(result))\n",
    "print(next(result))\n",
    "print(next(result))\n",
    "print(next(result))\n",
    "print(next(result))\n",
    "# Print the rest of the values\n",
    "for value in result:\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "5\n",
      "5\n",
      "6\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "# Define generator function get_lengths\n",
    "\n",
    "# A generator function is defined as you do a regular function, but whenever it generates a value, it uses the keyword \"yield\" instead of \"return\".\n",
    "# The syntax for a function header is def function name(parameters):.\n",
    "# You can use the len() function to get the length of a string.\n",
    "# Pass the list lannister to a call to get_lengths(). Use this function call as the iterable for the for loop for printing values.\n",
    "lannister = ['cersei', 'jaime', 'tywin', 'tyrion', 'joffrey']\n",
    "\n",
    "def get_lengths(input_list):\n",
    "    \"\"\"Generator function that yields the\n",
    "    length of the strings in input_list.\"\"\"\n",
    "\n",
    "    # Yield the length of a string\n",
    "    for person in input_list:\n",
    "        yield len(person)\n",
    "\n",
    "# Print the values generated by get_lengths()\n",
    "for value in get_lengths(lannister):\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{10: 'cersei', 15: 'jaime', 20: 'tywin', 30: 'tyrion', 40: 'joffrey'}\n"
     ]
    }
   ],
   "source": [
    "# Basic comprehension\n",
    "# [output expression for iterator variable in iterable]\n",
    "\n",
    "# Advanced comprehension\n",
    "# [output expression + conditional on output for iterator variable in iterable + conditional on iterable]\n",
    "\n",
    "# You can think of DataFrame columns as single-dimension arrays called Series.\n",
    "\n",
    "# Define lists2dict()\n",
    "def lists2dict(list1, list2):\n",
    "    \"\"\"Return a dictionary where list1 provides\n",
    "    the keys and list2 provides the values.\"\"\"\n",
    "\n",
    "    # Zip lists: zipped_lists\n",
    "    zipped_lists = zip(list1, list2)\n",
    "\n",
    "    # Create a dictionary: rs_dict\n",
    "    rs_dict = dict(zipped_lists)\n",
    "\n",
    "    # Return the dictionary\n",
    "    return rs_dict\n",
    "\n",
    "\n",
    "age = [10,15,20,30,40]\n",
    "lannister = ['cersei', 'jaime', 'tywin', 'tyrion', 'joffrey']\n",
    "A = lists2dict(age,lannister)\n",
    "print (A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 1, '2': 1, '3': 1, '4': 1, '5': 1, '6': 1, '7': 1, '8': 1, '9': 1, '10': 1, '11': 1, '12': 1, '13': 1, '14': 1, '15': 1, '16': 1, '17': 1, '18': 1, '19': 1, '20': 1, '21': 1, '22': 1, '23': 1, '24': 1, '25': 1, '26': 1, '27': 1, '28': 1, '29': 1, '30': 1, '31': 1, '32': 1, '33': 1, '34': 1, '35': 1, '36': 1, '37': 1, '38': 1, '39': 1, '40': 1, '41': 1, '42': 1, '43': 1, '44': 1, '45': 1, '46': 1, '47': 1, '48': 1, '49': 1, '50': 1, '51': 1, '52': 1, '53': 1, '54': 1, '55': 1, '56': 1, '57': 1, '58': 1, '59': 1, '60': 1, '61': 1, '62': 1, '63': 1, '64': 1, '65': 1, '66': 1, '67': 1, '68': 1, '69': 1, '70': 1, '71': 1, '72': 1, '73': 1, '74': 1, '75': 1, '76': 1, '77': 1, '': 923}\n"
     ]
    }
   ],
   "source": [
    "# Open a connection to the file\n",
    "with open('Census_Data_-_Selected_socioeconomic_indicators_in_Chicago__2008___2012.csv') as file:\n",
    "\n",
    "    # Skip the column names\n",
    "    file.readline()\n",
    "\n",
    "    # Initialize an empty dictionary: counts_dict\n",
    "    counts_dict = {}\n",
    "\n",
    "    # Process only the first 1000 rows\n",
    "    for j in range(0, 1000):\n",
    "\n",
    "        # Split the current line into a list: line\n",
    "        line = file.readline().split(',')\n",
    "\n",
    "        # Get the value for the first column: first_col\n",
    "        first_col = line[0]\n",
    "\n",
    "        # If the column value is in the dict, increment its value\n",
    "        if first_col in counts_dict.keys():\n",
    "            counts_dict[first_col] += 1\n",
    "\n",
    "        # Else, add to the dict and set value to 1\n",
    "        else:\n",
    "            counts_dict[first_col] = 1\n",
    "\n",
    "# Print the resulting dictionary\n",
    "print(counts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country Name,Country Code,Indicator Name,Indicator Code,1960,1961,1962,1963,1964,1965,1966,1967,1968,1969,1970,1971,1972,1973,1974,1975,1976,1977,1978,1979,1980,1981,1982,1983,1984,1985,1986,1987,1988,1989,1990,1991,1992,1993,1994,1995,1996,1997,1998,1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022,2023\n",
      "\n",
      "\"Africa Eastern and Southern\",\"AFE\",\"Access to clean fuels and technologies for cooking (% of population)\",\"EG.CFT.ACCS.ZS\",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\"11.52590439795372\",\"11.850593338062035\",\"12.175592061995731\",\"12.551722825279798\",\"12.924851256414607\",\"13.347701496159596\",\"13.804892752786618\",\"14.226284755377614\",\"14.644801461586704\",\"15.136871433447176\",\"15.5923281591116\",\"16.005356162999163\",\"16.46694455761363\",\"16.877312566791623\",\"17.401409684824703\",\"17.91123378561099\",\"18.463873940553047\",\"18.924037251517618\",\"19.43705420221095\",\"20.026254443085296\",\"20.647968706121976\",\"21.165877048846593\",\"21.863138698088935\",\n",
      "\n",
      "\"Africa Eastern and Southern\",\"AFE\",\"Access to clean fuels and technologies for cooking, rural (% of rural population)\",\"EG.CFT.ACCS.RU.ZS\",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\"3.4699771561813364\",\"3.642009673712452\",\"3.849941635994615\",\"4.054107969616864\",\"4.285291056845838\",\"4.490187072728173\",\"4.734117326126425\",\"4.9968422429400805\",\"5.2390063601874415\",\"5.457239937523774\",\"5.686399953526102\",\"5.959238618862823\",\"6.20222055561521\",\"6.45323969275503\",\"6.72881936065962\",\"7.005876550574386\",\"7.3085714281656\",\"7.547225990787448\",\"7.875917244207388\",\"8.243018268881743\",\"8.545483107437004\",\"8.906710920785311\",\"9.261319669800752\",\n",
      "\n",
      "\"Africa Eastern and Southern\",\"AFE\",\"Access to clean fuels and technologies for cooking, urban (% of urban population)\",\"EG.CFT.ACCS.UR.ZS\",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\"32.431714423851645\",\"32.82868263829109\",\"33.24773929577449\",\"33.70205881422651\",\"34.19489546550774\",\"34.584695995412105\",\"35.136730348822724\",\"35.69313109122375\",\"36.03878858612353\",\"36.50279649848295\",\"36.80532759055838\",\"37.16224346594934\",\"37.485980304312456\",\"37.7840288495253\",\"38.08093121317979\",\"38.422281861192175\",\"38.722108259370835\",\"38.993157285801686\",\"39.33787177431041\",\"39.695279318546405\",\"40.13784720189246\",\"40.5222092021985\",\"41.011131759951375\",\n",
      "\n",
      "\"Africa Eastern and Southern\",\"AFE\",\"Access to electricity (% of population)\",\"EG.ELC.ACCS.ZS\",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\"19.963881761037054\",\"19.96914819765615\",\"21.580282430579885\",\"22.516962406465094\",\"23.753952841609376\",\"23.48737636595644\",\"25.198526917364667\",\"26.807287870995317\",\"25.947513029882664\",\"26.185912441294366\",\"27.402014356380477\",\"28.91159373156528\",\"31.666038423091727\",\"31.70305724194824\",\"31.860474023503993\",\"33.903800128646196\",\"38.85462442215109\",\"40.19989806819498\",\"43.017147983264664\",\"44.381259442868085\",\"46.264874557718755\",\"48.10086213419303\",\"48.71199459355673\",\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Writing an iterator to load data in chunks\n",
    "# Define read_large_file()\n",
    "def read_large_file(file_object):\n",
    "    \"\"\"A generator function to read a large file lazily.\"\"\"\n",
    "\n",
    "    # Loop indefinitely until the end of the file\n",
    "    while True:\n",
    "\n",
    "        # Read a line from the file: data\n",
    "        data = file_object.readline()\n",
    "\n",
    "        # Break if this is the end of the file\n",
    "        if not data:\n",
    "            break\n",
    "\n",
    "        # Yield the line of data\n",
    "        yield data\n",
    "        \n",
    "# Open a connection to the file\n",
    "with open('WDICSV.csv') as file:\n",
    "\n",
    "    # Create a generator object for the file: gen_file\n",
    "    gen_file = read_large_file(file)\n",
    "\n",
    "    # Print the first three lines of the file\n",
    "    print(next(gen_file))\n",
    "    print(next(gen_file))\n",
    "    print(next(gen_file))\n",
    "    print(next(gen_file))\n",
    "    print(next(gen_file))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Country Name': 1, '\"Africa Eastern and Southern\"': 1492, '\"Africa Western and Central\"': 1492, '\"Arab World\"': 1492, '\"Caribbean small states\"': 1492, '\"Central Europe and the Baltics\"': 1492, '\"Early-demographic dividend\"': 1492, '\"East Asia & Pacific\"': 1492, '\"East Asia & Pacific (excluding high income)\"': 1492, '\"East Asia & Pacific (IDA & IBRD countries)\"': 1492, '\"Euro area\"': 1492, '\"Europe & Central Asia\"': 1492, '\"Europe & Central Asia (excluding high income)\"': 1492, '\"Europe & Central Asia (IDA & IBRD countries)\"': 1492, '\"European Union\"': 1492, '\"Fragile and conflict affected situations\"': 1492, '\"Heavily indebted poor countries (HIPC)\"': 1492, '\"High income\"': 1492, '\"IBRD only\"': 1492, '\"IDA & IBRD total\"': 1492, '\"IDA blend\"': 1492, '\"IDA only\"': 1492, '\"IDA total\"': 1492, '\"Late-demographic dividend\"': 1492, '\"Latin America & Caribbean\"': 1492, '\"Latin America & Caribbean (excluding high income)\"': 1492, '\"Latin America & the Caribbean (IDA & IBRD countries)\"': 1492, '\"Least developed countries: UN classification\"': 1492, '\"Low & middle income\"': 1492, '\"Low income\"': 1492, '\"Lower middle income\"': 1492, '\"Middle East & North Africa\"': 1492, '\"Middle East & North Africa (excluding high income)\"': 1492, '\"Middle East & North Africa (IDA & IBRD countries)\"': 1492, '\"Middle income\"': 1492, '\"North America\"': 1492, '\"Not classified\"': 1492, '\"OECD members\"': 1492, '\"Other small states\"': 1492, '\"Pacific island small states\"': 1492, '\"Post-demographic dividend\"': 1492, '\"Pre-demographic dividend\"': 1492, '\"Small states\"': 1492, '\"South Asia\"': 1492, '\"South Asia (IDA & IBRD)\"': 1492, '\"Sub-Saharan Africa\"': 1492, '\"Sub-Saharan Africa (excluding high income)\"': 1492, '\"Sub-Saharan Africa (IDA & IBRD countries)\"': 1492, '\"Upper middle income\"': 1492, '\"World\"': 1492, '\"Afghanistan\"': 1492, '\"Albania\"': 1492, '\"Algeria\"': 1492, '\"American Samoa\"': 1492, '\"Andorra\"': 1492, '\"Angola\"': 1492, '\"Antigua and Barbuda\"': 1492, '\"Argentina\"': 1492, '\"Armenia\"': 1492, '\"Aruba\"': 1492, '\"Australia\"': 1492, '\"Austria\"': 1492, '\"Azerbaijan\"': 1492, '\"Bahamas': 1492, '\"Bahrain\"': 1492, '\"Bangladesh\"': 1492, '\"Barbados\"': 1492, '\"Belarus\"': 1492, '\"Belgium\"': 1492, '\"Belize\"': 1492, '\"Benin\"': 1492, '\"Bermuda\"': 1492, '\"Bhutan\"': 1492, '\"Bolivia\"': 1492, '\"Bosnia and Herzegovina\"': 1492, '\"Botswana\"': 1492, '\"Brazil\"': 1492, '\"British Virgin Islands\"': 1492, '\"Brunei Darussalam\"': 1492, '\"Bulgaria\"': 1492, '\"Burkina Faso\"': 1492, '\"Burundi\"': 1492, '\"Cabo Verde\"': 1492, '\"Cambodia\"': 1492, '\"Cameroon\"': 1492, '\"Canada\"': 1492, '\"Cayman Islands\"': 1492, '\"Central African Republic\"': 1492, '\"Chad\"': 1492, '\"Channel Islands\"': 1492, '\"Chile\"': 1492, '\"China\"': 1492, '\"Colombia\"': 1492, '\"Comoros\"': 1492, '\"Congo': 2984, '\"Costa Rica\"': 1492, '\"Cote d\\'Ivoire\"': 1492, '\"Croatia\"': 1492, '\"Cuba\"': 1492, '\"Curacao\"': 1492, '\"Cyprus\"': 1492, '\"Czechia\"': 1492, '\"Denmark\"': 1492, '\"Djibouti\"': 1492, '\"Dominica\"': 1492, '\"Dominican Republic\"': 1492, '\"Ecuador\"': 1492, '\"Egypt': 1492, '\"El Salvador\"': 1492, '\"Equatorial Guinea\"': 1492, '\"Eritrea\"': 1492, '\"Estonia\"': 1492, '\"Eswatini\"': 1492, '\"Ethiopia\"': 1492, '\"Faroe Islands\"': 1492, '\"Fiji\"': 1492, '\"Finland\"': 1492, '\"France\"': 1492, '\"French Polynesia\"': 1492, '\"Gabon\"': 1492, '\"Gambia': 1492, '\"Georgia\"': 1492, '\"Germany\"': 1492, '\"Ghana\"': 1492, '\"Gibraltar\"': 1492, '\"Greece\"': 1492, '\"Greenland\"': 1492, '\"Grenada\"': 1492, '\"Guam\"': 1492, '\"Guatemala\"': 1492, '\"Guinea\"': 1492, '\"Guinea-Bissau\"': 1492, '\"Guyana\"': 1492, '\"Haiti\"': 1492, '\"Honduras\"': 1492, '\"Hong Kong SAR': 1492, '\"Hungary\"': 1492, '\"Iceland\"': 1492, '\"India\"': 1492, '\"Indonesia\"': 1492, '\"Iran': 1492, '\"Iraq\"': 1492, '\"Ireland\"': 1492, '\"Isle of Man\"': 1492, '\"Israel\"': 1492, '\"Italy\"': 1492, '\"Jamaica\"': 1492, '\"Japan\"': 1492, '\"Jordan\"': 1492, '\"Kazakhstan\"': 1492, '\"Kenya\"': 1492, '\"Kiribati\"': 1492, '\"Korea': 2984, '\"Kosovo\"': 1492, '\"Kuwait\"': 1492, '\"Kyrgyz Republic\"': 1492, '\"Lao PDR\"': 1492, '\"Latvia\"': 1492, '\"Lebanon\"': 1492, '\"Lesotho\"': 1492, '\"Liberia\"': 1492, '\"Libya\"': 1492, '\"Liechtenstein\"': 1492, '\"Lithuania\"': 1492, '\"Luxembourg\"': 1492, '\"Macao SAR': 1492, '\"Madagascar\"': 1492, '\"Malawi\"': 1492, '\"Malaysia\"': 1492, '\"Maldives\"': 1492, '\"Mali\"': 1492, '\"Malta\"': 1492, '\"Marshall Islands\"': 1492, '\"Mauritania\"': 1492, '\"Mauritius\"': 1492, '\"Mexico\"': 1492, '\"Micronesia': 1492, '\"Moldova\"': 1492, '\"Monaco\"': 1492, '\"Mongolia\"': 1492, '\"Montenegro\"': 1492, '\"Morocco\"': 1492, '\"Mozambique\"': 1492, '\"Myanmar\"': 1492, '\"Namibia\"': 1492, '\"Nauru\"': 1492, '\"Nepal\"': 1492, '\"Netherlands\"': 1492, '\"New Caledonia\"': 1492, '\"New Zealand\"': 1492, '\"Nicaragua\"': 1492, '\"Niger\"': 1492, '\"Nigeria\"': 1492, '\"North Macedonia\"': 1492, '\"Northern Mariana Islands\"': 1492, '\"Norway\"': 1492, '\"Oman\"': 1492, '\"Pakistan\"': 1492, '\"Palau\"': 1492, '\"Panama\"': 1492, '\"Papua New Guinea\"': 1492, '\"Paraguay\"': 1492, '\"Peru\"': 1492, '\"Philippines\"': 1492, '\"Poland\"': 1492, '\"Portugal\"': 1492, '\"Puerto Rico\"': 1492, '\"Qatar\"': 1492, '\"Romania\"': 1492, '\"Russian Federation\"': 1492, '\"Rwanda\"': 1492, '\"Samoa\"': 1492, '\"San Marino\"': 1492, '\"Sao Tome and Principe\"': 1492, '\"Saudi Arabia\"': 1492, '\"Senegal\"': 1492, '\"Serbia\"': 1492, '\"Seychelles\"': 1492, '\"Sierra Leone\"': 1492, '\"Singapore\"': 1492, '\"Sint Maarten (Dutch part)\"': 1492, '\"Slovak Republic\"': 1492, '\"Slovenia\"': 1492, '\"Solomon Islands\"': 1492, '\"Somalia\"': 1492, '\"South Africa\"': 1492, '\"South Sudan\"': 1492, '\"Spain\"': 1492, '\"Sri Lanka\"': 1492, '\"St. Kitts and Nevis\"': 1492, '\"St. Lucia\"': 1492, '\"St. Martin (French part)\"': 1492, '\"St. Vincent and the Grenadines\"': 1492, '\"Sudan\"': 1492, '\"Suriname\"': 1492, '\"Sweden\"': 1492, '\"Switzerland\"': 1492, '\"Syrian Arab Republic\"': 1492, '\"Tajikistan\"': 1492, '\"Tanzania\"': 1492, '\"Thailand\"': 1492, '\"Timor-Leste\"': 1492, '\"Togo\"': 1492, '\"Tonga\"': 1492, '\"Trinidad and Tobago\"': 1492, '\"Tunisia\"': 1492, '\"Turkiye\"': 1492, '\"Turkmenistan\"': 1492, '\"Turks and Caicos Islands\"': 1492, '\"Tuvalu\"': 1492, '\"Uganda\"': 1492, '\"Ukraine\"': 1492, '\"United Arab Emirates\"': 1492, '\"United Kingdom\"': 1492, '\"United States\"': 1492, '\"Uruguay\"': 1492, '\"Uzbekistan\"': 1492, '\"Vanuatu\"': 1492, '\"Venezuela': 1492, '\"Viet Nam\"': 1492, '\"Virgin Islands (U.S.)\"': 1492, '\"West Bank and Gaza\"': 1492, '\"Yemen': 1492, '\"Zambia\"': 1492, '\"Zimbabwe\"': 1492}\n"
     ]
    }
   ],
   "source": [
    "# Writing an iterator to load data in chunks\n",
    "# Define read_large_file()\n",
    "def read_large_file(file_object):\n",
    "    \"\"\"A generator function to read a large file lazily.\"\"\"\n",
    "\n",
    "    # Loop indefinitely until the end of the file\n",
    "    while True:\n",
    "\n",
    "        # Read a line from the file: data\n",
    "        data = file_object.readline()\n",
    "\n",
    "        # Break if this is the end of the file\n",
    "        if not data:\n",
    "            break\n",
    "\n",
    "        # Yield the line of data\n",
    "        yield data\n",
    "        \n",
    "# Initialize an empty dictionary: counts_dict\n",
    "counts_dict = {}\n",
    "\n",
    "# Open a connection to the file\n",
    "with open('WDICSV.csv') as file:\n",
    "\n",
    "    # Iterate over the generator from read_large_file()\n",
    "    for line in read_large_file(file):\n",
    "\n",
    "        row = line.split(',')\n",
    "        first_col = row[0]\n",
    "\n",
    "        if first_col in counts_dict.keys():\n",
    "            counts_dict[first_col] += 1\n",
    "        else:\n",
    "            counts_dict[first_col] = 1\n",
    "\n",
    "# Print            \n",
    "print(counts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Country Name Country Code  \\\n",
      "0  Africa Eastern and Southern          AFE   \n",
      "1  Africa Eastern and Southern          AFE   \n",
      "2  Africa Eastern and Southern          AFE   \n",
      "3  Africa Eastern and Southern          AFE   \n",
      "4  Africa Eastern and Southern          AFE   \n",
      "5  Africa Eastern and Southern          AFE   \n",
      "6  Africa Eastern and Southern          AFE   \n",
      "7  Africa Eastern and Southern          AFE   \n",
      "8  Africa Eastern and Southern          AFE   \n",
      "9  Africa Eastern and Southern          AFE   \n",
      "\n",
      "                                      Indicator Name     Indicator Code  1960  \\\n",
      "0  Access to clean fuels and technologies for coo...     EG.CFT.ACCS.ZS   NaN   \n",
      "1  Access to clean fuels and technologies for coo...  EG.CFT.ACCS.RU.ZS   NaN   \n",
      "2  Access to clean fuels and technologies for coo...  EG.CFT.ACCS.UR.ZS   NaN   \n",
      "3            Access to electricity (% of population)     EG.ELC.ACCS.ZS   NaN   \n",
      "4  Access to electricity, rural (% of rural popul...  EG.ELC.ACCS.RU.ZS   NaN   \n",
      "5  Access to electricity, urban (% of urban popul...  EG.ELC.ACCS.UR.ZS   NaN   \n",
      "6  Account ownership at a financial institution o...     FX.OWN.TOTL.ZS   NaN   \n",
      "7  Account ownership at a financial institution o...  FX.OWN.TOTL.FE.ZS   NaN   \n",
      "8  Account ownership at a financial institution o...  FX.OWN.TOTL.MA.ZS   NaN   \n",
      "9  Account ownership at a financial institution o...  FX.OWN.TOTL.OL.ZS   NaN   \n",
      "\n",
      "   1961  1962  1963  1964  1965  ...       2014       2015       2016  \\\n",
      "0   NaN   NaN   NaN   NaN   NaN  ...  17.401410  17.911234  18.463874   \n",
      "1   NaN   NaN   NaN   NaN   NaN  ...   6.728819   7.005877   7.308571   \n",
      "2   NaN   NaN   NaN   NaN   NaN  ...  38.080931  38.422282  38.722108   \n",
      "3   NaN   NaN   NaN   NaN   NaN  ...  31.860474  33.903800  38.854624   \n",
      "4   NaN   NaN   NaN   NaN   NaN  ...  17.619475  16.500171  24.605861   \n",
      "5   NaN   NaN   NaN   NaN   NaN  ...  65.996032  67.025849  68.905537   \n",
      "6   NaN   NaN   NaN   NaN   NaN  ...        NaN        NaN        NaN   \n",
      "7   NaN   NaN   NaN   NaN   NaN  ...        NaN        NaN        NaN   \n",
      "8   NaN   NaN   NaN   NaN   NaN  ...        NaN        NaN        NaN   \n",
      "9   NaN   NaN   NaN   NaN   NaN  ...        NaN        NaN        NaN   \n",
      "\n",
      "        2017       2018       2019       2020       2021       2022  2023  \n",
      "0  18.924037  19.437054  20.026254  20.647969  21.165877  21.863139   NaN  \n",
      "1   7.547226   7.875917   8.243018   8.545483   8.906711   9.261320   NaN  \n",
      "2  38.993157  39.337872  39.695279  40.137847  40.522209  41.011132   NaN  \n",
      "3  40.199898  43.017148  44.381259  46.264875  48.100862  48.711995   NaN  \n",
      "4  25.396929  27.037528  29.137914  31.001049  32.777910  33.747907   NaN  \n",
      "5  70.677652  71.568639  72.630158  74.127419  75.566702  75.922121   NaN  \n",
      "6        NaN        NaN        NaN        NaN        NaN        NaN   NaN  \n",
      "7        NaN        NaN        NaN        NaN        NaN        NaN   NaN  \n",
      "8        NaN        NaN        NaN        NaN        NaN        NaN   NaN  \n",
      "9        NaN        NaN        NaN        NaN        NaN        NaN   NaN  \n",
      "\n",
      "[10 rows x 68 columns]\n"
     ]
    }
   ],
   "source": [
    "# Writing an iterator to load data in chunks\n",
    "\n",
    "import pandas as pd\n",
    "# Initialize reader object: df_reader\n",
    "df_reader = pd.read_csv('WDICSV.csv', chunksize = 10)\n",
    "# Print first chunk\n",
    "print(next(df_reader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS'), ('East Asia & Pacific', 'EAS')]\n"
     ]
    }
   ],
   "source": [
    "# Initialize reader object: urb_pop_reader\n",
    "urb_pop_reader = pd.read_csv('WDICSV.csv', chunksize=10000)\n",
    "\n",
    "# Get the first DataFrame chunk: df_urb_pop\n",
    "df_urb_pop = next(urb_pop_reader)\n",
    "\n",
    "# Check out specific country: df_pop_ceb\n",
    "df_pop_ceb = df_urb_pop[df_urb_pop['Country Code'] =='EAS']\n",
    "\n",
    "# Zip DataFrame columns of interest: pops\n",
    "pops = zip(df_pop_ceb['Country Name'],\n",
    "           df_pop_ceb['Country Code'])\n",
    "# Turn zip object into list: pops_list\n",
    "pops_list = list(pops)\n",
    "\n",
    "# Print pops_list\n",
    "print(pops_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from previous exercise\n",
    "urb_pop_reader = pd.read_csv('WDICSV.csv', chunksize=10000)\n",
    "df_urb_pop = next(urb_pop_reader)\n",
    "df_pop_ceb = df_urb_pop[df_urb_pop['Country Code'] =='EAS']\n",
    "pops = zip(df_pop_ceb['Country Name'], \n",
    "           df_pop_ceb['Country Code'])\n",
    "pops_list = list(pops)\n",
    "\n",
    "# Use list comprehension to create new DataFrame column 'Total Urban Population'\n",
    "df_pop_ceb['Total Urban Population'] = [int(tup[0] * tup[1] * 0.01) for tup in pops_list]\n",
    "\n",
    "# Plot urban population data\n",
    "df_pop_ceb.plot(kind='hist', x='Year', y='Total Urban Population')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     CarName  MPG  Sport?  GasolineCapacity  Cylinder\n",
      "0  alfaRomeo  100   False                60         4\n",
      "1     Nissan  150   False                45         6\n",
      "2     Toyota  200    True                65         8\n",
      "3        BMW  250    True                70         8\n",
      "4       Benz  300    True                75         4\n",
      "5       Jeep  350   False                65         6\n",
      "     CarName  MPG  Sport?  GasolineCapacity  Cylinder\n",
      "0  alfaRomeo  100   False                60         4\n",
      "1     Nissan  150   False                45         6\n",
      "2     Toyota  200    True                65         8\n",
      "3        BMW  250    True                70         8\n",
      "4       Benz  300    True                75         4\n",
      "5       Jeep  350   False                65         6\n"
     ]
    }
   ],
   "source": [
    "# Joining two Data Frames\n",
    "# Merging (equivalent of SQL \"JOIN\")\n",
    "import pandas as pd\n",
    "D1       = ['alfaRomeo','Nissan','Toyota','BMW','Benz','Jeep']\n",
    "D2       = [100,150,200,250,300,350]\n",
    "D3       = [False,False,True,True,True,False]\n",
    "Dic1     = {'CarName':D1 ,'MPG':D2, \"Sport?\":D3}               #Definnig a dic\n",
    "Cars     = pd.DataFrame(Dic1)                                  #Definnig Datafram using dic\n",
    "\n",
    "D4       = ['alfaRomeo','Nissan','Toyota','BMW','Benz','Jeep']\n",
    "D5       = [60,45,65,70,75,65]\n",
    "D6       = [4,6,8,8,4,6]\n",
    "Dic2     = {'CarName':D4 ,'GasolineCapacity':D5, 'Cylinder':D6}               #Definnig a dic\n",
    "CarsDetail= pd.DataFrame(Dic2)                                  #Definnig Datafram using dic\n",
    "# Method 1:\n",
    "# right dataframe.merge(right dataframe, how = 'join type', on=[''])\n",
    "dfnew1 = Cars.merge(CarsDetail, how='inner', on=['CarName'])\n",
    "print(dfnew1)\n",
    "\n",
    "# Method 2:\n",
    "dfnew2 = pd.merge(Cars,CarsDetail, how='inner', on=['CarName'])\n",
    "print(dfnew2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (1432359456.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[8], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    df_sales_data = pd.read_parquet(\"C:\\Users\\as.moradi\\Dropbox\\Python (Work)\\Personal-Information-Data sample\",engine=\"fastparquet\")\u001b[0m\n\u001b[1;37m                                                                                                               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "# Reading Parquet Files (ETL and ELT in Python Datacamp)\n",
    "import pandas as pd\n",
    "df_sales_data = pd.read_parquet(\"C:\\Users\\as.moradi\\Dropbox\\Python (Work)\\Personal-Information-Data sample\",engine=\"fastparquet\")\n",
    "\n",
    "print(df_sales_data.dtypes)\n",
    "print(df_sales_data.shape)\n",
    "print(df_sales_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2355864105.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[12], line 11\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip install \"apache-airflow==${AIRFLOW_VERSION}\" --constraint \"${CONSTRAINT_URL}\"\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# export AIRFLOW_HOME=~/airflow\n",
    "AIRFLOW_VERSION='2.10.1'\n",
    "\n",
    "# Extract the version of Python you have installed. If you're currently using a Python version that is not supported by Airflow, you may want to set this manually.\n",
    "# See above for supported versions.\n",
    "# PYTHON_VERSION=\"$(python -c 'import sys; print(f\"{sys.version_info.major}.{sys.version_info.minor}\")')\"\n",
    "\n",
    "CONSTRAINT_URL=\"https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt\"\n",
    "# For example this would install 2.10.1 with python 3.8: https://raw.githubusercontent.com/apache/airflow/constraints-2.10.1/constraints-3.8.txt\n",
    "\n",
    "pip install \"apache-airflow==${AIRFLOW_VERSION}\" --constraint \"${CONSTRAINT_URL}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting apache-airflow\n",
      "  Downloading apache_airflow-2.10.1-py3-none-any.whl.metadata (44 kB)\n",
      "     ---------------------------------------- 0.0/44.7 kB ? eta -:--:--\n",
      "     --------------------------- ------------ 30.7/44.7 kB 1.4 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 30.7/44.7 kB 1.4 MB/s eta 0:00:01\n",
      "     -------------------------------------- 44.7/44.7 kB 315.8 kB/s eta 0:00:00\n",
      "Collecting alembic<2.0,>=1.13.1 (from apache-airflow)\n",
      "  Downloading alembic-1.13.2-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting argcomplete>=1.10 (from apache-airflow)\n",
      "  Downloading argcomplete-3.5.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting asgiref>=2.3.0 (from apache-airflow)\n",
      "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: attrs>=22.1.0 in \\\\thhq-s07\\demprof\\it\\as.moradi\\appdata\\python\\python311\\site-packages (from apache-airflow) (23.2.0)\n",
      "Requirement already satisfied: blinker>=1.6.2 in \\\\thhq-s07\\demprof\\it\\as.moradi\\appdata\\python\\python311\\site-packages (from apache-airflow) (1.8.2)\n",
      "Collecting colorlog>=6.8.2 (from apache-airflow)\n",
      "  Downloading colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting configupdater>=3.1.1 (from apache-airflow)\n",
      "  Downloading ConfigUpdater-3.2-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting connexion<3.0,>=2.14.2 (from connexion[flask]<3.0,>=2.14.2->apache-airflow)\n",
      "  Downloading connexion-2.14.2-py2.py3-none-any.whl.metadata (28 kB)\n",
      "Collecting cron-descriptor>=1.2.24 (from apache-airflow)\n",
      "  Downloading cron_descriptor-1.4.5-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting croniter>=2.0.2 (from apache-airflow)\n",
      "  Downloading croniter-3.0.3-py2.py3-none-any.whl.metadata (28 kB)\n",
      "Requirement already satisfied: cryptography>=41.0.0 in \\\\thhq-s07\\demprof\\it\\as.moradi\\appdata\\python\\python311\\site-packages (from apache-airflow) (42.0.8)\n",
      "Collecting deprecated>=1.2.13 (from apache-airflow)\n",
      "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting dill>=0.2.2 (from apache-airflow)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting flask-caching>=2.0.0 (from apache-airflow)\n",
      "  Downloading Flask_Caching-2.3.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting flask-session<0.6,>=0.4.0 (from apache-airflow)\n",
      "  Downloading flask_session-0.5.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting flask-wtf>=1.1.0 (from apache-airflow)\n",
      "  Downloading flask_wtf-1.2.1-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting flask<2.3,>=2.2.1 (from apache-airflow)\n",
      "  Downloading Flask-2.2.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: fsspec>=2023.10.0 in \\\\thhq-s07\\demprof\\it\\as.moradi\\appdata\\python\\python311\\site-packages (from apache-airflow) (2024.6.1)\n",
      "Collecting google-re2>=1.0 (from apache-airflow)\n",
      "  Downloading google_re2-1.1.20240702-1-cp311-cp311-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting gunicorn>=20.1.0 (from apache-airflow)\n",
      "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: httpx>=0.25.0 in \\\\thhq-s07\\demprof\\it\\as.moradi\\appdata\\python\\python311\\site-packages (from apache-airflow) (0.27.0)\n",
      "Collecting importlib_metadata>=6.5 (from apache-airflow)\n",
      "  Downloading importlib_metadata-8.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in \\\\thhq-s07\\demprof\\it\\as.moradi\\appdata\\python\\python311\\site-packages (from apache-airflow) (2.2.0)\n",
      "Requirement already satisfied: jinja2>=3.0.0 in \\\\thhq-s07\\demprof\\it\\as.moradi\\appdata\\python\\python311\\site-packages (from apache-airflow) (3.1.4)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in \\\\thhq-s07\\demprof\\it\\as.moradi\\appdata\\python\\python311\\site-packages (from apache-airflow) (4.22.0)\n",
      "Collecting lazy-object-proxy>=1.2.0 (from apache-airflow)\n",
      "  Downloading lazy_object_proxy-1.10.0-cp311-cp311-win_amd64.whl.metadata (8.1 kB)\n",
      "Collecting linkify-it-py>=2.0.0 (from apache-airflow)\n",
      "  Downloading linkify_it_py-2.0.3-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting lockfile>=0.12.2 (from apache-airflow)\n",
      "  Downloading lockfile-0.12.2-py2.py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.1.0 in \\\\thhq-s07\\demprof\\it\\as.moradi\\appdata\\python\\python311\\site-packages (from apache-airflow) (3.0.0)\n",
      "Requirement already satisfied: markupsafe>=1.1.1 in \\\\thhq-s07\\demprof\\it\\as.moradi\\appdata\\python\\python311\\site-packages (from apache-airflow) (2.1.5)\n",
      "Collecting marshmallow-oneofschema>=2.0.1 (from apache-airflow)\n",
      "  Downloading marshmallow_oneofschema-3.1.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting mdit-py-plugins>=0.3.0 (from apache-airflow)\n",
      "  Downloading mdit_py_plugins-0.4.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting methodtools>=0.4.7 (from apache-airflow)\n",
      "  Downloading methodtools-0.4.7-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting opentelemetry-api>=1.15.0 (from apache-airflow)\n",
      "  Downloading opentelemetry_api-1.27.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting opentelemetry-exporter-otlp>=1.15.0 (from apache-airflow)\n",
      "  Downloading opentelemetry_exporter_otlp-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: packaging>=23.0 in \\\\thhq-s07\\demprof\\it\\as.moradi\\appdata\\python\\python311\\site-packages (from apache-airflow) (24.1)\n",
      "Collecting pathspec>=0.9.0 (from apache-airflow)\n",
      "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting pendulum<4.0,>=2.1.2 (from apache-airflow)\n",
      "  Downloading pendulum-3.0.0-cp311-none-win_amd64.whl.metadata (7.0 kB)\n",
      "Collecting pluggy>=1.5.0 (from apache-airflow)\n",
      "  Downloading pluggy-1.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: psutil>=5.8.0 in \\\\thhq-s07\\demprof\\it\\as.moradi\\appdata\\python\\python311\\site-packages (from apache-airflow) (6.0.0)\n",
      "Requirement already satisfied: pygments>=2.0.1 in \\\\thhq-s07\\demprof\\it\\as.moradi\\appdata\\python\\python311\\site-packages (from apache-airflow) (2.18.0)\n",
      "Collecting pyjwt>=2.0.0 (from apache-airflow)\n",
      "  Downloading PyJWT-2.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting python-daemon>=3.0.0 (from apache-airflow)\n",
      "  Downloading python_daemon-3.0.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in \\\\thhq-s07\\demprof\\it\\as.moradi\\appdata\\python\\python311\\site-packages (from apache-airflow) (2.9.0.post0)\n",
      "Collecting python-nvd3>=0.15.0 (from apache-airflow)\n",
      "  Downloading python-nvd3-0.16.0.tar.gz (34 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: python-slugify>=5.0 in \\\\thhq-s07\\demprof\\it\\as.moradi\\appdata\\python\\python311\\site-packages (from apache-airflow) (8.0.4)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in \\\\thhq-s07\\demprof\\it\\as.moradi\\appdata\\python\\python311\\site-packages (from apache-airflow) (2.32.3)\n",
      "Collecting requests-toolbelt>=0.4.0 (from apache-airflow)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: rfc3339-validator>=0.1.4 in c:\\program files\\python311\\lib\\site-packages (from apache-airflow) (0.1.4)\n",
      "Collecting rich-argparse>=1.0.0 (from apache-airflow)\n",
      "  Downloading rich_argparse-1.5.2-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: rich>=12.4.4 in \\\\thhq-s07\\demprof\\it\\as.moradi\\appdata\\python\\python311\\site-packages (from apache-airflow) (13.7.1)\n",
      "Collecting setproctitle>=1.3.3 (from apache-airflow)\n",
      "  Downloading setproctitle-1.3.3-cp311-cp311-win_amd64.whl.metadata (10 kB)\n",
      "Collecting sqlalchemy<2.0,>=1.4.36 (from apache-airflow)\n",
      "  Downloading SQLAlchemy-1.4.54-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Collecting sqlalchemy-jsonfield>=1.0 (from apache-airflow)\n",
      "  Downloading SQLAlchemy_JSONField-1.0.2-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting tabulate>=0.7.5 (from apache-airflow)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting tenacity!=8.2.0,>=8.0.0 (from apache-airflow)\n",
      "  Downloading tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in \\\\thhq-s07\\demprof\\it\\as.moradi\\appdata\\python\\python311\\site-packages (from apache-airflow) (2.4.0)\n",
      "Collecting unicodecsv>=0.14.1 (from apache-airflow)\n",
      "  Downloading unicodecsv-0.14.1.tar.gz (10 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting universal-pathlib>=0.2.2 (from apache-airflow)\n",
      "  Downloading universal_pathlib-0.2.5-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting werkzeug<3,>=2.0 (from apache-airflow)\n",
      "  Downloading werkzeug-2.3.8-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting apache-airflow-providers-common-compat (from apache-airflow)\n",
      "  Downloading apache_airflow_providers_common_compat-1.2.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting apache-airflow-providers-common-io (from apache-airflow)\n",
      "  Downloading apache_airflow_providers_common_io-1.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting apache-airflow-providers-common-sql (from apache-airflow)\n",
      "  Downloading apache_airflow_providers_common_sql-1.16.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting apache-airflow-providers-fab>=1.0.2 (from apache-airflow)\n",
      "  Downloading apache_airflow_providers_fab-1.3.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting apache-airflow-providers-ftp (from apache-airflow)\n",
      "  Downloading apache_airflow_providers_ftp-3.11.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting apache-airflow-providers-http (from apache-airflow)\n",
      "  Downloading apache_airflow_providers_http-4.13.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting apache-airflow-providers-imap (from apache-airflow)\n",
      "  Downloading apache_airflow_providers_imap-3.7.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting apache-airflow-providers-smtp (from apache-airflow)\n",
      "  Downloading apache_airflow_providers_smtp-1.8.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting apache-airflow-providers-sqlite (from apache-airflow)\n",
      "  Downloading apache_airflow_providers_sqlite-3.9.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting Mako (from alembic<2.0,>=1.13.1->apache-airflow)\n",
      "  Downloading Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4 in \\\\thhq-s07\\demprof\\it\\as.moradi\\appdata\\python\\python311\\site-packages (from alembic<2.0,>=1.13.1->apache-airflow) (4.12.2)\n",
      "Collecting flask-appbuilder==4.5.0 (from apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
      "  Downloading Flask_AppBuilder-4.5.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting flask-login>=0.6.2 (from apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
      "  Downloading Flask_Login-0.6.3-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: jmespath>=0.7.0 in c:\\program files\\python311\\lib\\site-packages (from apache-airflow-providers-fab>=1.0.2->apache-airflow) (1.0.1)\n",
      "Requirement already satisfied: apispec<7,>=6.0.0 in \\\\thhq-s07\\demprof\\it\\as.moradi\\appdata\\python\\python311\\site-packages (from apispec[yaml]<7,>=6.0.0->flask-appbuilder==4.5.0->apache-airflow-providers-fab>=1.0.2->apache-airflow) (6.4.0)\n",
      "Requirement already satisfied: colorama<1,>=0.3.9 in c:\\program files\\python311\\lib\\site-packages (from flask-appbuilder==4.5.0->apache-airflow-providers-fab>=1.0.2->apache-airflow) (0.4.6)\n",
      "Requirement already satisfied: click<9,>=8 in \\\\thhq-s07\\demprof\\it\\as.moradi\\appdata\\python\\python311\\site-packages (from flask-appbuilder==4.5.0->apache-airflow-providers-fab>=1.0.2->apache-airflow) (8.1.7)\n",
      "Collecting email-validator>=1.0.5 (from flask-appbuilder==4.5.0->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
      "  Downloading email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting Flask-Babel<3,>=1 (from flask-appbuilder==4.5.0->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
      "  Downloading Flask_Babel-2.0.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting Flask-Limiter<4,>3 (from flask-appbuilder==4.5.0->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
      "  Downloading Flask_Limiter-3.8.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting Flask-SQLAlchemy<3,>=2.4 (from flask-appbuilder==4.5.0->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
      "  Downloading Flask_SQLAlchemy-2.5.1-py2.py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting Flask-JWT-Extended<5.0.0,>=4.0.0 (from flask-appbuilder==4.5.0->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
      "  Downloading Flask_JWT_Extended-4.6.0-py2.py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: marshmallow<4,>=3.18.0 in \\\\thhq-s07\\demprof\\it\\as.moradi\\appdata\\python\\python311\\site-packages (from flask-appbuilder==4.5.0->apache-airflow-providers-fab>=1.0.2->apache-airflow) (3.20.2)\n",
      "Collecting marshmallow-sqlalchemy<0.29.0,>=0.22.0 (from flask-appbuilder==4.5.0->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
      "  Downloading marshmallow_sqlalchemy-0.28.2-py2.py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting prison<1.0.0,>=0.2.1 (from flask-appbuilder==4.5.0->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
      "  Downloading prison-0.2.1-py2.py3-none-any.whl.metadata (973 bytes)\n",
      "Collecting sqlalchemy-utils<1,>=0.32.21 (from flask-appbuilder==4.5.0->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
      "  Downloading SQLAlchemy_Utils-0.41.2-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting WTForms<4 (from flask-appbuilder==4.5.0->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
      "  Downloading wtforms-3.1.2-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting clickclick<21,>=1.2 (from connexion<3.0,>=2.14.2->connexion[flask]<3.0,>=2.14.2->apache-airflow)\n",
      "  Downloading clickclick-20.10.2-py2.py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: PyYAML<7,>=5.1 in c:\\program files\\python311\\lib\\site-packages (from connexion<3.0,>=2.14.2->connexion[flask]<3.0,>=2.14.2->apache-airflow) (6.0.1)\n",
      "Collecting inflection<0.6,>=0.3.1 (from connexion<3.0,>=2.14.2->connexion[flask]<3.0,>=2.14.2->apache-airflow)\n",
      "  Downloading inflection-0.5.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting werkzeug<3,>=2.0 (from apache-airflow)\n",
      "  Downloading Werkzeug-2.2.3-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: pytz>2021.1 in \\\\thhq-s07\\demprof\\it\\as.moradi\\appdata\\python\\python311\\site-packages (from croniter>=2.0.2->apache-airflow) (2023.3.post1)\n",
      "Requirement already satisfied: cffi>=1.12 in \\\\thhq-s07\\demprof\\it\\as.moradi\\appdata\\python\\python311\\site-packages (from cryptography>=41.0.0->apache-airflow) (1.16.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in \\\\thhq-s07\\demprof\\it\\as.moradi\\appdata\\python\\python311\\site-packages (from deprecated>=1.2.13->apache-airflow) (1.16.0)\n",
      "Collecting cachelib<0.10.0,>=0.9.0 (from flask-caching>=2.0.0->apache-airflow)\n",
      "  Downloading cachelib-0.9.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: anyio in \\\\thhq-s07\\demprof\\it\\as.moradi\\appdata\\python\\python311\\site-packages (from httpx>=0.25.0->apache-airflow) (4.4.0)\n",
      "Requirement already satisfied: certifi in \\\\thhq-s07\\demprof\\it\\as.moradi\\appdata\\python\\python311\\site-packages (from httpx>=0.25.0->apache-airflow) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in \\\\thhq-s07\\demprof\\it\\as.moradi\\appdata\\python\\python311\\site-packages (from httpx>=0.25.0->apache-airflow) (1.0.5)\n",
      "Requirement already satisfied: idna in \\\\thhq-s07\\demprof\\it\\as.moradi\\appdata\\python\\python311\\site-packages (from httpx>=0.25.0->apache-airflow) (3.7)\n",
      "Requirement already satisfied: sniffio in \\\\thhq-s07\\demprof\\it\\as.moradi\\appdata\\python\\python311\\site-packages (from httpx>=0.25.0->apache-airflow) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in \\\\thhq-s07\\demprof\\it\\as.moradi\\appdata\\python\\python311\\site-packages (from httpcore==1.*->httpx>=0.25.0->apache-airflow) (0.14.0)\n",
      "Collecting zipp>=3.20 (from importlib_metadata>=6.5->apache-airflow)\n",
      "  Downloading zipp-3.20.2-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in \\\\thhq-s07\\demprof\\it\\as.moradi\\appdata\\python\\python311\\site-packages (from jsonschema>=4.18.0->apache-airflow) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in \\\\thhq-s07\\demprof\\it\\as.moradi\\appdata\\python\\python311\\site-packages (from jsonschema>=4.18.0->apache-airflow) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in \\\\thhq-s07\\demprof\\it\\as.moradi\\appdata\\python\\python311\\site-packages (from jsonschema>=4.18.0->apache-airflow) (0.18.1)\n",
      "Collecting uc-micro-py (from linkify-it-py>=2.0.0->apache-airflow)\n",
      "  Downloading uc_micro_py-1.0.3-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: mdurl~=0.1 in \\\\thhq-s07\\demprof\\it\\as.moradi\\appdata\\python\\python311\\site-packages (from markdown-it-py>=2.1.0->apache-airflow) (0.1.2)\n",
      "Collecting wirerope>=0.4.7 (from methodtools>=0.4.7->apache-airflow)\n",
      "  Downloading wirerope-0.4.7-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting importlib_metadata>=6.5 (from apache-airflow)\n",
      "  Downloading importlib_metadata-8.4.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc==1.27.0 (from opentelemetry-exporter-otlp>=1.15.0->apache-airflow)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-http==1.27.0 (from opentelemetry-exporter-otlp>=1.15.0->apache-airflow)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_http-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc==1.27.0->opentelemetry-exporter-otlp>=1.15.0->apache-airflow)\n",
      "  Downloading googleapis_common_protos-1.65.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.0.0 in \\\\thhq-s07\\demprof\\it\\as.moradi\\appdata\\python\\python311\\site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.27.0->opentelemetry-exporter-otlp>=1.15.0->apache-airflow) (1.64.1)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.27.0 (from opentelemetry-exporter-otlp-proto-grpc==1.27.0->opentelemetry-exporter-otlp>=1.15.0->apache-airflow)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.27.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.27.0 (from opentelemetry-exporter-otlp-proto-grpc==1.27.0->opentelemetry-exporter-otlp>=1.15.0->apache-airflow)\n",
      "  Downloading opentelemetry_proto-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-sdk~=1.27.0 (from opentelemetry-exporter-otlp-proto-grpc==1.27.0->opentelemetry-exporter-otlp>=1.15.0->apache-airflow)\n",
      "  Downloading opentelemetry_sdk-1.27.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.19 in \\\\thhq-s07\\demprof\\it\\as.moradi\\appdata\\python\\python311\\site-packages (from opentelemetry-proto==1.27.0->opentelemetry-exporter-otlp-proto-grpc==1.27.0->opentelemetry-exporter-otlp>=1.15.0->apache-airflow) (4.25.3)\n",
      "Requirement already satisfied: tzdata>=2020.1 in \\\\thhq-s07\\demprof\\it\\as.moradi\\appdata\\python\\python311\\site-packages (from pendulum<4.0,>=2.1.2->apache-airflow) (2024.1)\n",
      "Collecting time-machine>=2.6.0 (from pendulum<4.0,>=2.1.2->apache-airflow)\n",
      "  Downloading time_machine-2.15.0-cp311-cp311-win_amd64.whl.metadata (21 kB)\n",
      "Collecting docutils (from python-daemon>=3.0.0->apache-airflow)\n",
      "  Downloading docutils-0.21.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: setuptools>=62.4.0 in c:\\program files\\python311\\lib\\site-packages (from python-daemon>=3.0.0->apache-airflow) (70.1.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\program files\\python311\\lib\\site-packages (from python-dateutil>=2.7.0->apache-airflow) (1.16.0)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in \\\\thhq-s07\\demprof\\it\\as.moradi\\appdata\\python\\python311\\site-packages (from python-slugify>=5.0->apache-airflow) (1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in \\\\thhq-s07\\demprof\\it\\as.moradi\\appdata\\python\\python311\\site-packages (from requests<3,>=2.27.0->apache-airflow) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in \\\\thhq-s07\\demprof\\it\\as.moradi\\appdata\\python\\python311\\site-packages (from requests<3,>=2.27.0->apache-airflow) (2.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in \\\\thhq-s07\\demprof\\it\\as.moradi\\appdata\\python\\python311\\site-packages (from sqlalchemy<2.0,>=1.4.36->apache-airflow) (3.0.3)\n",
      "Collecting more-itertools>=9.0.0 (from apache-airflow-providers-common-sql->apache-airflow)\n",
      "  Downloading more_itertools-10.5.0-py3-none-any.whl.metadata (36 kB)\n",
      "Collecting sqlparse>=0.4.2 (from apache-airflow-providers-common-sql->apache-airflow)\n",
      "  Downloading sqlparse-0.5.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting aiohttp>=3.9.2 (from apache-airflow-providers-http->apache-airflow)\n",
      "  Downloading aiohttp-3.10.5-cp311-cp311-win_amd64.whl.metadata (7.8 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp>=3.9.2->apache-airflow-providers-http->apache-airflow)\n",
      "  Downloading aiohappyeyeballs-2.4.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp>=3.9.2->apache-airflow-providers-http->apache-airflow)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp>=3.9.2->apache-airflow-providers-http->apache-airflow)\n",
      "  Downloading frozenlist-1.4.1-cp311-cp311-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp>=3.9.2->apache-airflow-providers-http->apache-airflow)\n",
      "  Downloading multidict-6.1.0-cp311-cp311-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp>=3.9.2->apache-airflow-providers-http->apache-airflow)\n",
      "  Downloading yarl-1.11.1-cp311-cp311-win_amd64.whl.metadata (49 kB)\n",
      "     ---------------------------------------- 0.0/49.8 kB ? eta -:--:--\n",
      "     ------------------------ --------------- 30.7/49.8 kB 1.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 49.8/49.8 kB 627.3 kB/s eta 0:00:00\n",
      "Requirement already satisfied: pycparser in \\\\thhq-s07\\demprof\\it\\as.moradi\\appdata\\python\\python311\\site-packages (from cffi>=1.12->cryptography>=41.0.0->apache-airflow) (2.22)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in \\\\thhq-s07\\demprof\\it\\as.moradi\\appdata\\python\\python311\\site-packages (from email-validator>=1.0.5->flask-appbuilder==4.5.0->apache-airflow-providers-fab>=1.0.2->apache-airflow) (2.6.1)\n",
      "Requirement already satisfied: Babel>=2.3 in \\\\thhq-s07\\demprof\\it\\as.moradi\\appdata\\python\\python311\\site-packages (from Flask-Babel<3,>=1->flask-appbuilder==4.5.0->apache-airflow-providers-fab>=1.0.2->apache-airflow) (2.15.0)\n",
      "Collecting limits>=3.13 (from Flask-Limiter<4,>3->flask-appbuilder==4.5.0->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
      "  Downloading limits-3.13.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: ordered-set<5,>4 in \\\\thhq-s07\\demprof\\it\\as.moradi\\appdata\\python\\python311\\site-packages (from Flask-Limiter<4,>3->flask-appbuilder==4.5.0->apache-airflow-providers-fab>=1.0.2->apache-airflow) (4.1.0)\n",
      "Collecting opentelemetry-semantic-conventions==0.48b0 (from opentelemetry-sdk~=1.27.0->opentelemetry-exporter-otlp-proto-grpc==1.27.0->opentelemetry-exporter-otlp>=1.15.0->apache-airflow)\n",
      "  Downloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting importlib-resources>=1.3 (from limits>=3.13->Flask-Limiter<4,>3->flask-appbuilder==4.5.0->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
      "  Downloading importlib_resources-6.4.5-py3-none-any.whl.metadata (4.0 kB)\n",
      "Downloading apache_airflow-2.10.1-py3-none-any.whl (13.4 MB)\n",
      "   ---------------------------------------- 0.0/13.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/13.4 MB 1.3 MB/s eta 0:00:11\n",
      "   ---------------------------------------- 0.1/13.4 MB 1.1 MB/s eta 0:00:13\n",
      "   ---------------------------------------- 0.1/13.4 MB 744.7 kB/s eta 0:00:18\n",
      "   ---------------------------------------- 0.1/13.4 MB 726.2 kB/s eta 0:00:19\n",
      "   ---------------------------------------- 0.2/13.4 MB 702.7 kB/s eta 0:00:19\n",
      "    --------------------------------------- 0.2/13.4 MB 697.2 kB/s eta 0:00:20\n",
      "    --------------------------------------- 0.2/13.4 MB 655.1 kB/s eta 0:00:21\n",
      "    --------------------------------------- 0.3/13.4 MB 714.4 kB/s eta 0:00:19\n",
      "    --------------------------------------- 0.3/13.4 MB 710.0 kB/s eta 0:00:19\n",
      "    --------------------------------------- 0.3/13.4 MB 679.3 kB/s eta 0:00:20\n",
      "   - -------------------------------------- 0.3/13.4 MB 675.6 kB/s eta 0:00:20\n",
      "   - -------------------------------------- 0.4/13.4 MB 655.2 kB/s eta 0:00:20\n",
      "   - -------------------------------------- 0.4/13.4 MB 654.4 kB/s eta 0:00:20\n",
      "   - -------------------------------------- 0.4/13.4 MB 671.5 kB/s eta 0:00:20\n",
      "   - -------------------------------------- 0.5/13.4 MB 685.7 kB/s eta 0:00:19\n",
      "   - -------------------------------------- 0.5/13.4 MB 683.6 kB/s eta 0:00:19\n",
      "   - -------------------------------------- 0.5/13.4 MB 667.8 kB/s eta 0:00:20\n",
      "   - -------------------------------------- 0.6/13.4 MB 680.1 kB/s eta 0:00:19\n",
      "   - -------------------------------------- 0.6/13.4 MB 690.6 kB/s eta 0:00:19\n",
      "   - -------------------------------------- 0.6/13.4 MB 678.1 kB/s eta 0:00:19\n",
      "   - -------------------------------------- 0.6/13.4 MB 677.5 kB/s eta 0:00:19\n",
      "   -- ------------------------------------- 0.7/13.4 MB 686.1 kB/s eta 0:00:19\n",
      "   -- ------------------------------------- 0.7/13.4 MB 674.8 kB/s eta 0:00:19\n",
      "   -- ------------------------------------- 0.7/13.4 MB 673.8 kB/s eta 0:00:19\n",
      "   -- ------------------------------------- 0.8/13.4 MB 682.1 kB/s eta 0:00:19\n",
      "   -- ------------------------------------- 0.8/13.4 MB 681.5 kB/s eta 0:00:19\n",
      "   -- ------------------------------------- 0.8/13.4 MB 679.9 kB/s eta 0:00:19\n",
      "   -- ------------------------------------- 0.9/13.4 MB 687.5 kB/s eta 0:00:19\n",
      "   -- ------------------------------------- 0.9/13.4 MB 678.5 kB/s eta 0:00:19\n",
      "   -- ------------------------------------- 1.0/13.4 MB 685.1 kB/s eta 0:00:19\n",
      "   -- ------------------------------------- 1.0/13.4 MB 684.1 kB/s eta 0:00:19\n",
      "   --- ------------------------------------ 1.0/13.4 MB 690.2 kB/s eta 0:00:18\n",
      "   --- ------------------------------------ 1.0/13.4 MB 689.5 kB/s eta 0:00:18\n",
      "   --- ------------------------------------ 1.1/13.4 MB 688.4 kB/s eta 0:00:18\n",
      "   --- ------------------------------------ 1.1/13.4 MB 693.3 kB/s eta 0:00:18\n",
      "   --- ------------------------------------ 1.2/13.4 MB 692.1 kB/s eta 0:00:18\n",
      "   --- ------------------------------------ 1.2/13.4 MB 691.1 kB/s eta 0:00:18\n",
      "   --- ------------------------------------ 1.2/13.4 MB 690.5 kB/s eta 0:00:18\n",
      "   --- ------------------------------------ 1.2/13.4 MB 695.8 kB/s eta 0:00:18\n",
      "   --- ------------------------------------ 1.2/13.4 MB 695.8 kB/s eta 0:00:18\n",
      "   --- ------------------------------------ 1.3/13.4 MB 671.5 kB/s eta 0:00:19\n",
      "   --- ------------------------------------ 1.3/13.4 MB 681.6 kB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 1.4/13.4 MB 680.9 kB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 1.4/13.4 MB 685.4 kB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 1.4/13.4 MB 689.6 kB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 1.5/13.4 MB 689.1 kB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 1.5/13.4 MB 688.3 kB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 1.5/13.4 MB 692.3 kB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 1.6/13.4 MB 696.3 kB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 1.6/13.4 MB 695.4 kB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 1.7/13.4 MB 698.6 kB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 1.7/13.4 MB 702.0 kB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 1.7/13.4 MB 701.0 kB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 1.8/13.4 MB 700.4 kB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 1.8/13.4 MB 699.7 kB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 1.8/13.4 MB 702.7 kB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 1.8/13.4 MB 702.7 kB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 1.9/13.4 MB 705.1 kB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 1.9/13.4 MB 704.4 kB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 1.9/13.4 MB 704.4 kB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 1.9/13.4 MB 704.4 kB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 1.9/13.4 MB 704.4 kB/s eta 0:00:17\n",
      "   ------ --------------------------------- 2.0/13.4 MB 697.5 kB/s eta 0:00:17\n",
      "   ------ --------------------------------- 2.1/13.4 MB 693.0 kB/s eta 0:00:17\n",
      "   ------ --------------------------------- 2.1/13.4 MB 696.0 kB/s eta 0:00:17\n",
      "   ------ --------------------------------- 2.1/13.4 MB 688.7 kB/s eta 0:00:17\n",
      "   ------ --------------------------------- 2.1/13.4 MB 688.4 kB/s eta 0:00:17\n",
      "   ------ --------------------------------- 2.2/13.4 MB 691.4 kB/s eta 0:00:17\n",
      "   ------ --------------------------------- 2.2/13.4 MB 683.9 kB/s eta 0:00:17\n",
      "   ------ --------------------------------- 2.2/13.4 MB 686.9 kB/s eta 0:00:17\n",
      "   ------ --------------------------------- 2.3/13.4 MB 683.3 kB/s eta 0:00:17\n",
      "   ------ --------------------------------- 2.3/13.4 MB 683.1 kB/s eta 0:00:17\n",
      "   ------ --------------------------------- 2.3/13.4 MB 676.4 kB/s eta 0:00:17\n",
      "   ------ --------------------------------- 2.3/13.4 MB 679.3 kB/s eta 0:00:17\n",
      "   ------- -------------------------------- 2.4/13.4 MB 672.9 kB/s eta 0:00:17\n",
      "   ------- -------------------------------- 2.4/13.4 MB 678.4 kB/s eta 0:00:17\n",
      "   ------- -------------------------------- 2.4/13.4 MB 675.4 kB/s eta 0:00:17\n",
      "   ------- -------------------------------- 2.4/13.4 MB 672.3 kB/s eta 0:00:17\n",
      "   ------- -------------------------------- 2.5/13.4 MB 674.9 kB/s eta 0:00:17\n",
      "   ------- -------------------------------- 2.5/13.4 MB 674.6 kB/s eta 0:00:17\n",
      "   ------- -------------------------------- 2.5/13.4 MB 671.7 kB/s eta 0:00:17\n",
      "   ------- -------------------------------- 2.6/13.4 MB 671.4 kB/s eta 0:00:17\n",
      "   ------- -------------------------------- 2.6/13.4 MB 671.2 kB/s eta 0:00:17\n",
      "   ------- -------------------------------- 2.6/13.4 MB 665.7 kB/s eta 0:00:17\n",
      "   ------- -------------------------------- 2.6/13.4 MB 668.4 kB/s eta 0:00:17\n",
      "   ------- -------------------------------- 2.7/13.4 MB 665.6 kB/s eta 0:00:17\n",
      "   -------- ------------------------------- 2.7/13.4 MB 667.9 kB/s eta 0:00:17\n",
      "   -------- ------------------------------- 2.7/13.4 MB 662.8 kB/s eta 0:00:17\n",
      "   -------- ------------------------------- 2.8/13.4 MB 665.3 kB/s eta 0:00:17\n",
      "   -------- ------------------------------- 2.8/13.4 MB 662.6 kB/s eta 0:00:17\n",
      "   -------- ------------------------------- 2.8/13.4 MB 665.0 kB/s eta 0:00:16\n",
      "   -------- ------------------------------- 2.9/13.4 MB 667.3 kB/s eta 0:00:16\n",
      "   -------- ------------------------------- 2.9/13.4 MB 667.2 kB/s eta 0:00:16\n",
      "   -------- ------------------------------- 2.9/13.4 MB 669.4 kB/s eta 0:00:16\n",
      "   -------- ------------------------------- 3.0/13.4 MB 669.2 kB/s eta 0:00:16\n",
      "   -------- ------------------------------- 3.0/13.4 MB 669.2 kB/s eta 0:00:16\n",
      "   -------- ------------------------------- 3.0/13.4 MB 662.1 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 3.1/13.4 MB 668.9 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 3.1/13.4 MB 666.4 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 3.1/13.4 MB 664.0 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 3.1/13.4 MB 663.9 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 3.1/13.4 MB 664.0 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 3.2/13.4 MB 663.9 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 3.2/13.4 MB 665.8 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 3.3/13.4 MB 663.6 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 3.3/13.4 MB 667.8 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 3.3/13.4 MB 663.6 kB/s eta 0:00:16\n",
      "   ---------- ----------------------------- 3.4/13.4 MB 667.6 kB/s eta 0:00:16\n",
      "   ---------- ----------------------------- 3.4/13.4 MB 665.4 kB/s eta 0:00:16\n",
      "   ---------- ----------------------------- 3.4/13.4 MB 667.3 kB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 3.5/13.4 MB 669.2 kB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 3.5/13.4 MB 669.0 kB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 3.5/13.4 MB 670.9 kB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 3.6/13.4 MB 670.7 kB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 3.6/13.4 MB 670.6 kB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 3.7/13.4 MB 672.3 kB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 3.7/13.4 MB 672.2 kB/s eta 0:00:15\n",
      "   ----------- ---------------------------- 3.7/13.4 MB 674.0 kB/s eta 0:00:15\n",
      "   ----------- ---------------------------- 3.8/13.4 MB 675.7 kB/s eta 0:00:15\n",
      "   ----------- ---------------------------- 3.8/13.4 MB 669.9 kB/s eta 0:00:15\n",
      "   ----------- ---------------------------- 3.8/13.4 MB 675.3 kB/s eta 0:00:15\n",
      "   ----------- ---------------------------- 3.8/13.4 MB 675.2 kB/s eta 0:00:15\n",
      "   ----------- ---------------------------- 3.9/13.4 MB 671.4 kB/s eta 0:00:15\n",
      "   ----------- ---------------------------- 3.9/13.4 MB 676.5 kB/s eta 0:00:15\n",
      "   ----------- ---------------------------- 3.9/13.4 MB 674.6 kB/s eta 0:00:15\n",
      "   ----------- ---------------------------- 4.0/13.4 MB 674.5 kB/s eta 0:00:15\n",
      "   ----------- ---------------------------- 4.0/13.4 MB 677.8 kB/s eta 0:00:14\n",
      "   ------------ --------------------------- 4.0/13.4 MB 677.6 kB/s eta 0:00:14\n",
      "   ------------ --------------------------- 4.1/13.4 MB 679.1 kB/s eta 0:00:14\n",
      "   ------------ --------------------------- 4.1/13.4 MB 679.0 kB/s eta 0:00:14\n",
      "   ------------ --------------------------- 4.1/13.4 MB 677.2 kB/s eta 0:00:14\n",
      "   ------------ --------------------------- 4.2/13.4 MB 680.3 kB/s eta 0:00:14\n",
      "   ------------ --------------------------- 4.2/13.4 MB 678.4 kB/s eta 0:00:14\n",
      "   ------------ --------------------------- 4.3/13.4 MB 679.9 kB/s eta 0:00:14\n",
      "   ------------ --------------------------- 4.3/13.4 MB 679.7 kB/s eta 0:00:14\n",
      "   ------------ --------------------------- 4.3/13.4 MB 679.5 kB/s eta 0:00:14\n",
      "   ------------ --------------------------- 4.4/13.4 MB 679.4 kB/s eta 0:00:14\n",
      "   ------------- -------------------------- 4.4/13.4 MB 684.0 kB/s eta 0:00:14\n",
      "   ------------- -------------------------- 4.4/13.4 MB 683.8 kB/s eta 0:00:14\n",
      "   ------------- -------------------------- 4.5/13.4 MB 681.9 kB/s eta 0:00:14\n",
      "   ------------- -------------------------- 4.5/13.4 MB 681.8 kB/s eta 0:00:14\n",
      "   ------------- -------------------------- 4.5/13.4 MB 684.7 kB/s eta 0:00:13\n",
      "   ------------- -------------------------- 4.6/13.4 MB 683.0 kB/s eta 0:00:13\n",
      "   ------------- -------------------------- 4.6/13.4 MB 684.4 kB/s eta 0:00:13\n",
      "   ------------- -------------------------- 4.6/13.4 MB 684.4 kB/s eta 0:00:13\n",
      "   ------------- -------------------------- 4.6/13.4 MB 684.4 kB/s eta 0:00:13\n",
      "   ------------- -------------------------- 4.6/13.4 MB 670.3 kB/s eta 0:00:14\n",
      "   ------------- -------------------------- 4.6/13.4 MB 668.6 kB/s eta 0:00:14\n",
      "   ------------- -------------------------- 4.7/13.4 MB 674.4 kB/s eta 0:00:13\n",
      "   -------------- ------------------------- 4.7/13.4 MB 674.4 kB/s eta 0:00:13\n",
      "   -------------- ------------------------- 4.7/13.4 MB 671.3 kB/s eta 0:00:13\n",
      "   -------------- ------------------------- 4.8/13.4 MB 672.7 kB/s eta 0:00:13\n",
      "   -------------- ------------------------- 4.8/13.4 MB 669.7 kB/s eta 0:00:13\n",
      "   -------------- ------------------------- 4.8/13.4 MB 669.6 kB/s eta 0:00:13\n",
      "   -------------- ------------------------- 4.8/13.4 MB 669.5 kB/s eta 0:00:13\n",
      "   -------------- ------------------------- 4.9/13.4 MB 669.4 kB/s eta 0:00:13\n",
      "   -------------- ------------------------- 4.9/13.4 MB 667.9 kB/s eta 0:00:13\n",
      "   -------------- ------------------------- 4.9/13.4 MB 669.2 kB/s eta 0:00:13\n",
      "   -------------- ------------------------- 5.0/13.4 MB 666.4 kB/s eta 0:00:13\n",
      "   -------------- ------------------------- 5.0/13.4 MB 666.3 kB/s eta 0:00:13\n",
      "   -------------- ------------------------- 5.0/13.4 MB 667.6 kB/s eta 0:00:13\n",
      "   --------------- ------------------------ 5.1/13.4 MB 668.9 kB/s eta 0:00:13\n",
      "   --------------- ------------------------ 5.1/13.4 MB 666.1 kB/s eta 0:00:13\n",
      "   --------------- ------------------------ 5.1/13.4 MB 667.4 kB/s eta 0:00:13\n",
      "   --------------- ------------------------ 5.1/13.4 MB 667.3 kB/s eta 0:00:13\n",
      "   --------------- ------------------------ 5.2/13.4 MB 665.9 kB/s eta 0:00:13\n",
      "   --------------- ------------------------ 5.2/13.4 MB 665.9 kB/s eta 0:00:13\n",
      "   --------------- ------------------------ 5.2/13.4 MB 667.1 kB/s eta 0:00:13\n",
      "   --------------- ------------------------ 5.3/13.4 MB 665.8 kB/s eta 0:00:13\n",
      "   --------------- ------------------------ 5.3/13.4 MB 667.0 kB/s eta 0:00:13\n",
      "   --------------- ------------------------ 5.3/13.4 MB 667.0 kB/s eta 0:00:13\n",
      "   --------------- ------------------------ 5.4/13.4 MB 665.5 kB/s eta 0:00:13\n",
      "   --------------- ------------------------ 5.4/13.4 MB 664.2 kB/s eta 0:00:13\n",
      "   ---------------- ----------------------- 5.4/13.4 MB 662.9 kB/s eta 0:00:13\n",
      "   ---------------- ----------------------- 5.4/13.4 MB 664.1 kB/s eta 0:00:13\n",
      "   ---------------- ----------------------- 5.4/13.4 MB 664.1 kB/s eta 0:00:13\n",
      "   ---------------- ----------------------- 5.5/13.4 MB 664.0 kB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 5.5/13.4 MB 663.9 kB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 5.6/13.4 MB 663.9 kB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 5.6/13.4 MB 665.1 kB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 5.6/13.4 MB 665.1 kB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 5.7/13.4 MB 665.0 kB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 5.7/13.4 MB 665.0 kB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 5.7/13.4 MB 665.0 kB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 5.7/13.4 MB 662.4 kB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 5.8/13.4 MB 663.6 kB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 5.8/13.4 MB 660.0 kB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 5.8/13.4 MB 660.0 kB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 5.8/13.4 MB 661.1 kB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 5.9/13.4 MB 660.0 kB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 5.9/13.4 MB 659.9 kB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 5.9/13.4 MB 659.9 kB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 6.0/13.4 MB 661.0 kB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 6.0/13.4 MB 659.9 kB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 6.0/13.4 MB 662.1 kB/s eta 0:00:12\n",
      "   ------------------ --------------------- 6.1/13.4 MB 662.1 kB/s eta 0:00:12\n",
      "   ------------------ --------------------- 6.1/13.4 MB 660.9 kB/s eta 0:00:12\n",
      "   ------------------ --------------------- 6.1/13.4 MB 663.1 kB/s eta 0:00:12\n",
      "   ------------------ --------------------- 6.2/13.4 MB 662.0 kB/s eta 0:00:11\n",
      "   ------------------ --------------------- 6.2/13.4 MB 664.1 kB/s eta 0:00:11\n",
      "   ------------------ --------------------- 6.2/13.4 MB 664.1 kB/s eta 0:00:11\n",
      "   ------------------ --------------------- 6.3/13.4 MB 662.9 kB/s eta 0:00:11\n",
      "   ------------------ --------------------- 6.3/13.4 MB 665.0 kB/s eta 0:00:11\n",
      "   ------------------ --------------------- 6.3/13.4 MB 665.1 kB/s eta 0:00:11\n",
      "   ------------------ --------------------- 6.4/13.4 MB 662.8 kB/s eta 0:00:11\n",
      "   ------------------- -------------------- 6.4/13.4 MB 666.0 kB/s eta 0:00:11\n",
      "   ------------------- -------------------- 6.4/13.4 MB 666.0 kB/s eta 0:00:11\n",
      "   ------------------- -------------------- 6.5/13.4 MB 663.8 kB/s eta 0:00:11\n",
      "   ------------------- -------------------- 6.5/13.4 MB 663.8 kB/s eta 0:00:11\n",
      "   ------------------- -------------------- 6.5/13.4 MB 663.8 kB/s eta 0:00:11\n",
      "   ------------------- -------------------- 6.5/13.4 MB 663.8 kB/s eta 0:00:11\n",
      "   ------------------- -------------------- 6.5/13.4 MB 653.2 kB/s eta 0:00:11\n",
      "   ------------------- -------------------- 6.5/13.4 MB 653.3 kB/s eta 0:00:11\n",
      "   ------------------- -------------------- 6.5/13.4 MB 650.2 kB/s eta 0:00:11\n",
      "   ------------------- -------------------- 6.6/13.4 MB 652.3 kB/s eta 0:00:11\n",
      "   ------------------- -------------------- 6.6/13.4 MB 652.3 kB/s eta 0:00:11\n",
      "   ------------------- -------------------- 6.6/13.4 MB 649.3 kB/s eta 0:00:11\n",
      "   ------------------- -------------------- 6.6/13.4 MB 648.3 kB/s eta 0:00:11\n",
      "   ------------------- -------------------- 6.6/13.4 MB 646.3 kB/s eta 0:00:11\n",
      "   ------------------- -------------------- 6.7/13.4 MB 648.4 kB/s eta 0:00:11\n",
      "   ------------------- -------------------- 6.7/13.4 MB 646.4 kB/s eta 0:00:11\n",
      "   ------------------- -------------------- 6.7/13.4 MB 645.5 kB/s eta 0:00:11\n",
      "   -------------------- ------------------- 6.7/13.4 MB 646.5 kB/s eta 0:00:11\n",
      "   -------------------- ------------------- 6.7/13.4 MB 646.5 kB/s eta 0:00:11\n",
      "   -------------------- ------------------- 6.8/13.4 MB 644.6 kB/s eta 0:00:11\n",
      "   -------------------- ------------------- 6.8/13.4 MB 644.7 kB/s eta 0:00:11\n",
      "   -------------------- ------------------- 6.8/13.4 MB 643.7 kB/s eta 0:00:11\n",
      "   -------------------- ------------------- 6.9/13.4 MB 644.8 kB/s eta 0:00:11\n",
      "   -------------------- ------------------- 6.9/13.4 MB 643.8 kB/s eta 0:00:11\n",
      "   -------------------- ------------------- 6.9/13.4 MB 643.0 kB/s eta 0:00:11\n",
      "   -------------------- ------------------- 6.9/13.4 MB 643.9 kB/s eta 0:00:11\n",
      "   -------------------- ------------------- 7.0/13.4 MB 644.0 kB/s eta 0:00:11\n",
      "   -------------------- ------------------- 7.0/13.4 MB 643.1 kB/s eta 0:00:10\n",
      "   -------------------- ------------------- 7.0/13.4 MB 642.2 kB/s eta 0:00:10\n",
      "   --------------------- ------------------ 7.1/13.4 MB 643.2 kB/s eta 0:00:10\n",
      "   --------------------- ------------------ 7.1/13.4 MB 642.3 kB/s eta 0:00:10\n",
      "   --------------------- ------------------ 7.1/13.4 MB 643.3 kB/s eta 0:00:10\n",
      "   --------------------- ------------------ 7.2/13.4 MB 644.3 kB/s eta 0:00:10\n",
      "   --------------------- ------------------ 7.2/13.4 MB 642.5 kB/s eta 0:00:10\n",
      "   --------------------- ------------------ 7.2/13.4 MB 643.4 kB/s eta 0:00:10\n",
      "   --------------------- ------------------ 7.2/13.4 MB 642.6 kB/s eta 0:00:10\n",
      "   --------------------- ------------------ 7.3/13.4 MB 642.7 kB/s eta 0:00:10\n",
      "   --------------------- ------------------ 7.3/13.4 MB 643.6 kB/s eta 0:00:10\n",
      "   --------------------- ------------------ 7.3/13.4 MB 643.6 kB/s eta 0:00:10\n",
      "   --------------------- ------------------ 7.4/13.4 MB 642.8 kB/s eta 0:00:10\n",
      "   --------------------- ------------------ 7.4/13.4 MB 642.8 kB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 7.4/13.4 MB 642.9 kB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 7.5/13.4 MB 643.0 kB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 7.5/13.4 MB 643.9 kB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 7.5/13.4 MB 643.9 kB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 7.6/13.4 MB 644.0 kB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 7.6/13.4 MB 645.8 kB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 7.6/13.4 MB 645.8 kB/s eta 0:00:09\n",
      "   ---------------------- ----------------- 7.7/13.4 MB 645.0 kB/s eta 0:00:09\n",
      "   ---------------------- ----------------- 7.7/13.4 MB 645.0 kB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 7.7/13.4 MB 645.9 kB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 7.8/13.4 MB 645.1 kB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 7.8/13.4 MB 645.1 kB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 7.8/13.4 MB 646.9 kB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 7.9/13.4 MB 645.2 kB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 7.9/13.4 MB 646.9 kB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 7.9/13.4 MB 644.5 kB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 8.0/13.4 MB 647.0 kB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 8.0/13.4 MB 645.3 kB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 8.0/13.4 MB 644.6 kB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 8.0/13.4 MB 646.3 kB/s eta 0:00:09\n",
      "   ------------------------ --------------- 8.1/13.4 MB 647.1 kB/s eta 0:00:09\n",
      "   ------------------------ --------------- 8.1/13.4 MB 646.3 kB/s eta 0:00:09\n",
      "   ------------------------ --------------- 8.2/13.4 MB 647.2 kB/s eta 0:00:09\n",
      "   ------------------------ --------------- 8.2/13.4 MB 647.2 kB/s eta 0:00:09\n",
      "   ------------------------ --------------- 8.2/13.4 MB 648.1 kB/s eta 0:00:09\n",
      "   ------------------------ --------------- 8.3/13.4 MB 648.1 kB/s eta 0:00:08\n",
      "   ------------------------ --------------- 8.3/13.4 MB 648.9 kB/s eta 0:00:08\n",
      "   ------------------------ --------------- 8.3/13.4 MB 649.7 kB/s eta 0:00:08\n",
      "   ------------------------ --------------- 8.3/13.4 MB 649.7 kB/s eta 0:00:08\n",
      "   ------------------------ --------------- 8.4/13.4 MB 649.8 kB/s eta 0:00:08\n",
      "   ------------------------- -------------- 8.4/13.4 MB 650.6 kB/s eta 0:00:08\n",
      "   ------------------------- -------------- 8.5/13.4 MB 650.6 kB/s eta 0:00:08\n",
      "   ------------------------- -------------- 8.5/13.4 MB 650.6 kB/s eta 0:00:08\n",
      "   ------------------------- -------------- 8.5/13.4 MB 651.4 kB/s eta 0:00:08\n",
      "   ------------------------- -------------- 8.6/13.4 MB 652.2 kB/s eta 0:00:08\n",
      "   ------------------------- -------------- 8.6/13.4 MB 653.0 kB/s eta 0:00:08\n",
      "   ------------------------- -------------- 8.6/13.4 MB 652.2 kB/s eta 0:00:08\n",
      "   ------------------------- -------------- 8.7/13.4 MB 651.5 kB/s eta 0:00:08\n",
      "   ------------------------- -------------- 8.7/13.4 MB 653.0 kB/s eta 0:00:08\n",
      "   -------------------------- ------------- 8.7/13.4 MB 652.3 kB/s eta 0:00:08\n",
      "   -------------------------- ------------- 8.8/13.4 MB 653.8 kB/s eta 0:00:08\n",
      "   -------------------------- ------------- 8.8/13.4 MB 653.8 kB/s eta 0:00:08\n",
      "   -------------------------- ------------- 8.8/13.4 MB 653.1 kB/s eta 0:00:08\n",
      "   -------------------------- ------------- 8.9/13.4 MB 653.1 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 8.9/13.4 MB 653.8 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 8.9/13.4 MB 653.1 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 8.9/13.4 MB 652.3 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 9.0/13.4 MB 653.8 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 9.0/13.4 MB 653.9 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 9.0/13.4 MB 653.9 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 9.0/13.4 MB 653.9 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 9.0/13.4 MB 653.9 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 9.1/13.4 MB 653.1 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 9.2/13.4 MB 651.7 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 9.2/13.4 MB 652.4 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 9.2/13.4 MB 651.7 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 9.2/13.4 MB 650.3 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 9.3/13.4 MB 650.3 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 9.3/13.4 MB 650.3 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 9.3/13.4 MB 649.6 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 9.3/13.4 MB 649.6 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 9.3/13.4 MB 648.2 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 9.4/13.4 MB 649.0 kB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 9.4/13.4 MB 648.3 kB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 9.4/13.4 MB 648.3 kB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 9.5/13.4 MB 648.3 kB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 9.5/13.4 MB 647.6 kB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 9.5/13.4 MB 647.7 kB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 9.5/13.4 MB 647.0 kB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 9.6/13.4 MB 647.0 kB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 9.6/13.4 MB 647.7 kB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 9.6/13.4 MB 648.4 kB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 9.7/13.4 MB 647.8 kB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 9.7/13.4 MB 646.5 kB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 9.7/13.4 MB 647.8 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 9.8/13.4 MB 647.2 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 9.8/13.4 MB 647.9 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 9.8/13.4 MB 647.2 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 9.9/13.4 MB 647.9 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 9.9/13.4 MB 647.3 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 9.9/13.4 MB 646.6 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 9.9/13.4 MB 647.3 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 10.0/13.4 MB 646.7 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 10.0/13.4 MB 646.7 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 10.0/13.4 MB 646.7 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 10.1/13.4 MB 646.8 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 10.1/13.4 MB 646.8 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 10.1/13.4 MB 647.5 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 10.2/13.4 MB 648.1 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 10.2/13.4 MB 648.8 kB/s eta 0:00:05\n",
      "   ------------------------------ --------- 10.2/13.4 MB 648.8 kB/s eta 0:00:05\n",
      "   ------------------------------ --------- 10.3/13.4 MB 648.2 kB/s eta 0:00:05\n",
      "   ------------------------------ --------- 10.3/13.4 MB 648.2 kB/s eta 0:00:05\n",
      "   ------------------------------ --------- 10.3/13.4 MB 648.8 kB/s eta 0:00:05\n",
      "   ------------------------------ --------- 10.4/13.4 MB 650.2 kB/s eta 0:00:05\n",
      "   ------------------------------ --------- 10.4/13.4 MB 650.2 kB/s eta 0:00:05\n",
      "   ------------------------------- -------- 10.4/13.4 MB 650.2 kB/s eta 0:00:05\n",
      "   ------------------------------- -------- 10.5/13.4 MB 650.2 kB/s eta 0:00:05\n",
      "   ------------------------------- -------- 10.5/13.4 MB 647.6 kB/s eta 0:00:05\n",
      "   ------------------------------- -------- 10.5/13.4 MB 648.2 kB/s eta 0:00:05\n",
      "   ------------------------------- -------- 10.6/13.4 MB 649.5 kB/s eta 0:00:05\n",
      "   ------------------------------- -------- 10.6/13.4 MB 650.1 kB/s eta 0:00:05\n",
      "   ------------------------------- -------- 10.6/13.4 MB 650.8 kB/s eta 0:00:05\n",
      "   ------------------------------- -------- 10.7/13.4 MB 650.8 kB/s eta 0:00:05\n",
      "   ------------------------------- -------- 10.7/13.4 MB 650.8 kB/s eta 0:00:05\n",
      "   ------------------------------- -------- 10.7/13.4 MB 652.1 kB/s eta 0:00:05\n",
      "   -------------------------------- ------- 10.8/13.4 MB 652.1 kB/s eta 0:00:05\n",
      "   -------------------------------- ------- 10.8/13.4 MB 651.5 kB/s eta 0:00:05\n",
      "   -------------------------------- ------- 10.9/13.4 MB 653.4 kB/s eta 0:00:04\n",
      "   -------------------------------- ------- 10.9/13.4 MB 653.4 kB/s eta 0:00:04\n",
      "   -------------------------------- ------- 10.9/13.4 MB 652.1 kB/s eta 0:00:04\n",
      "   -------------------------------- ------- 11.0/13.4 MB 653.4 kB/s eta 0:00:04\n",
      "   -------------------------------- ------- 11.0/13.4 MB 651.5 kB/s eta 0:00:04\n",
      "   -------------------------------- ------- 11.0/13.4 MB 651.5 kB/s eta 0:00:04\n",
      "   -------------------------------- ------- 11.0/13.4 MB 651.5 kB/s eta 0:00:04\n",
      "   -------------------------------- ------- 11.0/13.4 MB 651.5 kB/s eta 0:00:04\n",
      "   -------------------------------- ------- 11.0/13.4 MB 645.7 kB/s eta 0:00:04\n",
      "   -------------------------------- ------- 11.0/13.4 MB 645.7 kB/s eta 0:00:04\n",
      "   -------------------------------- ------- 11.0/13.4 MB 645.7 kB/s eta 0:00:04\n",
      "   -------------------------------- ------- 11.0/13.4 MB 645.7 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 11.2/13.4 MB 647.6 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 11.2/13.4 MB 647.6 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 11.2/13.4 MB 647.6 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 11.2/13.4 MB 642.5 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 11.2/13.4 MB 642.5 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 11.2/13.4 MB 641.9 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 11.2/13.4 MB 640.0 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 11.3/13.4 MB 640.0 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 11.3/13.4 MB 640.0 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 11.3/13.4 MB 640.0 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 11.4/13.4 MB 640.6 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 11.4/13.4 MB 638.8 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 11.4/13.4 MB 638.8 kB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 11.4/13.4 MB 637.5 kB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 11.5/13.4 MB 640.6 kB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 11.5/13.4 MB 640.6 kB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 11.6/13.4 MB 639.4 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 11.6/13.4 MB 638.1 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 11.6/13.4 MB 638.1 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 11.6/13.4 MB 638.1 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 11.7/13.4 MB 638.1 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 11.7/13.4 MB 638.1 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 11.7/13.4 MB 638.1 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 11.7/13.4 MB 636.9 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 11.8/13.4 MB 637.5 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 11.8/13.4 MB 636.2 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 11.8/13.4 MB 635.6 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 11.9/13.4 MB 636.3 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 11.9/13.4 MB 635.0 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 11.9/13.4 MB 634.4 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 12.0/13.4 MB 635.7 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 12.0/13.4 MB 634.4 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 12.0/13.4 MB 635.7 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 12.1/13.4 MB 634.4 kB/s eta 0:00:03\n",
      "   ------------------------------------ --- 12.1/13.4 MB 635.0 kB/s eta 0:00:03\n",
      "   ------------------------------------ --- 12.1/13.4 MB 634.4 kB/s eta 0:00:03\n",
      "   ------------------------------------ --- 12.2/13.4 MB 640.6 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 12.2/13.4 MB 638.7 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 12.2/13.4 MB 637.5 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 12.3/13.4 MB 635.0 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 12.3/13.4 MB 635.7 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 12.3/13.4 MB 636.3 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 12.4/13.4 MB 638.1 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 12.4/13.4 MB 638.1 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 12.4/13.4 MB 639.4 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 12.5/13.4 MB 637.5 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 12.5/13.4 MB 638.7 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 12.5/13.4 MB 640.0 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 12.6/13.4 MB 639.4 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 12.6/13.4 MB 641.9 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 12.6/13.4 MB 640.6 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 12.7/13.4 MB 641.9 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 12.7/13.4 MB 641.9 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 12.8/13.4 MB 645.0 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 12.8/13.4 MB 643.7 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.8/13.4 MB 643.8 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.9/13.4 MB 645.7 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.9/13.4 MB 645.7 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.9/13.4 MB 645.7 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.9/13.4 MB 645.7 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.9/13.4 MB 639.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.9/13.4 MB 643.1 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 13.0/13.4 MB 643.1 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 13.0/13.4 MB 641.9 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 13.0/13.4 MB 640.6 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 13.0/13.4 MB 640.0 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 13.1/13.4 MB 640.0 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 13.1/13.4 MB 640.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------  13.1/13.4 MB 638.7 kB/s eta 0:00:01\n",
      "   ---------------------------------------  13.2/13.4 MB 638.7 kB/s eta 0:00:01\n",
      "   ---------------------------------------  13.2/13.4 MB 638.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------  13.2/13.4 MB 639.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------  13.3/13.4 MB 638.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------  13.3/13.4 MB 637.5 kB/s eta 0:00:01\n",
      "   ---------------------------------------  13.3/13.4 MB 640.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------  13.3/13.4 MB 638.1 kB/s eta 0:00:01\n",
      "   ---------------------------------------  13.3/13.4 MB 638.1 kB/s eta 0:00:01\n",
      "   ---------------------------------------  13.4/13.4 MB 637.5 kB/s eta 0:00:01\n",
      "   ---------------------------------------  13.4/13.4 MB 638.1 kB/s eta 0:00:01\n",
      "   ---------------------------------------  13.4/13.4 MB 635.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------  13.4/13.4 MB 635.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.4/13.4 MB 634.3 kB/s eta 0:00:00\n",
      "Downloading alembic-1.13.2-py3-none-any.whl (232 kB)\n",
      "   ---------------------------------------- 0.0/233.0 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/233.0 kB ? eta -:--:--\n",
      "   ------ -------------------------------- 41.0/233.0 kB 991.0 kB/s eta 0:00:01\n",
      "   ---------- ---------------------------- 61.4/233.0 kB 825.8 kB/s eta 0:00:01\n",
      "   --------------- ----------------------- 92.2/233.0 kB 585.1 kB/s eta 0:00:01\n",
      "   ------------------ ------------------- 112.6/233.0 kB 656.4 kB/s eta 0:00:01\n",
      "   ----------------------- -------------- 143.4/233.0 kB 711.9 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 174.1/233.0 kB 618.3 kB/s eta 0:00:01\n",
      "   ------------------------------------ - 225.3/233.0 kB 689.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- 233.0/233.0 kB 649.4 kB/s eta 0:00:00\n",
      "Downloading apache_airflow_providers_fab-1.3.0-py3-none-any.whl (80 kB)\n",
      "   ---------------------------------------- 0.0/80.1 kB ? eta -:--:--\n",
      "   ---------- ----------------------------- 20.5/80.1 kB 640.0 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 61.4/80.1 kB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 80.1/80.1 kB 743.6 kB/s eta 0:00:00\n",
      "Downloading Flask_AppBuilder-4.5.0-py3-none-any.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/2.2 MB 653.6 kB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.1/2.2 MB 751.6 kB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.1/2.2 MB 656.4 kB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.1/2.2 MB 655.8 kB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.2/2.2 MB 700.2 kB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 0.2/2.2 MB 724.0 kB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 0.3/2.2 MB 684.6 kB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 0.3/2.2 MB 681.0 kB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 0.3/2.2 MB 701.4 kB/s eta 0:00:03\n",
      "   ------ --------------------------------- 0.3/2.2 MB 677.0 kB/s eta 0:00:03\n",
      "   ------ --------------------------------- 0.4/2.2 MB 675.0 kB/s eta 0:00:03\n",
      "   ------- -------------------------------- 0.4/2.2 MB 726.4 kB/s eta 0:00:03\n",
      "   ------- -------------------------------- 0.4/2.2 MB 726.4 kB/s eta 0:00:03\n",
      "   -------- ------------------------------- 0.5/2.2 MB 701.0 kB/s eta 0:00:03\n",
      "   --------- ------------------------------ 0.5/2.2 MB 713.7 kB/s eta 0:00:03\n",
      "   --------- ------------------------------ 0.5/2.2 MB 695.1 kB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 0.6/2.2 MB 717.9 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 0.6/2.2 MB 728.0 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 0.7/2.2 MB 712.1 kB/s eta 0:00:03\n",
      "   ------------ --------------------------- 0.7/2.2 MB 730.7 kB/s eta 0:00:03\n",
      "   ------------ --------------------------- 0.7/2.2 MB 718.0 kB/s eta 0:00:03\n",
      "   ------------ --------------------------- 0.7/2.2 MB 718.0 kB/s eta 0:00:03\n",
      "   -------------- ------------------------- 0.8/2.2 MB 711.4 kB/s eta 0:00:03\n",
      "   -------------- ------------------------- 0.8/2.2 MB 709.0 kB/s eta 0:00:02\n",
      "   -------------- ------------------------- 0.8/2.2 MB 699.0 kB/s eta 0:00:02\n",
      "   --------------- ------------------------ 0.9/2.2 MB 714.0 kB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 0.9/2.2 MB 712.7 kB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 0.9/2.2 MB 701.9 kB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 1.0/2.2 MB 715.8 kB/s eta 0:00:02\n",
      "   ------------------ --------------------- 1.0/2.2 MB 713.7 kB/s eta 0:00:02\n",
      "   ------------------ --------------------- 1.0/2.2 MB 697.1 kB/s eta 0:00:02\n",
      "   ------------------- -------------------- 1.1/2.2 MB 710.0 kB/s eta 0:00:02\n",
      "   -------------------- ------------------- 1.1/2.2 MB 707.6 kB/s eta 0:00:02\n",
      "   -------------------- ------------------- 1.1/2.2 MB 706.4 kB/s eta 0:00:02\n",
      "   --------------------- ------------------ 1.2/2.2 MB 711.4 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 1.2/2.2 MB 715.5 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 1.3/2.2 MB 713.9 kB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 1.3/2.2 MB 723.7 kB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 1.3/2.2 MB 717.0 kB/s eta 0:00:02\n",
      "   ------------------------ --------------- 1.4/2.2 MB 720.9 kB/s eta 0:00:02\n",
      "   ------------------------- -------------- 1.4/2.2 MB 724.6 kB/s eta 0:00:02\n",
      "   ------------------------- -------------- 1.4/2.2 MB 717.7 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 1.5/2.2 MB 726.3 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 1.5/2.2 MB 719.5 kB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.5/2.2 MB 719.5 kB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.5/2.2 MB 719.5 kB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.5/2.2 MB 719.5 kB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.5/2.2 MB 719.5 kB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.7/2.2 MB 717.7 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.7/2.2 MB 716.5 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.7/2.2 MB 715.7 kB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.7/2.2 MB 710.3 kB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.8/2.2 MB 704.9 kB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.8/2.2 MB 704.5 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.8/2.2 MB 694.8 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.8/2.2 MB 698.3 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.9/2.2 MB 693.7 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.9/2.2 MB 693.3 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.9/2.2 MB 692.3 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.9/2.2 MB 688.0 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.0/2.2 MB 691.0 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.0/2.2 MB 691.0 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.0/2.2 MB 686.3 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.1/2.2 MB 686.3 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.1/2.2 MB 685.3 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.1/2.2 MB 688.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.2/2.2 MB 684.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 681.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 680.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 676.6 kB/s eta 0:00:00\n",
      "Downloading argcomplete-3.5.0-py3-none-any.whl (43 kB)\n",
      "   ---------------------------------------- 0.0/43.5 kB ? eta -:--:--\n",
      "   --------- ------------------------------ 10.2/43.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 43.5/43.5 kB 707.1 kB/s eta 0:00:00\n",
      "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
      "Downloading ConfigUpdater-3.2-py2.py3-none-any.whl (34 kB)\n",
      "Downloading connexion-2.14.2-py2.py3-none-any.whl (95 kB)\n",
      "   ---------------------------------------- 0.0/95.1 kB ? eta -:--:--\n",
      "   ------------ --------------------------- 30.7/95.1 kB ? eta -:--:--\n",
      "   ------------------------------ --------- 71.7/95.1 kB 787.7 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 95.1/95.1 kB 778.8 kB/s eta 0:00:00\n",
      "Downloading cron_descriptor-1.4.5-py3-none-any.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/50.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.4/50.4 kB 853.8 kB/s eta 0:00:00\n",
      "Downloading croniter-3.0.3-py2.py3-none-any.whl (22 kB)\n",
      "Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "   ---------------------------------------- 0.0/116.3 kB ? eta -:--:--\n",
      "   ---------- ----------------------------- 30.7/116.3 kB 1.3 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 61.4/116.3 kB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------ -------- 92.2/116.3 kB 751.6 kB/s eta 0:00:01\n",
      "   -------------------------------------- 116.3/116.3 kB 679.6 kB/s eta 0:00:00\n",
      "Downloading Flask-2.2.5-py3-none-any.whl (101 kB)\n",
      "   ---------------------------------------- 0.0/101.8 kB ? eta -:--:--\n",
      "   ---- ----------------------------------- 10.2/101.8 kB ? eta -:--:--\n",
      "   --------------- ----------------------- 41.0/101.8 kB 991.0 kB/s eta 0:00:01\n",
      "   --------------------------- ----------- 71.7/101.8 kB 653.6 kB/s eta 0:00:01\n",
      "   -------------------------------------- 101.8/101.8 kB 649.5 kB/s eta 0:00:00\n",
      "Downloading Flask_Caching-2.3.0-py3-none-any.whl (28 kB)\n",
      "Downloading flask_session-0.5.0-py3-none-any.whl (7.2 kB)\n",
      "Downloading flask_wtf-1.2.1-py3-none-any.whl (12 kB)\n",
      "Downloading google_re2-1.1.20240702-1-cp311-cp311-win_amd64.whl (496 kB)\n",
      "   ---------------------------------------- 0.0/496.7 kB ? eta -:--:--\n",
      "    --------------------------------------- 10.2/496.7 kB ? eta -:--:--\n",
      "   ---- ----------------------------------- 61.4/496.7 kB 1.1 MB/s eta 0:00:01\n",
      "   ------ -------------------------------- 81.9/496.7 kB 919.0 kB/s eta 0:00:01\n",
      "   -------- ----------------------------- 112.6/496.7 kB 819.2 kB/s eta 0:00:01\n",
      "   ------------ ------------------------- 163.8/496.7 kB 821.4 kB/s eta 0:00:01\n",
      "   ------------- ------------------------ 174.1/496.7 kB 700.2 kB/s eta 0:00:01\n",
      "   --------------- ---------------------- 204.8/496.7 kB 734.2 kB/s eta 0:00:01\n",
      "   ------------------ ------------------- 245.8/496.7 kB 793.0 kB/s eta 0:00:01\n",
      "   --------------------- ---------------- 286.7/496.7 kB 737.3 kB/s eta 0:00:01\n",
      "   ------------------------- ------------ 327.7/496.7 kB 752.2 kB/s eta 0:00:01\n",
      "   ------------------------- ------------ 337.9/496.7 kB 723.4 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 368.6/496.7 kB 695.9 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 419.8/496.7 kB 728.8 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 419.8/496.7 kB 728.8 kB/s eta 0:00:01\n",
      "   ------------------------------------ - 471.0/496.7 kB 720.0 kB/s eta 0:00:01\n",
      "   -------------------------------------- 496.7/496.7 kB 708.1 kB/s eta 0:00:00\n",
      "Downloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
      "   ---------------------------------------- 0.0/85.0 kB ? eta -:--:--\n",
      "   ---- ----------------------------------- 10.2/85.0 kB ? eta -:--:--\n",
      "   ---------------------------- ----------- 61.4/85.0 kB 1.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 81.9/85.0 kB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 85.0/85.0 kB 686.1 kB/s eta 0:00:00\n",
      "Downloading lazy_object_proxy-1.10.0-cp311-cp311-win_amd64.whl (27 kB)\n",
      "Downloading linkify_it_py-2.0.3-py3-none-any.whl (19 kB)\n",
      "Downloading lockfile-0.12.2-py2.py3-none-any.whl (13 kB)\n",
      "Downloading marshmallow_oneofschema-3.1.1-py3-none-any.whl (5.7 kB)\n",
      "Downloading mdit_py_plugins-0.4.2-py3-none-any.whl (55 kB)\n",
      "   ---------------------------------------- 0.0/55.3 kB ? eta -:--:--\n",
      "   ---------------------- ----------------- 30.7/55.3 kB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 55.3/55.3 kB 577.9 kB/s eta 0:00:00\n",
      "Downloading methodtools-0.4.7-py2.py3-none-any.whl (4.0 kB)\n",
      "Downloading opentelemetry_api-1.27.0-py3-none-any.whl (63 kB)\n",
      "   ---------------------------------------- 0.0/64.0 kB ? eta -:--:--\n",
      "   -------------------------------------- - 61.4/64.0 kB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 64.0/64.0 kB 1.1 MB/s eta 0:00:00\n",
      "Downloading importlib_metadata-8.4.0-py3-none-any.whl (26 kB)\n",
      "Downloading opentelemetry_exporter_otlp-1.27.0-py3-none-any.whl (7.0 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_grpc-1.27.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_http-1.27.0-py3-none-any.whl (17 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.27.0-py3-none-any.whl (17 kB)\n",
      "Downloading opentelemetry_proto-1.27.0-py3-none-any.whl (52 kB)\n",
      "   ---------------------------------------- 0.0/52.5 kB ? eta -:--:--\n",
      "   ------------------------------- -------- 41.0/52.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 52.5/52.5 kB 898.4 kB/s eta 0:00:00\n",
      "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
      "Downloading pendulum-3.0.0-cp311-none-win_amd64.whl (293 kB)\n",
      "   ---------------------------------------- 0.0/293.5 kB ? eta -:--:--\n",
      "   ---- ----------------------------------- 30.7/293.5 kB 1.4 MB/s eta 0:00:01\n",
      "   --------- ----------------------------- 71.7/293.5 kB 991.0 kB/s eta 0:00:01\n",
      "   ------------ -------------------------- 92.2/293.5 kB 880.9 kB/s eta 0:00:01\n",
      "   ------------------ ------------------- 143.4/293.5 kB 853.3 kB/s eta 0:00:01\n",
      "   ---------------------- --------------- 174.1/293.5 kB 807.1 kB/s eta 0:00:01\n",
      "   ------------------------- ------------ 194.6/293.5 kB 845.5 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 235.5/293.5 kB 801.7 kB/s eta 0:00:01\n",
      "   -------------------------------------  286.7/293.5 kB 806.1 kB/s eta 0:00:01\n",
      "   -------------------------------------- 293.5/293.5 kB 755.3 kB/s eta 0:00:00\n",
      "Downloading pluggy-1.5.0-py3-none-any.whl (20 kB)\n",
      "Downloading PyJWT-2.9.0-py3-none-any.whl (22 kB)\n",
      "Downloading python_daemon-3.0.1-py3-none-any.whl (31 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "   ---------------------------------------- 0.0/54.5 kB ? eta -:--:--\n",
      "   ------- -------------------------------- 10.2/54.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 54.5/54.5 kB 702.2 kB/s eta 0:00:00\n",
      "Downloading rich_argparse-1.5.2-py3-none-any.whl (19 kB)\n",
      "Downloading setproctitle-1.3.3-cp311-cp311-win_amd64.whl (11 kB)\n",
      "Downloading SQLAlchemy-1.4.54-cp311-cp311-win_amd64.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/1.6 MB 1.6 MB/s eta 0:00:01\n",
      "   - -------------------------------------- 0.1/1.6 MB 975.2 kB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.1/1.6 MB 737.3 kB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.1/1.6 MB 847.9 kB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.2/1.6 MB 833.5 kB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 0.2/1.6 MB 778.2 kB/s eta 0:00:02\n",
      "   ------ --------------------------------- 0.3/1.6 MB 827.5 kB/s eta 0:00:02\n",
      "   ------ --------------------------------- 0.3/1.6 MB 744.2 kB/s eta 0:00:02\n",
      "   ------- -------------------------------- 0.3/1.6 MB 785.7 kB/s eta 0:00:02\n",
      "   -------- ------------------------------- 0.3/1.6 MB 776.5 kB/s eta 0:00:02\n",
      "   --------- ------------------------------ 0.4/1.6 MB 781.2 kB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 0.4/1.6 MB 778.2 kB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 0.5/1.6 MB 775.8 kB/s eta 0:00:02\n",
      "   ------------ --------------------------- 0.5/1.6 MB 786.4 kB/s eta 0:00:02\n",
      "   ------------ --------------------------- 0.5/1.6 MB 746.7 kB/s eta 0:00:02\n",
      "   -------------- ------------------------- 0.6/1.6 MB 769.1 kB/s eta 0:00:02\n",
      "   -------------- ------------------------- 0.6/1.6 MB 780.2 kB/s eta 0:00:02\n",
      "   --------------- ------------------------ 0.6/1.6 MB 768.3 kB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 0.7/1.6 MB 774.0 kB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 0.7/1.6 MB 779.0 kB/s eta 0:00:02\n",
      "   ------------------ --------------------- 0.7/1.6 MB 772.8 kB/s eta 0:00:02\n",
      "   ------------------- -------------------- 0.8/1.6 MB 771.4 kB/s eta 0:00:02\n",
      "   -------------------- ------------------- 0.8/1.6 MB 762.9 kB/s eta 0:00:02\n",
      "   --------------------- ------------------ 0.8/1.6 MB 768.7 kB/s eta 0:00:01\n",
      "   --------------------- ------------------ 0.9/1.6 MB 755.5 kB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 0.9/1.6 MB 758.5 kB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 0.9/1.6 MB 764.4 kB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.0/1.6 MB 742.8 kB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.0/1.6 MB 756.5 kB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.0/1.6 MB 754.3 kB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.1/1.6 MB 748.9 kB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.1/1.6 MB 759.8 kB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.1/1.6 MB 743.0 kB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.2/1.6 MB 748.0 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.2/1.6 MB 731.0 kB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.2/1.6 MB 748.8 kB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.3/1.6 MB 746.2 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.3/1.6 MB 737.8 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.3/1.6 MB 735.1 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.4/1.6 MB 727.5 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.4/1.6 MB 730.3 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.4/1.6 MB 740.6 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.5/1.6 MB 732.5 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.5/1.6 MB 730.7 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.5/1.6 MB 730.7 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.6/1.6 MB 742.1 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.6/1.6 MB 734.0 kB/s eta 0:00:00\n",
      "Downloading SQLAlchemy_JSONField-1.0.2-py3-none-any.whl (10 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Downloading universal_pathlib-0.2.5-py3-none-any.whl (49 kB)\n",
      "   ---------------------------------------- 0.0/49.9 kB ? eta -:--:--\n",
      "   ------------------------ --------------- 30.7/49.9 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 49.9/49.9 kB 843.7 kB/s eta 0:00:00\n",
      "Downloading Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
      "   ---------------------------------------- 0.0/233.6 kB ? eta -:--:--\n",
      "   ------ -------------------------------- 41.0/233.6 kB 960.0 kB/s eta 0:00:01\n",
      "   ----------- --------------------------- 71.7/233.6 kB 975.2 kB/s eta 0:00:01\n",
      "   ------------------- ------------------ 122.9/233.6 kB 901.1 kB/s eta 0:00:01\n",
      "   ----------------------- -------------- 143.4/233.6 kB 944.1 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 174.1/233.6 kB 871.5 kB/s eta 0:00:01\n",
      "   ------------------------------------ - 225.3/233.6 kB 808.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- 233.6/233.6 kB 791.9 kB/s eta 0:00:00\n",
      "Downloading apache_airflow_providers_common_compat-1.2.0-py3-none-any.whl (13 kB)\n",
      "Downloading apache_airflow_providers_common_io-1.4.0-py3-none-any.whl (18 kB)\n",
      "Downloading apache_airflow_providers_common_sql-1.16.0-py3-none-any.whl (46 kB)\n",
      "   ---------------------------------------- 0.0/46.7 kB ? eta -:--:--\n",
      "   ----------------------------------- ---- 41.0/46.7 kB 991.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 46.7/46.7 kB 578.0 kB/s eta 0:00:00\n",
      "Downloading apache_airflow_providers_ftp-3.11.0-py3-none-any.whl (19 kB)\n",
      "Downloading apache_airflow_providers_http-4.13.0-py3-none-any.whl (27 kB)\n",
      "Downloading apache_airflow_providers_imap-3.7.0-py3-none-any.whl (17 kB)\n",
      "Downloading apache_airflow_providers_smtp-1.8.0-py3-none-any.whl (22 kB)\n",
      "Downloading apache_airflow_providers_sqlite-3.9.0-py3-none-any.whl (13 kB)\n",
      "Downloading aiohttp-3.10.5-cp311-cp311-win_amd64.whl (379 kB)\n",
      "   ---------------------------------------- 0.0/379.1 kB ? eta -:--:--\n",
      "   ------ --------------------------------- 61.4/379.1 kB 1.6 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 71.7/379.1 kB 1.3 MB/s eta 0:00:01\n",
      "   ----------- -------------------------- 112.6/379.1 kB 726.2 kB/s eta 0:00:01\n",
      "   --------------- ---------------------- 153.6/379.1 kB 762.6 kB/s eta 0:00:01\n",
      "   ------------------- ------------------ 194.6/379.1 kB 841.6 kB/s eta 0:00:01\n",
      "   ---------------------- --------------- 225.3/379.1 kB 808.4 kB/s eta 0:00:01\n",
      "   ----------------------- -------------- 235.5/379.1 kB 798.9 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 286.7/379.1 kB 803.7 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 337.9/379.1 kB 805.1 kB/s eta 0:00:01\n",
      "   -------------------------------------- 379.1/379.1 kB 786.6 kB/s eta 0:00:00\n",
      "Downloading cachelib-0.9.0-py3-none-any.whl (15 kB)\n",
      "Downloading clickclick-20.10.2-py2.py3-none-any.whl (7.4 kB)\n",
      "Downloading Flask_Login-0.6.3-py3-none-any.whl (17 kB)\n",
      "Downloading inflection-0.5.1-py2.py3-none-any.whl (9.5 kB)\n",
      "Downloading more_itertools-10.5.0-py3-none-any.whl (60 kB)\n",
      "   ---------------------------------------- 0.0/61.0 kB ? eta -:--:--\n",
      "   -------------------------- ------------- 41.0/61.0 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 61.0/61.0 kB 1.1 MB/s eta 0:00:00\n",
      "Downloading sqlparse-0.5.1-py3-none-any.whl (44 kB)\n",
      "   ---------------------------------------- 0.0/44.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 44.2/44.2 kB 1.1 MB/s eta 0:00:00\n",
      "Downloading time_machine-2.15.0-cp311-cp311-win_amd64.whl (20 kB)\n",
      "Downloading wirerope-0.4.7-py2.py3-none-any.whl (9.0 kB)\n",
      "Downloading wtforms-3.1.2-py3-none-any.whl (145 kB)\n",
      "   ---------------------------------------- 0.0/146.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/146.0 kB ? eta -:--:--\n",
      "   -------- ------------------------------- 30.7/146.0 kB ? eta -:--:--\n",
      "   ------------------------- -------------- 92.2/146.0 kB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 112.6/146.0 kB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 112.6/146.0 kB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- 146.0/146.0 kB 721.9 kB/s eta 0:00:00\n",
      "Downloading zipp-3.20.2-py3-none-any.whl (9.2 kB)\n",
      "Downloading docutils-0.21.2-py3-none-any.whl (587 kB)\n",
      "   ---------------------------------------- 0.0/587.4 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 30.7/587.4 kB ? eta -:--:--\n",
      "   ---- ---------------------------------- 71.7/587.4 kB 991.0 kB/s eta 0:00:01\n",
      "   ------- -------------------------------- 112.6/587.4 kB 1.1 MB/s eta 0:00:01\n",
      "   --------- ---------------------------- 143.4/587.4 kB 944.1 kB/s eta 0:00:01\n",
      "   ------------ ------------------------- 194.6/587.4 kB 908.0 kB/s eta 0:00:01\n",
      "   -------------- ----------------------- 225.3/587.4 kB 860.2 kB/s eta 0:00:01\n",
      "   ---------------- --------------------- 256.0/587.4 kB 827.5 kB/s eta 0:00:01\n",
      "   ------------------ ------------------- 286.7/587.4 kB 803.7 kB/s eta 0:00:01\n",
      "   --------------------- ---------------- 337.9/587.4 kB 838.1 kB/s eta 0:00:01\n",
      "   ----------------------- -------------- 358.4/587.4 kB 825.0 kB/s eta 0:00:01\n",
      "   -------------------------- ----------- 409.6/587.4 kB 825.1 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 460.8/587.4 kB 873.2 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 491.5/587.4 kB 832.7 kB/s eta 0:00:01\n",
      "   ---------------------------------- --- 532.5/587.4 kB 857.5 kB/s eta 0:00:01\n",
      "   -------------------------------------  583.7/587.4 kB 853.3 kB/s eta 0:00:01\n",
      "   -------------------------------------- 587.4/587.4 kB 840.1 kB/s eta 0:00:00\n",
      "Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.6 kB ? eta -:--:--\n",
      "   -------------------- ------------------- 41.0/78.6 kB ? eta -:--:--\n",
      "   ------------------------------------ --- 71.7/78.6 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 78.6/78.6 kB 1.1 MB/s eta 0:00:00\n",
      "Downloading uc_micro_py-1.0.3-py3-none-any.whl (6.2 kB)\n",
      "Downloading aiohappyeyeballs-2.4.0-py3-none-any.whl (12 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
      "Downloading Flask_Babel-2.0.0-py3-none-any.whl (9.3 kB)\n",
      "Downloading Flask_JWT_Extended-4.6.0-py2.py3-none-any.whl (22 kB)\n",
      "Downloading Flask_Limiter-3.8.0-py3-none-any.whl (28 kB)\n",
      "Downloading Flask_SQLAlchemy-2.5.1-py2.py3-none-any.whl (17 kB)\n",
      "Downloading frozenlist-1.4.1-cp311-cp311-win_amd64.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.5 kB ? eta -:--:--\n",
      "   -------------------------------- ------- 41.0/50.5 kB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 50.5/50.5 kB 1.3 MB/s eta 0:00:00\n",
      "Downloading googleapis_common_protos-1.65.0-py2.py3-none-any.whl (220 kB)\n",
      "   ---------------------------------------- 0.0/220.9 kB ? eta -:--:--\n",
      "   ------- -------------------------------- 41.0/220.9 kB 1.9 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 92.2/220.9 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 122.9/220.9 kB 1.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 184.3/220.9 kB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 215.0/220.9 kB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- 220.9/220.9 kB 896.4 kB/s eta 0:00:00\n",
      "Downloading marshmallow_sqlalchemy-0.28.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading multidict-6.1.0-cp311-cp311-win_amd64.whl (28 kB)\n",
      "Downloading opentelemetry_sdk-1.27.0-py3-none-any.whl (110 kB)\n",
      "   ---------------------------------------- 0.0/110.5 kB ? eta -:--:--\n",
      "   ---------------------- ----------------- 61.4/110.5 kB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 102.4/110.5 kB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 110.5/110.5 kB 1.1 MB/s eta 0:00:00\n",
      "Downloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl (149 kB)\n",
      "   ---------------------------------------- 0.0/149.7 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/149.7 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/149.7 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/149.7 kB ? eta -:--:--\n",
      "   ------------------------------------ - 143.4/149.7 kB 847.9 kB/s eta 0:00:01\n",
      "   -------------------------------------- 149.7/149.7 kB 810.7 kB/s eta 0:00:00\n",
      "Downloading prison-0.2.1-py2.py3-none-any.whl (5.8 kB)\n",
      "Downloading SQLAlchemy_Utils-0.41.2-py3-none-any.whl (93 kB)\n",
      "   ---------------------------------------- 0.0/93.1 kB ? eta -:--:--\n",
      "   -------- ------------------------------- 20.5/93.1 kB ? eta -:--:--\n",
      "   ------------------------------ --------- 71.7/93.1 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 93.1/93.1 kB 881.3 kB/s eta 0:00:00\n",
      "Downloading yarl-1.11.1-cp311-cp311-win_amd64.whl (110 kB)\n",
      "   ---------------------------------------- 0.0/110.5 kB ? eta -:--:--\n",
      "   --- ------------------------------------ 10.2/110.5 kB ? eta -:--:--\n",
      "   ---------------------- ----------------- 61.4/110.5 kB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 92.2/110.5 kB 744.7 kB/s eta 0:00:01\n",
      "   -------------------------------------- 110.5/110.5 kB 710.9 kB/s eta 0:00:00\n",
      "Downloading limits-3.13.0-py3-none-any.whl (45 kB)\n",
      "   ---------------------------------------- 0.0/45.5 kB ? eta -:--:--\n",
      "   ----------------------------------- ---- 41.0/45.5 kB 991.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 45.5/45.5 kB 751.2 kB/s eta 0:00:00\n",
      "Downloading importlib_resources-6.4.5-py3-none-any.whl (36 kB)\n",
      "Building wheels for collected packages: python-nvd3, unicodecsv\n",
      "  Building wheel for python-nvd3 (setup.py): started\n",
      "  Building wheel for python-nvd3 (setup.py): finished with status 'done'\n",
      "  Created wheel for python-nvd3: filename=python_nvd3-0.16.0-py3-none-any.whl size=37611 sha256=952a8c14e93f3cbdaceedd31049a75681587a646dabaa0526a07148247050dfb\n",
      "  Stored in directory: c:\\users\\as.moradi\\appdata\\local\\pip\\cache\\wheels\\88\\26\\3f\\0695457939a8025f982dca665d4d7018468ccf8ccf43e45c5f\n",
      "  Building wheel for unicodecsv (setup.py): started\n",
      "  Building wheel for unicodecsv (setup.py): finished with status 'done'\n",
      "  Created wheel for unicodecsv: filename=unicodecsv-0.14.1-py3-none-any.whl size=10757 sha256=567a86c999b95cbfaed0ce5a0fe8c7e8d140fe56a1b4418293d4d41a3f4feb1d\n",
      "  Stored in directory: c:\\users\\as.moradi\\appdata\\local\\pip\\cache\\wheels\\ec\\03\\6f\\d2e0162d94c0d451556fa43dd4d5531457245c34a36b41ef4a\n",
      "Successfully built python-nvd3 unicodecsv\n",
      "Installing collected packages: unicodecsv, lockfile, cron-descriptor, zipp, WTForms, wirerope, werkzeug, universal-pathlib, uc-micro-py, tenacity, tabulate, sqlparse, sqlalchemy, setproctitle, pyjwt, prison, pluggy, pathspec, opentelemetry-proto, multidict, more-itertools, Mako, lazy-object-proxy, inflection, importlib-resources, gunicorn, googleapis-common-protos, google-re2, frozenlist, email-validator, docutils, dill, deprecated, configupdater, colorlog, cachelib, asgiref, argcomplete, aiohappyeyeballs, yarl, time-machine, sqlalchemy-utils, sqlalchemy-jsonfield, requests-toolbelt, python-nvd3, python-daemon, opentelemetry-exporter-otlp-proto-common, methodtools, mdit-py-plugins, marshmallow-sqlalchemy, marshmallow-oneofschema, linkify-it-py, limits, importlib_metadata, flask, croniter, clickclick, alembic, aiosignal, rich-argparse, pendulum, opentelemetry-api, flask-wtf, Flask-SQLAlchemy, flask-session, flask-login, Flask-Limiter, Flask-JWT-Extended, flask-caching, Flask-Babel, aiohttp, opentelemetry-semantic-conventions, flask-appbuilder, connexion, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-exporter-otlp, apache-airflow-providers-common-sql, apache-airflow-providers-sqlite, apache-airflow-providers-smtp, apache-airflow-providers-imap, apache-airflow-providers-http, apache-airflow-providers-ftp, apache-airflow-providers-fab, apache-airflow-providers-common-io, apache-airflow-providers-common-compat, apache-airflow\n",
      "  Attempting uninstall: werkzeug\n",
      "    Found existing installation: Werkzeug 3.0.3\n",
      "    Uninstalling Werkzeug-3.0.3:\n",
      "      Successfully uninstalled Werkzeug-3.0.3\n",
      "  Attempting uninstall: sqlalchemy\n",
      "    Found existing installation: SQLAlchemy 2.0.25\n",
      "    Uninstalling SQLAlchemy-2.0.25:\n",
      "      Successfully uninstalled SQLAlchemy-2.0.25\n",
      "  Attempting uninstall: flask\n",
      "    Found existing installation: Flask 3.0.2\n",
      "    Uninstalling Flask-3.0.2:\n",
      "      Successfully uninstalled Flask-3.0.2\n",
      "Successfully installed Flask-Babel-2.0.0 Flask-JWT-Extended-4.6.0 Flask-Limiter-3.8.0 Flask-SQLAlchemy-2.5.1 Mako-1.3.5 WTForms-3.1.2 aiohappyeyeballs-2.4.0 aiohttp-3.10.5 aiosignal-1.3.1 alembic-1.13.2 apache-airflow-2.10.1 apache-airflow-providers-common-compat-1.2.0 apache-airflow-providers-common-io-1.4.0 apache-airflow-providers-common-sql-1.16.0 apache-airflow-providers-fab-1.3.0 apache-airflow-providers-ftp-3.11.0 apache-airflow-providers-http-4.13.0 apache-airflow-providers-imap-3.7.0 apache-airflow-providers-smtp-1.8.0 apache-airflow-providers-sqlite-3.9.0 argcomplete-3.5.0 asgiref-3.8.1 cachelib-0.9.0 clickclick-20.10.2 colorlog-6.8.2 configupdater-3.2 connexion-2.14.2 cron-descriptor-1.4.5 croniter-3.0.3 deprecated-1.2.14 dill-0.3.8 docutils-0.21.2 email-validator-2.2.0 flask-2.2.5 flask-appbuilder-4.5.0 flask-caching-2.3.0 flask-login-0.6.3 flask-session-0.5.0 flask-wtf-1.2.1 frozenlist-1.4.1 google-re2-1.1.20240702 googleapis-common-protos-1.65.0 gunicorn-23.0.0 importlib-resources-6.4.5 importlib_metadata-8.4.0 inflection-0.5.1 lazy-object-proxy-1.10.0 limits-3.13.0 linkify-it-py-2.0.3 lockfile-0.12.2 marshmallow-oneofschema-3.1.1 marshmallow-sqlalchemy-0.28.2 mdit-py-plugins-0.4.2 methodtools-0.4.7 more-itertools-10.5.0 multidict-6.1.0 opentelemetry-api-1.27.0 opentelemetry-exporter-otlp-1.27.0 opentelemetry-exporter-otlp-proto-common-1.27.0 opentelemetry-exporter-otlp-proto-grpc-1.27.0 opentelemetry-exporter-otlp-proto-http-1.27.0 opentelemetry-proto-1.27.0 opentelemetry-sdk-1.27.0 opentelemetry-semantic-conventions-0.48b0 pathspec-0.12.1 pendulum-3.0.0 pluggy-1.5.0 prison-0.2.1 pyjwt-2.9.0 python-daemon-3.0.1 python-nvd3-0.16.0 requests-toolbelt-1.0.0 rich-argparse-1.5.2 setproctitle-1.3.3 sqlalchemy-1.4.54 sqlalchemy-jsonfield-1.0.2 sqlalchemy-utils-0.41.2 sqlparse-0.5.1 tabulate-0.9.0 tenacity-9.0.0 time-machine-2.15.0 uc-micro-py-1.0.3 unicodecsv-0.14.1 universal-pathlib-0.2.5 werkzeug-2.2.3 wirerope-0.4.7 yarl-1.11.1 zipp-3.20.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "taipy-core 3.1.1 requires sqlalchemy<=2.0.25,>=2.0.16, but you have sqlalchemy 1.4.54 which is incompatible.\n",
      "taipy-gui 3.1.4 requires flask<=3.0.2,>=3.0.0, but you have flask 2.2.5 which is incompatible.\n",
      "taipy-rest 3.1.1 requires flask<=3.0.2,>=3.0.0, but you have flask 2.2.5 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install apache-airflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">C:\\Users\\as.moradi\\AppData\\Local\\Temp\\ipykernel_34316\\</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">2182584547.</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">py:</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">5</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> RemovedInAirflow3Warning</span><span style=\"color: #808000; text-decoration-color: #808000\">: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mC:\\Users\\as.moradi\\AppData\\Local\\Temp\\ipykernel_34316\\\u001b[0m\u001b[1;33m2182584547.\u001b[0m\u001b[1;33mpy:\u001b[0m\u001b[1;33m5\u001b[0m\u001b[1;33m RemovedInAirflow3Warning\u001b[0m\u001b[33m: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Task(EmptyOperator): start_pipeline>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.empty import EmptyOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "with DAG(\n",
    "  'etl_twitter_pipeline',\n",
    "  description=\"A simple twitter ETL pipeline using Python,PostgreSQL and Apache Airflow\",\n",
    "  start_date=datetime(year=2023, month=2, day=5),\n",
    "  schedule_interval=timedelta(minutes=2)\n",
    ") as dag:\n",
    "\n",
    "  start_pipeline = EmptyOperator(\n",
    "    task_id='start_pipeline',\n",
    "  )\n",
    "\n",
    "start_pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
